# A Case Study on Filtering for End-to-End Speech Translation

Md Mahfuz Ibn Alamα Antonios Anastasopoulosα,γ αDepartment of Computer Science, George Mason University γArchimedes AI Research Unit, RC Athena, Greece {malam21,antonis}@gmu.edu

# Abstract

It is relatively easy to mine a large parallel corpus for any machine learning task, such as speech-to-text or speech-to-speech translation. Although these mined corpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model’s performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.

# 1 Introduction

End-to-end speech translation (E2E ST) represents a paradigm shift from cascaded speech translation by utilizing a direct model for converting source language speech into target-language text. This approach offers advantages such as low latency and avoiding the "error propagation" issue often encountered in traditional cascade methods (Weiss et al., 2017; Sperber and Paulik, 2020). In recent times, end-to-end speech translation (ST) models have demonstrated noteworthy progress, achieving comparable or, in some instances, superior results when compared to traditional cascaded ST models (Bentivogli et al., 2021; Anastasopoulos et al., 2021, 2022).

Despite these benefits, E2E ST models face challenges when compared to the cascaded methods (Di Gangi et al., 2019), as there are robust training data available for automatic speech recognition (ASR) and machine translation (MT) which can be employed in cascade methods. The relatively limited training data for E2E ST models may lead to sub-optimal performance in certain scenarios. Prior research endeavors frequently tap into machine translation (MT) data to enhance training by applying multi-task learning techniques, as observed in works by Ye et al. (2022); Tang et al. (2021). Here the encoder and decoder are shared between speech translation (ST) and machine translation (MT) and the model acquires similar representations from distinct modalities.

![](images/9cbdfed33a55036902e17aaa3c0641631d72dd34d095c94d49dd7f5c7e2357d1.jpg)  
Figure 1: Pipeline of our noisy data creation.

Another research direction can be automatically creating large-scale mined corpus (Duquenne et al., 2023). However, the created data is often too noisy mostly because it is too hard to align data in two different languages. On top of that, for Speech Translation we have to also align between two different modalities. Then the research focus falls on filtering the noisy data. This work focuses on filtering Speech Translation data and observing if by doing so the systems’ performance can be improved.

Our main contributions in a nutshell:

• We try different filtering methods for Speech Translation.   
• We show that the simplest ratio-based filtering can effectively differentiate clean data from noisy data.   
• We show that a somewhat good ST model trained by a small clean data can efficiently filter a noisy corpus and thus improve the models’ performance.

Table 1: WER score of the ASR systems used to transcribe the SpeechMatrix dataset. The multilingual model uses FR, ES, PT, and IT languages. The monolingual models outperform the multilingual models.   

<table><tr><td>Language</td><td>Model</td><td>WER↓</td><td>Dataset</td></tr><tr><td rowspan="3">English (EN)</td><td>(Di Gangi et al., 2019)</td><td>25.81</td><td rowspan="3">MuST-C</td></tr><tr><td>Monolingual</td><td>14.26</td></tr><tr><td>Multilingual</td><td>-</td></tr><tr><td rowspan="3">French (FR)</td><td>(Salesky et al., 2021)</td><td>45.6</td><td rowspan="3">MTedX</td></tr><tr><td>Monolingual</td><td>19.2</td></tr><tr><td>Multilingual</td><td>22.24</td></tr><tr><td rowspan="3">Spanish (ES)</td><td>(Salesky et al., 2021)</td><td>46.4</td><td rowspan="3">MTedX</td></tr><tr><td>Monolingual</td><td>17.06</td></tr><tr><td>Multilingual</td><td>18.17</td></tr><tr><td rowspan="3">Portuguese (PT)</td><td>(Salesky et al., 2021)</td><td>54.8</td><td rowspan="3">MTedX</td></tr><tr><td>Monolingual</td><td>22.86</td></tr><tr><td>Multilingual</td><td>24.48</td></tr><tr><td rowspan="3">Italian (IT)</td><td>(Salesky et al., 2021)</td><td>48.0</td><td rowspan="3">MTedX</td></tr><tr><td>Monolingual</td><td>-</td></tr><tr><td>Multilingual</td><td>21.37</td></tr></table>

# 2 Approach

Our approach is of three parts: Firstly, we create noisy speech-text data pairs. Secondly, we score each pair using our sorting techniques. Thirdly, we filter the pairs based on the scores and create subsets of pairs to train the model.

# 2.1 Noisy Data Creation

For our work, we have used a speech-to-speech translation dataset, SpeechMatrix (SM). To create the speech-to-text dataset for ST training, we have used ASR models to transcribe both the source and target sides. Figure 1 outlines our approach. We created an ASR system, using each language’s MuST-C or MTedX dataset. The WER of these ASR systems is shown in Table 1. The speechto-speech translation dataset itself is noisy, and this approach adds one more layer of noise as the ASR models are imperfect. We explore if filtering approaches can work on this super noisy data.

# 2.2 Scoring Techniques

For our experiments, we explore ratio-based scoring techniques and one NLL(Negative Log likelihood)-based technique. The ratio-based techniques are faster to compute than NLL-based ones.

Text-Text Ratio To calculate this score, we divide the number of tokens from the source side transcript sentence by the number of tokens from the target side transcript sentence.

Speech-Speech Ratio As our original data is Speech-to-Speech data, we can calculate this score where on both the source and target side, we find the length (audio seconds) to compute the ratio.

Text-Speech Ratio Here, on the source side, we calculate the number of tokens, and on the target side, we calculate seconds of speech.

Speech-Text Ratio This is the same as the TextText Ratio, but now, on the source side, instead of the token count of the transcript text, we use the length in seconds of the speech as the numerator.

NLL(Negative Log Likelihood)-based We have used our baseline systems to calculate this score. Given a speech-text data pair $< x , y >$ , we use Equation 1 to calculate the NLL loss. Here, $f _ { \theta }$ is our baseline model.

$$
\begin{array} { c } { { \mathcal { L } ( \theta ) = - \left( y l o g \hat { y } _ { \theta } + \left( 1 - y \right) l o g \left( 1 - \hat { y } _ { \theta } \right) \right) } } \\ { { } } \\ { { \hat { y } _ { \theta } = \sigma ( f _ { \theta } ( x ) ) } } \end{array}
$$

# 2.3 Filtering

After obtaining the score of each data instance using the techniques explained above, we use two approaches to select a subset of clean data. For ratio-based techniques, we calculate $z$ -scores, and for NLL-based techniques, we use percentiles.

$z$ -score calculation requires first determining the mean, $\mu$ and standard deviation, $\sigma$ of the ratiobased scores of all the instances. Once we have these, we can calculate the $z$ -score using Equation 3. Here, $x$ is the ratio of a data instance. After calculating the $z$ -score from each technique, we create four subsets where the $z$ -score of the instances are less than or equal to 0.25, 0.5, 0.75, and 1.0. $z$ -score here ranges from 0 to $\infty$ .

$$
z = | ( x - \mu ) / \sigma |
$$

Percentile calculation requires sorting the data instances based on NLL loss score. After sorting in ascending order, we create four subsets that take the first $20 \%$ , $40 \%$ , $60 \%$ , and $80 \%$ data instances.

Union and Intersection We create another subset of data instances by performing a union and intersection operation between the best two subsets of each language pair.

# 3 Experimental Settings

# 3.1 Datasets

MuST-C (Di Gangi et al., 2019) are multilingual speech translation corpora whose size and quality facilitate end-to-end ST training from English into German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. MuST-C includes at least 385 hours of audio recordings from TED

Talks in English for each target language. They are automatically aligned with their manual transcriptions and translations on a sentence level.

MTedX (Salesky et al., 2021) supports speech recognition (ASR) and speech translation (ST) research across multiple non-English languages. The corpus is a collection of audio recordings from TEDx talks in 8 source languages. Transcriptions are segmented into sentences and aligned to the audio in the source language and the translation in the target language.

SpeechMatrix (Duquenne et al., 2023) is a groundbreaking initiative that introduces a vast and extensive multilingual corpus focused on speechto-speech translations (S2ST). This corpus consists of authentic speech recordings of the European Parliament, offering a rich and diverse collection. In total, SpeechMatrix boasts speech alignments across 136 language pairs, providing a comprehensive resource with a staggering cumulative duration of 418 thousand hours of speech data.

# 3.2 Baseline Models

We have created three baseline models. First, we have created a bilingual baseline model using the MuST-C or MTedX dataset. We consider these datasets as clean datasets. Thus, we expect these models’ performance to be quite high in quality. Next, we have created another baseline model using the SpeechMatrix dataset. This dataset is quite noisy; thus, we expect this model not to perform very well. Our last baseline model combines the MTedX or MuST-C datasets and the SpeechMatrix datasets without filtering. These baselines aim to quantify how adding the unfiltered SpeechMatrix affects the models’ performance.

# 3.3 ASR and ST Model

For our experiments in ASR and ST, we use the same neural architecture. This setting allows us to initialize the ST encoder with the ASR encoder for faster convergence (Bahar et al., 2019; Bansal et al., 2019). We also created a multilingual ASR model and used it to initialize the multilingual ST model’s encoder. We used FAIRSEQ (Ott et al., 2019) to train end-to-end Transformer models for ST, using 80-dimensional log mel filterbank features with cmvn and SpecAugment (Park et al., 2019), and 2-d convolutions for downsampling of speech in time. We use the s2t_transformer_s architecture of FAIRSEQ, which has a network depth of 12 encoder layers and 6 decoder layers. All other hyperparameters are discussed in the Appendix A.1.

# 3.4 Language Pairs

According to the clean data availability of the language pairs, we divided the language pairs that we experimented on into three categories: high-, mid-, and low-resource. High-resourced language pairs like English-French have a large amount of clean data as opposed to mid- or low-resourced. Thus, we can get a high-quality ST system in the highresourced language pairs. In contrast, the models of the mid-resourced pairs like French-English and Spanish-Portuguese will have moderate quality. The low-resourced ones like Portuguese-Spanish will be even more degraded in quality.

# 4 Results

Using the above approach we create different subsets of data to train our systems with. We run experiments in two different settings: Bilingual, and Multilingual. This is to get an understanding of the filtering approaches’ generalization ability.

# 4.1 Bilingual Systems

We have experimented on four language pairs. Due to space constraints, all results of this experiment are in Appendix A.2. Table 2 presents a summarized version of the results where we only show the highest-scoring models. We have two setups in each pair: trained only using the filtered SpeechMatrix data and trained by combining the filtered SpeechMatrix data with either MuST-C or MTedX. High Resource (EN-FR) This language pair has the most clean data (270,000 instances) and the highest baseline score (31.51 BLEU) among the four pairs. This is the only pair where adding the whole noisy SpeechMatrix data substantially hurts the system’s performance. All models created from five filtering techniques lead to higher BLEU scores than the Baseline $( \mathrm { M u } \mathrm { S T } { \mathrm { - } } \mathrm { C } \ +$ unfiltered SM). This shows that even a simple filtering technique is helpful instead of using all the noisy data.

We achieved the best performance (32.07 BLEU) by using only $20 \%$ clean data from SpeechMatrix on top of the clean data, a 0.56 BLEU score improvement over the baseline model, Baseline(MuST-C). We find a similar trend for all filtering techniques, where adding more data hurts performance. Note that systems trained using just the SpeechMatrix are awful, yet our best model earns 3.55 BLEU points just by filtering compared to the Baseline(unfiltered SM) model.

Table 2: Results of filtering approaches for Bilingual ST systems. The models created using filtering approaches outperform the baseline in each language direction.   

<table><tr><td>Lang Pair</td><td>ST System (Only SM) # of pairs BLEU</td><td></td><td>ST System (MuST-C/MTedX + SM) # of pairs BLEU</td><td></td><td></td></tr><tr><td rowspan="2">En-Fr</td><td>Baseline (unfiltered SM)</td><td>1384112 4.87</td><td>Baseline (MusT-C)</td><td></td><td>26925631.51</td></tr><tr><td>80%NLL</td><td>1107289</td><td>8.42 MuST-C + SM (20% NLL)</td><td></td><td>546078</td><td>32.07</td></tr><tr><td rowspan="2">Fr-En</td><td>Baseline (unfiltered SM)</td><td>1403985</td><td>5.20</td><td>Baseline (MTedX + unfiltered SM)</td><td>1433931</td><td>26.74</td></tr><tr><td>80%NLL</td><td>1107289</td><td>6.31</td><td>MTedX + SM (60% NLL)</td><td>872337</td><td>27.66</td></tr><tr><td rowspan="2">Es-Pt</td><td>Baseline (unfiltered SM)</td><td>1074027</td><td>0.57</td><td>Baseline (MTedX + unfiltered SM)</td><td>1094848</td><td>26.28</td></tr><tr><td>0.5 z Text-Text</td><td>559285</td><td>5.13</td><td>MTedX + SM (0.5 z TT U 0.5 z Speech-Speech)</td><td>834039</td><td>27.07</td></tr><tr><td rowspan="2">Pt-Es</td><td>Baseline (unfiltered SM)</td><td>1107283</td><td>3.23</td><td>Baseline (MTedX + unfiltered SM)</td><td>1118635</td><td>22.64</td></tr><tr><td>60%NLL</td><td>664369</td><td>4.38</td><td>MTedX + SM (80% NLL)</td><td>897178</td><td>23.7</td></tr></table>

<table><tr><td>Multi ST System</td><td>ES-EN (∆)</td><td>FR-EN (∆)</td><td>PT-EN (∆)</td><td>IT-EN (∆)</td><td>Average (∆)</td><td># of pairs</td></tr><tr><td>(Salesky et al., 2021)</td><td>12.3</td><td>12.0</td><td>12.0</td><td>10.7</td><td>11.75</td><td>120538</td></tr><tr><td>(i) Baseline (MTedX)</td><td>17.34</td><td>18.92</td><td>20.02</td><td>15.74</td><td>18.01</td><td>120538</td></tr><tr><td>(i) + SM (100%)</td><td>15.46 (-1.88)</td><td>19.93 (1.01)</td><td>16.61 (-3.41)</td><td>17.43 (1.69)</td><td>17.36 (-0.65)</td><td>7292751</td></tr><tr><td>(i) + SM (80% NLL)</td><td>17.04 (-0.30)</td><td>21.01 (2.09)</td><td>18.53 (-1.49)</td><td>18.59 (2.85)</td><td>18.79 (0.78)</td><td>5858310</td></tr><tr><td> (i) + SM (60% NLL)</td><td>18.15 (0.81)</td><td>22.86 (3.94)</td><td>21.1 (1.08)</td><td>18.92 (3.24)</td><td>20.26 (2.25)</td><td>4423867</td></tr><tr><td> (i) + SM (40% NLL)</td><td>19.86 (2.52)</td><td>24.76 (5.84)</td><td>22.35 (2.33)</td><td>20.51 (4.77)</td><td>21.87 (3.86)</td><td>2989425</td></tr><tr><td> (i) + SM (20% NLL)</td><td>20.4 (3.06)</td><td>24.96 (6.04)</td><td>24.45 (4.43)</td><td>20.81 (5.07)</td><td>22.66 (4.65)</td><td>1554982</td></tr></table>

Table 3: Results of filtering approaches for Multilingual ST system. $\Delta$ is the difference from the baseline model.

Mid Resource (FR-EN & ES-PT) With data sizes of 30,000 and 21,000, respectively, FR-EN and ES-PT language pairs are mid-resourced. When trained using only the SpeechMatrix, the BLEU score of the best model for FR-EN increases by 1.11 BLEU, whereas ES-PT increases by 4.56 in comparison to the Baseline (unfiltered SM) system.

Combining the unfiltered SpeechMatrix data with the MTedX data for FR-EN gives us an additional 8-BLEU points boost compared to only using MTedX data. Filtering the SpeechMatrix with the NLL-based technique and using $60 \%$ of the data again gives an additional boost of 0.92.

For ES-PT, the baseline score increases when combining the unfiltered SpeechMatrix data with the MTedX by 5.23 BLEU compared to Baseline (MTedX). Our best model is the union of our two best subsets ( $0 . 5 \mathrm { ~ z ~ }$ Text-Text $\cup 0 . 5 \ : z$ SpeechSpeech), with an increase of 0.79 BLEU points over the Baseline (MTedX $^ +$ unfiltered SM). Even using the intersection of our two best subsets, the system is better than the baseline one. This exhibits the prowess of the filtering techniques.

Low Resource (PT-ES) This language pair has the lowest amount of clean data (11,000 instances). Thus, the baseline score (12.62 BLEU) is the lowest among the four pairs. Just adding the SpeechMatrix gives an astonishing 10.02 BLEU score improvement. Noisy parallel data apparently helps improve the system to a certain level when the models are low-resourced. Beyond that, showing the model noisy data only hurts the performance. Filtering is crucial in that aspect to trim down the noisy dataset to a cleaner version. The best model uses NLL-based filtering and $80 \%$ of the dataset, which increases its performance by 1.06 BLEU points on top of the Baseline (MTedX $^ +$ unfiltered SM).

# 4.2 Multilingual Systems

After observing the success of the NLL-based filtering for four language pairs, we have also explored a multilingual setup. Table 3 shows the result of our multilingual experiments. The language directions are to English from Spanish, French, Portuguese, and Italian. Compared to the bilingual FR-EN results trained on the same dataset, we obtained a higher BLEU score (14.74 to 18.92).

We achieve even better scores by combining MTedX data with the first $20 \%$ SpeechMatrix data filtered by NLL-loss for all language pairs. On average, this boosts our BLEU score by 4.65. We obtained the highest jump for FR-EN, a 6.04 BLEU score improvement. We obtain a similar trend for all other subsets, where the quality degrades the more noisy data we use.

# 5 Conclusion

All language pairs benefit from the simplest filtering techniques, from high-resourced to lowresourced pairs. The multilingual models’ increase in model robustness is much more consistent and higher than that of bilingual ones.

# 6 Limitations

Even though we observed a huge increase in BLEU points by incorporating the simplest scoring and filtering techniques for all the language pairs that we have experimented with, one limitation arises when computing the NLL-based score. The limitation is that the computation time is huge. On the other hand ratio-based scoring techniques are faster by a huge margin. So, even if our best models are from NLL-based scoring, ratio-based scoring can be an obvious choice where time is an issue.

# References

Antonios Anastasopoulos, Loïc Barrault, Luisa Bentivogli, Marcely Zanon Boito, Ondˇrej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Estève, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz, Barry Haddow, Benjamin Hsu, Dávid Javorský, Vera Kloudová, Surafel Lakew, Xutai Ma, Prashant˘ Mathur, Paul McNamee, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Jan ˇ Niehues, Xing Niu, John Ortega, Juan Pino, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Yogesh Virkar, Alexander Waibel, Changhan Wang, and Shinji Watanabe. 2022. Findings of the IWSLT 2022 evaluation campaign. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 98–157, Dublin, Ireland (in-person and online). Association for Computational Linguistics.

Antonios Anastasopoulos, Ondˇrej Bojar, Jacob Bremerman, Roldano Cattoni, Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Alexander Waibel, Changhan Wang, and Matthew Wiesner. 2021. FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN. In Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), pages 1–29, Bangkok, Thailand (online). Association for Computational Linguistics.

Parnia Bahar, Tobias Bieschke, and Hermann Ney. 2019. A comparative study on end-to-end speech to text translation. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 792–799.

Sameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pre-training on high-resource speech recognition improves lowresource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and

Short Papers), pages 58–68, Minneapolis, Minnesota.   
Association for Computational Linguistics.

Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, Matteo Negri, and Marco Turchi. 2021. Cascade versus direct speech translation: Do the differences still make a difference? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2873–2887, Online. Association for Computational Linguistics.

Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2012–2017, Minneapolis, Minnesota. Association for Computational Linguistics.

Paul-Ambroise Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswami, Changhan Wang, Juan Pino, Benoît Sagot, and Holger Schwenk. 2023. SpeechMatrix: A large-scale mined corpus of multilingual speech-to-speech translations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16251–16269, Toronto, Canada. Association for Computational Linguistics.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.

Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019, interspeech_2019. ISCA.

Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas W. Oard, and Matt Post. 2021. The multilingual tedx corpus for speech recognition and translation. CoRR, abs/2102.01757.

Matthias Sperber and Matthias Paulik. 2020. Speech translation and the end-to-end promise: Taking stock of where we are. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7409–7421, Online. Association for Computational Linguistics.

Yun Tang, Juan Pino, Xian Li, Changhan Wang, and Dmitriy Genzel. 2021. Improving speech translation by understanding and learning from the auxiliary text translation task. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4252–4261, Online. Association for Computational Linguistics.

Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to-sequence models can directly transcribe foreign speech. CoRR, abs/1703.08581.

Rong Ye, Mingxuan Wang, and Lei Li. 2022. Crossmodal contrastive learning for speech translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5099–5113, Seattle, United States. Association for Computational Linguistics.

# A Appendix

A.1 Hyper-parameters   

<table><tr><td colspan="2">Training Parameters</td></tr><tr><td>Criterion</td><td>Label Smoothing Cross Entropy</td></tr><tr><td>Label Smoothing</td><td>0.1</td></tr><tr><td>Optimizer</td><td>Adam</td></tr><tr><td>Learning Rate (LR)</td><td>2e-3</td></tr><tr><td>LR Scheduler</td><td>Inverse Sqrt</td></tr><tr><td>Max Tokens</td><td>40000</td></tr><tr><td>Warm-up Updates</td><td>10000</td></tr><tr><td>Max Update</td><td>100000</td></tr><tr><td>Clip Norm</td><td>10.0</td></tr><tr><td>Update Frequency</td><td>8</td></tr></table>

Table 4: Hyper-parameters during training our ASR and ST models.   
Table 5: Hyper-parameters during inference of our ASR and ST models.   

<table><tr><td colspan="2">Inference Parameters</td></tr><tr><td>Criterion</td><td>Label Smoothing Cross Entropy</td></tr><tr><td>Beam Max Tokens</td><td>5 10000</td></tr><tr><td>Scoring (ST)</td><td>sacrebleu</td></tr><tr><td></td><td></td></tr><tr><td>Scoring (ASR) - WER Tokenizer</td><td>WER 13a</td></tr><tr><td>- WER Lowercase</td><td></td></tr><tr><td>- WER remove Punct</td><td>True True</td></tr></table>

Table 6: Hyper-parameters of our ASR and ST model architecture.   

<table><tr><td colspan="2">Model Architecture</td></tr><tr><td>Vocab Size (ASR) Vocab Size (ST) Vocab Type Convld Kernel Size Convld Channels</td><td>5000 8000 Unigram 5,5 1024 256</td></tr><tr><td>Con21d Channels Encoder Embedding Dim Encoder FFN Embedding Dim Encoder Attention Head</td><td>256 256*8 4</td></tr><tr><td>Encoder Layer Decoder Embedding Dim Decoder FFN Embedding Dim Decoder Attention Head Decoder Layer Dropout Activation Function</td><td>12 256 256*8 4 6 0.1 RELU</td></tr></table>

# A.2 Bilingual Systems Full Results

Table 7: Results of filtering approaches for En-Fr direction   

<table><tr><td>ST System (Only SM)</td><td># of pairs</td><td>BLEU</td><td>ST System (MuST-C + SM)</td><td># of pairs</td><td>BLEU</td><td>Overlap</td></tr><tr><td></td><td></td><td></td><td>(Di Gangi et al., 2019)</td><td>269256</td><td>22.29</td><td></td></tr><tr><td>Baseline (unfiltered SM)</td><td>1384112</td><td>4.87</td><td>(i) Baseline (MuST-C) (i) Baseline (MuST-C + unfiltered SM)</td><td>269256 1653367</td><td>31.51</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>28.06</td><td></td></tr><tr><td>20%NLL</td><td>276823</td><td>6.01</td><td>(i) + 20% NLL</td><td>546078</td><td>32.07</td><td>100.00</td></tr><tr><td>40%NLL 60%NLL</td><td>553645</td><td>5.41 7.74</td><td>(i) + 40% NLL (i) + 60% NLL</td><td>822900 1099722</td><td>31.86 31</td><td>66.36 49.66</td></tr><tr><td>80%NLL</td><td>830467 1107289</td><td>8.42</td><td>(i) + 80% NLL</td><td>1376544</td><td>30.13</td><td>39.67</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.25 z Text-Text 0.5 z Text-Text</td><td>251201</td><td>4.64</td><td> (i) + 0.25 z Text-Text</td><td>520456</td><td>31.82</td><td>62.14</td></tr><tr><td>0.75 z Text-Text</td><td>582453 820294</td><td>7.39 7.79</td><td>(i) + 0.5 z Text-Text (i) + 0.75 z Text-Text</td><td>851708 1089549</td><td>31.2 30.04</td><td>74.57 84.66</td></tr><tr><td>1 z Text-Text</td><td>1017152</td><td>5.54</td><td> (i) + 1 z Text-Text</td><td>1286407</td><td>29.52</td><td>91.43</td></tr><tr><td>0.25 z Speech-Speech</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.5 z Speech-Speech</td><td>409934 723714</td><td>2.76 4.63</td><td> (i) + 0.25 z Speech-Speech</td><td>679189 992969</td><td>31.28</td><td>58.38</td></tr><tr><td>0.75 z Speech-Speech</td><td>945962</td><td>5.05</td><td>(i) + 0.5 z Speech-Speech (i) + 0.75 z Speech-Speech</td><td>1215217</td><td>29.95 29.6</td><td>66.60 73.84</td></tr><tr><td>1 z Speech-Speech</td><td>1095298</td><td>4.81</td><td> (i) + 1 z Speech-Speech</td><td>1364553</td><td>29.13</td><td>79.89</td></tr><tr><td>0.25 z Speech-Text</td><td>458789</td><td>5.77</td><td></td><td></td><td></td><td></td></tr><tr><td>0.5 z Speech-Text</td><td>860457</td><td>7.3</td><td>(i) + 0.25 z Speech-Text (i) + 0.5 z Speech-Text</td><td>728044 1129712</td><td>31.05 29.64</td><td>55.30 67.51</td></tr><tr><td>0.75 z Speech-Text</td><td>1133782</td><td>7.57</td><td>(i) + 0.75 z Speech-Text</td><td>1403037</td><td>29.04</td><td>75.42</td></tr><tr><td>1 z Speech-Text</td><td>1266324</td><td>6.82</td><td> (i) + 1 z Speech-Text</td><td>1535579</td><td>28.89</td><td>82.75</td></tr><tr><td>0.25 z Text-Speech</td><td>311053</td><td>2.13</td><td>(i) + 0.25 z Text-Speech</td><td>580308</td><td>31.81</td><td></td></tr><tr><td>0.5 z Text-Speech</td><td>601697</td><td>3.88</td><td> (i) + 0.5 z Text-Speech</td><td>870952</td><td>30.57</td><td>57.82 66.56</td></tr><tr><td>0.75 z Text-Speech</td><td>849956</td><td>5.23</td><td>(i) + 0.75 z Text-Speech</td><td>1119211</td><td>29.91</td><td>74.09</td></tr><tr><td>1 z Text-Speech</td><td>1042400</td><td>6.21</td><td> (i) + 1 z Text-Speech</td><td>1311655</td><td>29.69</td><td>80.55</td></tr><tr><td></td><td></td><td></td><td>(i) + 20% NLL U 0.25 z Text-Text</td><td></td><td></td><td></td></tr><tr><td colspan="3"></td><td> (i) + 20% NLL ∩ 0.25 z Text-Text</td><td>764533 302001</td><td>31.58 30.94</td><td>100.00 100.00</td></tr></table>

Table 8: Results of filtering approaches for Fr-En direction   

<table><tr><td>ST System (Only SM)</td><td># of pairs</td><td>BLEU</td><td>ST System (MTedX + SM)</td><td># of pairs</td><td>BLEU</td><td>Overlap</td></tr><tr><td></td><td></td><td></td><td>(Salesky et al., 2021)</td><td>29946</td><td>8.9</td><td></td></tr><tr><td></td><td></td><td></td><td>(i) Baseline (MTedX)</td><td>29946</td><td>14.74</td><td></td></tr><tr><td>Baseline (unfiltered SM)</td><td>1403985</td><td>5.2</td><td> (i) Baseline (MTedX + unfiltered SM)</td><td>1433931</td><td>26.74</td><td></td></tr><tr><td>20%NLL</td><td>280797</td><td>5.15</td><td>(i) + 20% NLL</td><td>310743</td><td>22.33</td><td>100.00</td></tr><tr><td>40%NLL</td><td>561594</td><td>6.31</td><td>(i) + 40% NLL</td><td>591540</td><td>25.46</td><td>100.00</td></tr><tr><td>60%NLL</td><td>842391</td><td>5.36</td><td>(i) + 60% NLL</td><td>872337</td><td>27.66</td><td>100.00</td></tr><tr><td>80% NLL</td><td>1123188</td><td>5.68</td><td>(i) + 80% NLL</td><td>1153134</td><td>27.23</td><td>75.65</td></tr><tr><td>0.25 z Text-Text</td><td>475032</td><td>3.8</td><td>(i) + 0.25 z Text-Text</td><td>504978</td><td>26.52</td><td>37.18</td></tr><tr><td>0.5 z Text-Text</td><td>893780</td><td>4.78</td><td>(i) + 0.5 z Text-Text</td><td>923726</td><td>26.58</td><td>66.73</td></tr><tr><td>0.75 z Text-Text</td><td>1159658</td><td>4.11</td><td>(i) + 0.75 z Text-Text</td><td>1189604</td><td>26.81</td><td>85.20</td></tr><tr><td>1 z Text-Text</td><td>1285668</td><td>4.61</td><td> (i) + 1 z Text-Text</td><td>1315614</td><td>26.33</td><td>93.89</td></tr><tr><td>0.25 z Speech-Speech</td><td>413584</td><td>4.24</td><td> (i) + 0.25 z Speech-Speech</td><td>443530</td><td>26.85</td><td>32.76</td></tr><tr><td>0.5 z Speech-Speech</td><td>729455</td><td>4.28</td><td>(i) + 0.5 z Speech-Speech</td><td>759401</td><td>26.68</td><td>55.00</td></tr><tr><td>0.75 z Speech-Speech 1 z Speech-Speech</td><td>952797 1102227</td><td>4.51 4.94</td><td>(i) + 0.75 z Speech-Speech (i) + 1 z Speech-Speech</td><td>982743</td><td>27.06</td><td>70.73</td></tr><tr><td></td><td></td><td></td><td></td><td>1132173</td><td>26.62</td><td>81.25</td></tr><tr><td>0.25 z Speech-Text</td><td>308869</td><td>4.92</td><td>(i) + 0.25 z Speech-Text</td><td>338815</td><td>25.21</td><td>25.28</td></tr><tr><td>0.5 z Speech-Text 0.75 z Speech-Text</td><td>600976</td><td>4.36</td><td>(i) + 0.5 z Speech-Text</td><td>630922</td><td>26.35</td><td>45.88</td></tr><tr><td>1 z Speech-Text</td><td>849820 1041531</td><td>4.56 4.25</td><td>(i) + 0.75 z Speech-Text (i) + 1 z Speech-Text</td><td>879766 1071477</td><td>26.55 26.7</td><td>63.44 76.93</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.25 z Text-Speech 0.5 z Text-Speech</td><td>458975</td><td>4.37 3.4</td><td>(i) + 0.25 z Text-Speech</td><td>488921</td><td>26.07</td><td>35.79</td></tr><tr><td>0.75 z Text-Speech</td><td>861614 1134478</td><td>4.91</td><td>(i) + 0.5 z Text-Speech (i) + 0.75 z Text-Speech</td><td>891560 1164424</td><td>26.22 26.26</td><td>64.12 83.33</td></tr><tr><td>1 z Text-Speech</td><td>1266947</td><td>4.31</td><td>(i) + 1 z Text-Speech</td><td>1296893</td><td>26.64</td><td>92.59</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="3"></td><td>(i) + 60% NLL U 0.75 z Speech-Speech (i) + 60% NLL ∩ 0.75 z Speech-Speech</td><td>1250087 604993</td><td>27.18 26.22</td><td>100.00 100.00</td></tr></table>

Table 9: Results of filtering approaches for Es-Pt direction   

<table><tr><td>ST System (Only SM)</td><td># of pairs</td><td>BLEU</td><td>ST System (MTEdX + SM)</td><td># of pairs</td><td>BLEU</td><td>Overlap</td></tr><tr><td></td><td></td><td></td><td>(Salesky et al., 2021)</td><td>20821</td><td>12.2</td><td></td></tr><tr><td>Baseline (unfiltered SM)</td><td></td><td>0.57</td><td>(i) Baseline (MTedX)</td><td>20821</td><td>21.05</td><td></td></tr><tr><td></td><td>1074027</td><td></td><td> (i) Baseline (MTedX + unfiltered SM)</td><td>1094848</td><td>26.28</td><td></td></tr><tr><td>20%NLL</td><td>214806</td><td>2.54</td><td>(i) + 20% NLL</td><td>235627</td><td>24.44</td><td>49.43</td></tr><tr><td>40% NLL</td><td>429611</td><td>2.76 4.07</td><td>(i) + 40% NLL</td><td>450432</td><td>24.57</td><td>48.60</td></tr><tr><td>60% NLL 80%NLL</td><td>644416 859221</td><td>4.24</td><td>(i) + 60% NLL (i) + 80% NLL</td><td>665237 880042</td><td>24.45 26.16</td><td>49.72 51.21</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.25 z Text-Text 0.5 z Text-Text</td><td>294944</td><td>3.9</td><td>(i) + 0.25 z Text-Text</td><td>315765</td><td>25.62</td><td>54.43</td></tr><tr><td>0.75 z Text-Text</td><td>559285</td><td>5.13 4.55</td><td>(i) + 0.5 z Text-Text</td><td>580106 783625</td><td>26.77</td><td>100.00</td></tr><tr><td>1 z Text-Text</td><td>762804 904329</td><td>4.2</td><td>(i) + 0.75 z Text-Text (i) + 1 z Text-Text</td><td>925150</td><td>26.22 26.13</td><td>100.00 100.00</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.25 z Speech-Speech 0.5 z Speech-Speech</td><td>370267</td><td>3.34</td><td>(i) + 0.25 z Speech-Speech</td><td>391088</td><td>26.3</td><td>39.95</td></tr><tr><td>0.75 z Speech-Speech</td><td>584872 739279</td><td>4.09 4.39</td><td>(i) + 0.5 z Speech-Speech (i) + 0.75 z Speech-Speech</td><td>605693 760100</td><td>26.67</td><td>60.64</td></tr><tr><td>1 z Speech-Speech</td><td>847397</td><td>4.51</td><td>(i) + 1 z Speech-Speech</td><td>868218</td><td>26.57 26.35</td><td>74.69 83.81</td></tr><tr><td>0.25 z Speech-Text</td><td>295074</td><td>3.68</td><td></td><td>315895</td><td></td><td></td></tr><tr><td>0.5 z Speech-Text</td><td>570658</td><td>5.02</td><td>(i) + 0.25 z Speech-Text (i) + 0.5 z Speech-Text</td><td>591479</td><td>24.74 26.13</td><td>36.25 64.95</td></tr><tr><td>0.75 z Speech-Text</td><td>788590</td><td>4.95</td><td>(i) + 0.75 z Speech-Text</td><td>809411</td><td>26.47</td><td>84.40</td></tr><tr><td>1 z Speech-Text</td><td>924162</td><td>4.56</td><td>(i) + 1 z Speech-Text</td><td>944983</td><td>26.64</td><td>93.82</td></tr><tr><td>0.25 z Text-Speech</td><td>288588</td><td>2.32</td><td>(i) + 0.25 z Text-Speech</td><td>309409</td><td></td><td></td></tr><tr><td>0.5 z Text-Speech</td><td>549748</td><td>3.79</td><td>(i) + 0.5 z Text-Speech</td><td>570569</td><td>24.98 25.55</td><td>34.47 60.88</td></tr><tr><td>0.75 z Text-Speech</td><td>755145</td><td>3.2</td><td>(i) + 0.75 z Text-Speech</td><td>775966</td><td>24.96</td><td>79.37</td></tr><tr><td>1 z Text-Speech</td><td>897009</td><td>2.81</td><td>(i) + 1 z Text-Speech</td><td>917830</td><td>25.65</td><td>89.99</td></tr><tr><td></td><td></td><td></td><td> (i) + 0.5 z Text-Text UJ 0.5 z Speech-Speech</td><td>834039</td><td>27.07</td><td>100.00</td></tr><tr><td colspan="2"></td><td></td><td> (i) + 0.5 z Text-Text ∩ 0.5 z Speech-Speech</td><td>351760</td><td>26.46</td><td>100.00</td></tr></table>

Table 10: Results of filtering approaches for Pt-Es direction   

<table><tr><td>ST System (Only SM)</td><td># of pairs</td><td>BLEU</td><td>ST System (MTedX + SM)</td><td># of pairs</td><td>BLEU</td><td>Overlap</td></tr><tr><td></td><td></td><td></td><td>(Salesky et al., 2021)</td><td>11353</td><td>8.7</td><td></td></tr><tr><td></td><td></td><td></td><td>(i) Baseline (MTedX)</td><td>11353</td><td>12.62</td><td></td></tr><tr><td>Baseline (SM)</td><td>1107283</td><td>3.23</td><td> (i) Baseline (MTedX + unfiltered SM)</td><td>1118635</td><td>22.64</td><td></td></tr><tr><td>20% NLL</td><td>221457</td><td>3.24</td><td>(i) + 20% NLL</td><td>232810</td><td>20.01</td><td>100.00</td></tr><tr><td>40%NLL</td><td>442913</td><td>4.11</td><td>(i) + 40% NLL</td><td>454266</td><td>20.07</td><td>100.00</td></tr><tr><td>60%NLL</td><td>664369</td><td>4.38</td><td>(i) + 60% NLL</td><td>675722</td><td>21.02</td><td>100.00</td></tr><tr><td>80%NLL</td><td>885825</td><td>4.02</td><td>(i) + 80% NLL</td><td>897178</td><td>23.7</td><td>100.00</td></tr><tr><td>0.25 z Text-Text</td><td>314739</td><td>1.35</td><td>(i) + 0.25 z Text-Text</td><td>326092</td><td>21.66</td><td>30.41</td></tr><tr><td>0.5 z Text-Text</td><td>603732</td><td>2.98</td><td>(i) + 0.5 z Text-Text</td><td>615085</td><td>22.88</td><td>57.11</td></tr><tr><td>0.75 z Text-Text 1 z Text-Text</td><td>818558</td><td>2.97</td><td>(i) + 0.75 z Text-Text</td><td>829911</td><td>22.91</td><td>76.83</td></tr><tr><td></td><td>950625</td><td>3.53</td><td> (i) + 1 z Text-Text</td><td>961978</td><td>23.07</td><td>88.98</td></tr><tr><td>0.25 z Speech-Speech</td><td>363865</td><td>2.2</td><td> (i) + 0.25 z Speech-Speech</td><td>375218</td><td>22.51</td><td>35.40</td></tr><tr><td>0.5 z Speech-Speech 0.75 z Speech-Speech</td><td>596387</td><td>2.97</td><td>(i) + 0.5 z Speech-Speech</td><td>607740</td><td>22.08</td><td>56.79</td></tr><tr><td>1 z Speech-Speech</td><td>756379 865675</td><td>3.22 3.34</td><td> (i) + 0.75 z Speech-Speech</td><td>767732</td><td>23.19</td><td>71.46</td></tr><tr><td></td><td></td><td></td><td> (i) + 1 z Speech-Speech</td><td>877028</td><td>22.46</td><td>81.42</td></tr><tr><td>0.25 z Speech-Text</td><td>290424</td><td>2.24</td><td> (i) + 0.25 z Speech-Text</td><td>301777</td><td>20.97</td><td>28.18</td></tr><tr><td>0.5 z Speech-Text</td><td>552538</td><td>2.51</td><td>(i) + 0.5 z Speech-Text</td><td>563891</td><td>22.7</td><td>52.39</td></tr><tr><td>0.75 z Speech-Text 1 z Speech-Text</td><td>760190 901333</td><td>2.78 3.88</td><td>(i) + 0.75 z Speech-Text</td><td>771543</td><td>22.67</td><td>71.52</td></tr><tr><td></td><td></td><td></td><td> (i) + 1 z Speech-Text</td><td>912686</td><td>21.88</td><td>84.46</td></tr><tr><td>0.25 z Text-Speech 0.5 z Text-Speech</td><td>295699</td><td>1.27</td><td> (i) + 0.25 z Text-Speech</td><td>307052</td><td>20.07</td><td>28.66</td></tr><tr><td>0.75 z Text-Speech</td><td>571514</td><td>2.77 3.29</td><td>(i) + 0.5 z Text-Speech (i) + 0.75 z Text-Speech</td><td>582867 801429</td><td>22.69 23.32</td><td>54.09</td></tr><tr><td>1 z Text-Speech</td><td>790076</td><td>2.4</td><td> (i) + 1 z Text-Speech</td><td>938848</td><td>22.96</td><td>74.20</td></tr><tr><td></td><td>927495</td><td></td><td></td><td></td><td></td><td>86.83</td></tr><tr><td colspan="3"></td><td>(i) + 80% NLL U 0.75 z Speech-Speech (i) + 80% NLL ∩ 0.75 z Speech-Speech</td><td>1043876 621034</td><td>22.6 22.94</td><td>100.00 100.00</td></tr></table>