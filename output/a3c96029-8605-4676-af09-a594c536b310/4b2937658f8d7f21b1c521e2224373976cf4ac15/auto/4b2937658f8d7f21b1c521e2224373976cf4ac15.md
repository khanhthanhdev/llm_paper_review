# A STUDY OF DATA AUGMENTATION AND ACCURACY IMPROVEMENT IN MACHINE TRANSLATION FOR VIETNAMESE SIGN LANGUAGE

THI BICH DIEP NGUYEN $^ { \cdot 1 , 2 , }$ ∗, TRUNG-NGHIA PHUNG $2$ , TAT-THANG VU $^ 3$

$^ { 1 }$ Graduate University of Science and Technology, Vietnam Academy of Science and Technology, 18 Hoang Quoc Viet Street, Cau Giay District, Ha Noi, Viet Nam   
$^ 2$ Thai Nguyen University of Information and Communication Technology, Z115 Street, Quyet Thang Ward, Thai Nguyen City, Viet Nam   
$^ 3$ Institute of Information Technology, Vietnam Academy of Science and Technology, 18 Hoang Quoc Viet Street, Cau Giay District, Ha Noi, Viet Nam

Abstract. Sign languages are independent languages of deaf communities. The translation from normal languages (i.e., Vietnamese Language - VL) as long as other sign languages to Vietnamese sign language (VSL) is a meaningful task that breaks down communication barriers and improves the quality of life for the deaf community. In this paper, we experimented with and proposed several methods for building and improving models for the VL to VSL translation task. We presented a data augmentation method to improve the performance of our neural machine translation models. Using an initial dataset of 10k bilingual sentence pairs, we were able to obtain a new dataset of 60k sentence pairs with a perplexity score no more than 1.5 times that of the original dataset. Experiments on the original dataset showed that rule-based models achieved the highest BLEU score of 68.02 among the translation models. However, with the augmented dataset, the Transformer model achieved the best performance with a BLEU score of 89.23, which is significantly better than that of other conventional approach methods.

Keywords. Natural language processing, machine translation, Vietnamese sign language, data augmentation.

# 1. INTRODUCTION

Sign language has been developed for a long time and is recognized as the official language of the deaf community in various countries. The sign language used by the deaf community in Vietnam is called Vietnamese sign language (VSL). Although sign language has many similarities with spoken language, there are significant differences between sign language and spoken/written language [1]. For example, in American sign language (ASL), there is a separate grammar system (separate rules for phonetics, morphology, syntax, and semantics) that differs from English [2]. Similarly, VSL is used as the official language in the deaf community of Vietnam with about 7 million people. Like other foreign languages, the communication barrier is significant if one cannot understand and interpret sign language.

The sign language interpretation process involves two tasks, which are translating from sign language to spoken language and vice versa. Among them, the translation task from spoken language is an important task to convey information and provide social knowledge to the deaf.

The process of translating spoken language into sign language involves the following steps.

![](images/2741f8af50a282d749ffe390db1a71c3ba4a26965e2aebd8be26882bd10b2686.jpg)  
Figure 1: The process of translating speech into sign language

In which, (1) refers to the process of translating speech recognition into text. Many studies and applications have effectively handled this task, such as Google’s API. (2) is the process of processing ordinary text into correct syntax in sign language. (3) is the process of simulating correctly syntaxed sign language text into representations such as 3D models, videos, or images of sign language.

In this procedure, the second step gets the most attention due to the completion of the conveyed message. The basic challenge is that sign language, in general, has a limited vocabulary compared to spoken/written language. If the machine translation is poorly performed, the complete message might not be successfully communicated, or in some cases, the conveyed message has a different meaning from the original [3].

The VSL translation task involves taking a regular Vietnamese sentence as input and producing an image, video, or 3D model as the final output. However, an important intermediate step in the translation process is to convert the regular Vietnamese sentence to a syntactically correct sentence in VSL. This is because VSL has some basic features such as reductionism, emphasis on focal points, and changes in word order compared to regular Vietnamese. In addition, there have been proposed technical methods for representing syntactically correct VSL sentences as images or 3D models that have produced good results. This means that the components of the sentence are separated, and we store them in a dictionary as a code that contains two components: the word/phrase and how it is represented using a 3D model. The soft-linkage motion between the sentence components is handled using interpolation techniques. Therefore, the scope of the task is focused on translating regular Vietnamese sentences into syntactically correct sign language sentences.

With remarkable advances in information technology, there have been the sign language translation systems developed worldwide, such as TESSA, which translates speech into British sign language (BSL) [4]; ViSiCAST, a tool for translating English into British sign language [5]; SignSynth project that employs the ASCII-Stokoe model [6]; ASL Workbench, an automated text-to-American sign language translation system [7]; and TEAM project, a system that translates text into American sign language using a contiguous bilingual parse tree technique [8]. Most of these research projects initially relied on structural-based translation models.

Recent studies have been maximizing the use of advances in natural language processing (NLP), deep neural networks (DNN), and machine translation (MT) to develop systems that can translate between sign language and spoken language, to bridge the communication gap between the sign language community and the spoken language community [9]. A recent study of Gouri Sankar Mishra and colleagues proposed a system for translating spoken English into Indian sign language (ISL)[10]. The translation model follows a rule-based approach in which a parser is used to parse the full English sentence into a dependency structure representing the syntax and grammar information of a sentence. An ISL sentence is then generated from an ISL bilingual dictionary and a word network, with the ISL cues corresponding to the appropriate ISL signs being displayed.

Galian et al. experimented with two NMT architectures with optimized hyperparameters, various tokenization methods, and two data augmentation techniques (back-translation and paraphrasing). Through experimentation, they achieved significant improvements for models trained on the Phoenix 14T and DGS datasets for German sign language [11]. Following research on sign name adoption by Ka corri et al. [12], acceptance within the Deaf community is crucial for the application of sign language technologies. The perspective of Deaf users must be accurately analyzed, and the implementation of technology for the deaf community must be effective [13].

Currently, machine translation of Vietnamese sign language (VSL) is still a new and underexplored research field. Like other sign language translation problems in the world, many studies on VSL focus on the second step of the translation process - translating from regular text to the correct syntax in sign language. Therefore, there have been some studies on VSL related to the problem of translating Vietnamese to VSL with promising results, but there are also many limitations. The prominent limitation of these studies is the small database, which leads to low accuracy [14, 15, 8].

We have achieved certain results with the methods and translation models on a small dataset that we have constructed. Our research process has gone through several stages [16, 17]. Initially, we proposed a rule-based translation method based on the syntax rules of VSL. In this paper, we have experimented with some more advanced machine translation methods using a neural network approach and proposed a simple data enrichment method to apply to translation models. This is necessary for training models to help the translation system become more accurate. Section 3 presents the experimental results with some proposed modern translation models, and finally, a detailed analysis and evaluation results will be presented.

# 2. DATA AUGMENTATION

# 2.1. Data augmentation background

The base dataset is a bilingual corpus consisting of 10,000 sentence pairs in Vietnamese - Vietnamese sign language that we semi-automatically built and evaluated by language experts. The process of constructing the bilingual data is described in the following steps:

Step 1. Build the VSL-lexicon dictionary. The VSL-lexicon data stores lexical units with accompanying information such as word type, annotation code, synonyms, and corresponding animation models. Due to the difficulty of manually producing animation models with a large workload, currently, there are only 200 models in the VSL-lexicon. The models are saved in .FBX files. For the “.FBX” file format, 3D models can be exported with all animations, motions, rigging, and other parameters stored in the file. The “.FBX” file format is supported by many different 3D software and is the standard file format used in Unity. Table 1 describes the structure of the VSL-lexicon data.

Table 1: Table describing the VSL-lexicon dictionary   

<table><tr><td rowspan=1 colspan=1>ID</td><td rowspan=1 colspan=1>Lexical unit</td><td rowspan=1 colspan=1>Lexical category</td><td rowspan=1 colspan=1>Synonym</td><td rowspan=1 colspan=1>Tag code</td><td rowspan=1 colspan=1>Corresponding 3D animation model</td></tr><tr><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>a</td><td rowspan=1 colspan=1>Alphabet</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL0001</td><td rowspan=1 colspan=1>M3D0001.FBX</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>ǎ</td><td rowspan=1 colspan=1>Alphabet</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL0002</td><td rowspan=1 colspan=1>M3D0002.FBX</td></tr><tr><td rowspan=1 colspan=1>153</td><td rowspan=1 colspan=1>Toi (1)</td><td rowspan=1 colspan=1>Pronoun (P)</td><td rowspan=1 colspan=1>tao, tó</td><td rowspan=1 colspan=1>VSL0153</td><td rowspan=1 colspan=1>M3D0153.FBX</td></tr><tr><td rowspan=1 colspan=1>154</td><td rowspan=1 colspan=1>ho (They)</td><td rowspan=1 colspan=1>Pronoun(P)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL0154</td><td rowspan=1 colspan=1>M3D0154.FBX</td></tr><tr><td rowspan=1 colspan=1>296</td><td rowspan=1 colspan=1>chét (die)</td><td rowspan=1 colspan=1>Verb (V)</td><td rowspan=1 colspan=1>hi sinh, tú nan</td><td rowspan=1 colspan=1>VSL0296</td><td rowspan=1 colspan=1>M3D0296.FBX</td></tr><tr><td rowspan=1 colspan=1>3035</td><td rowspan=1 colspan=1>truòng hoc (school)</td><td rowspan=1 colspan=1>Noun (N)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL3035</td><td rowspan=1 colspan=1>M3D3035.FBX</td></tr><tr><td rowspan=1 colspan=1>3036</td><td rowspan=1 colspan=1>Nhà (house)</td><td rowspan=1 colspan=1>Noun (N)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL3036</td><td rowspan=1 colspan=1>M3D3036.FBX</td></tr><tr><td rowspan=1 colspan=1>6176</td><td rowspan=1 colspan=1>xuong ròng (Cactus)</td><td rowspan=1 colspan=1>Noun(N)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>VSL6176</td><td rowspan=1 colspan=1>Not in database yet</td></tr></table>

In this dictionary, there is a compilation of a set of synonyms to maximize the representation of words/phrases in Vietnamese sentences to VSL, as the lexicon of sign language is limited.

Step 2. Construct the Vie-VSL-10K dataset, which consists of bilingual sentence pairs. The data includes sentences in the communication domain, partially processed using automatic methods. We utilize a Vietnamese syntactic parsing toolkit, which is a research product by Dr. Nguyen Phuong Thai and colleagues, for our specific task. The preprocessing stage involves data normalization along with tokenization and part-of-speech tagging using the VietWS toolkit [18]. Subsequently, this dataset undergoes preliminary reviews and finally, the data is evaluated by a group of sign language experts. Finally, we have collected 10,000 bilingual sentence pairs in Vietnamese and VSL. The data is publicly available and shared at https://github.com/BichDiep/data-rules-VSL. We propose a method to augment this dataset based on Wordnet from the original 10,000 sentence pairs. Table 2 provides some examples of the different syntax between regular Vietnamese sentences and the correctly formatted VSL sentences in the Vie-VSL-10K dataset we have constructed.

Table 2: Syntax differences between regular Vietnamese sentences and correctly formatted VSL sentences.   

<table><tr><td rowspan=1 colspan=1>ID</td><td rowspan=1 colspan=1>The Vietnamese sentence is syntactically analyzed</td><td rowspan=1 colspan=1>The VSL sentence is syntactically analyzed</td></tr><tr><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>SQ (NP (N Ban) (N ten)) (VP (V la) (WHNP (P gi))) (? ?)</td><td rowspan=1 colspan=1>SQ (NP (N Ban) (N ten) (P gi)) (? ?)</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>S (NP (P Toi)) (NP (N ten) (VP (V la) (NP (Np Hiéu))) (..)</td><td rowspan=1 colspan=1>S (NP (P Toi)) (NP (N ten) (Np Hiéu)) (..)</td></tr><tr><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>S (NP (N Khé)) (C thì) (AP (A chua)) (..)</td><td rowspan=1 colspan=1>S (NP( (N Khé)) (AP (A chua)) (.)</td></tr><tr><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>S (NP (P Toi)) (NP (M 19) (N tuoi)) (.)</td><td rowspan=1 colspan=1>S (NP(P Toi)) (NP (N tuoi) (M 19)(..)</td></tr><tr><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>S (NP (P toi)) (VP (R khong) (V di)) (.)</td><td rowspan=1 colspan=1>S (NP (P toi) (VP (V di) (R khong)) (.)</td></tr><tr><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>S (NP (P toi)) (VP(R khng) (V choi)) (..)</td><td rowspan=1 colspan=1>S (NP (P toi) (VP (V choi) (R khong)) (.)</td></tr><tr><td rowspan=1 colspan=1>7</td><td rowspan=1 colspan=1>S (NP (P Toi))(VP ( thich) ) (NP (N mèo))) (..)</td><td rowspan=1 colspan=1>S (NP (P Toi)) (VP (N mèo) (V thích)) (.)</td></tr><tr><td rowspan=1 colspan=1>8</td><td rowspan=1 colspan=1>SQ (NP (P Ai)) (VP (V biét) (VP (V boi)) (? ?)</td><td rowspan=1 colspan=1>SQ (VP (V Biét) (VP (V boi) (NP (P ai)))) (? ?)</td></tr></table>

The idea behind data augmentation is to substitute words in a sentence to generate new data. The newly generated sentences maintain the same syntax and logical coherence, so the translation to VSL (Visual sign language) follows the same conversion rules. This ensures accurate translation while preserving semantic similarity, as evaluated in the experimental phase. We have observed that the semantic relationships between words in Wordnet align perfectly with the concept of data augmentation. Therefore, we propose a data augmentation method based on Wordnet.

The Wordnet semantic network is a lexical dataset that represents semantic relationships between words. Wordnet only captures semantic relationships and does not encompass phonetic or morphological relationships [19].

![](images/ef51b0f6747a5ebf51e6bc7a2e9903a7fedea9f891aa2b4eb0580391a18e991d.jpg)  
Figure 2: Hierarchical structure of Wordnet

Syntactic parsing provides us with the syntactic structure of a sentence. However, syntactic parsing only checks for grammatical correctness and does not verify semantic correctness. Take the sentence “cái bàn ăn con gà” (the table eats the chicken) as an example. If we analyze this sentence syntactically, we find that it is grammatically correct (“cái bàn” serves as the subject, “ăn” is the verb, and “con gà” functions as the object). However, it is evident that “cái bàn” cannot “ăn” “con gà”. Instead, if we replace it with “con chó ăn con gà” (the dog eats the chicken), it becomes more logical. So, how can we determine if “cái bàn” or “con chó” can “eat” “con gà”? - By using the hyponym-hypernym relationship in Wordnet. Let’s assume there is a heuristic that only “động vật” (animals) can perform the action of “ăn” (eating). Therefore, to check if an object can eat, we check if it is “động vật” by traversing its hypernyms. By traversing the hypernyms in reverse, we can easily determine that the “con chó” (dog) can perform the action of “ăn” (eating), whereas the “cái bàn” (table) cannot. Similarly, we can add semantic constraints to ensure semantic correctness in the sentence. This allows us to generate new sentences by replacing words with the same hypernym. The hierarchical structure with the keyword “con chó” is depicted in Figure 3.

![](images/e28258e6b888a1a475445c60a23de8d138b5543a78414c9bfbfc8bee72da47b5.jpg)  
Figure 3: The structure of the hypernyms - hyponyms for the keyword “con chó”

![](images/e7e51b21f94db9e296d5b29ace101bbee443dc9e64e3d4e6a1404ed2a70abb3a.jpg)  
Figure 4: Illustration of criteria using the Synset $E _ { i } ^ { j }$

In our problem, we use three criteria:

Sibling criterion: applied when all synset sets $S _ { i } ^ { j }$ when all synset sets contain sibling synsets (with the same synset and hypernym). Then the synset $\{ E _ { 1 } ^ { 1 } , E _ { 1 } ^ { 2 } , \ldots \}$ is selected as sibling synsets.

That is

$$
S V = \{ S _ { i } ^ { j k } / S _ { g } \in S _ { i } ^ { j } ( \forall j : 0 \leqslant j \leqslant n _ { i } ^ { j } ) , S _ { p } i s \_ h y p e r S _ { i } ^ { j k } \} .
$$

Parent-child criterion: applied when the synset sets $S _ { i } ^ { j }$ contain a synset that is superior to the remaining synsets (as long as each remaining synset has a synset that is a subordinate of the above-mentioned superior synset). Then the synset $\{ E _ { 1 } ^ { 1 } , E _ { 1 } ^ { 2 } , \ldots \}$ is selected as sibling synsets.

That is

$$
S V = \{ S _ { i } ^ { j k } / \exists S _ { p } \in S _ { i } ^ { h } ( h \in [ 1 . . . n _ { i } ^ { j } ] ) , S _ { i } ^ { j k } \in S _ { i } ^ { h } ) , ( \forall j : 0 \leqslant j \leqslant n _ { i } ^ { j } , j \neq h ) , S _ { p } i s _ { - } h y p e r S _ { i } ^ { j k } \} .
$$

Grandparent - grandchildren criterion: Applied when in synset sets $S _ { i } ^ { j }$ contain a synset that is superior to the remaining synsets (as long as each remaining synset has a synset that is a subordinate of the above-mentioned superior synset). Then the synset $\{ E _ { 1 } ^ { 1 } , E _ { 1 } ^ { 2 } , \ldots \}$ is selected as these subordinate synsets.

$S V = \{ S _ { i } ^ { j k } / \exists S _ { g } \in S _ { i } ^ { h } ( h \in [ 1 . . . n _ { i } ^ { j } ] ) , S _ { i } ^ { j k } \in S _ { i } ^ { j } )$ , $( \forall j : 0 \leqslant j \leqslant n _ { i } ^ { j } , j \neq h ) , S _ { g } i s _ { - } d i s t \_ h y p e r S _ { i } ^ { j k } ) \}$

Thus, when the word $W$ appears in a phrase, $W$ can be replaced with $W$ ’ if $W$ and $W$ ’ satisfy the sibling, parent-child, and grandparent-grandchild criteria. Therefore, depending on the structure of the hypernyms and hyponyms and other characteristics of Wordnet, we may construct fuzzy data by changing words in previous phrases according to predetermined criteria.

# 2.2. Data augmentation process

Based on the characteristics and properties of Wordnet for semantic constraints to verify semantic correctness in a sentence, we integrate it with the Vietnamese Wordnet dataset from the VLSP (association for Vietnamese language and speech processing) community. This dataset comprises 10,000 core vocabulary units, each containing information such as English translations, synonyms, antonyms in Vietnamese, and hypernym-hyponym structure [18]. The data augmentation algorithm is described in pseudocode as follows.

om there, we have the process of constructing new data through the following steps.

<table><tr><td>Algorithm: Data-augment-VSL</td></tr><tr><td>Input: Sentences S Output: Set of sentences S&#x27; are generated based on S 1: Split W word ∈ S 2: X ← W.hypernyms() 3: For i = 1, n do Xi ← X.hyponyms() Add Xi to set T</td></tr><tr><td>4: While ! Xi.hyponyms:</td></tr><tr><td>Yi ← Xi.hyponyms() Add Yi to set T 5: Replace replace each element in T, create new data S&#x27;</td></tr></table>

We proceed to construct new data based on a set of initially built data. Our data is evaluated by a community of individuals who are deaf and language experts in the field. Subsequently, we enrich the data using the proposed method.

Figure 5 illustrates the process of generating new data from an original sentence $S$ . The sentence “tôi ăn táo” (I eat an apple) is syntactically parsed, and the noun “táo” (apple) is extracted from the sentence. Applying the algorithm, a set $T$ is obtained, which consists of words that can be used as replacements to generate a new set of sentences, $S ^ { \prime }$ . This set $T$ includes 92 words (excluding the root word), resulting in the generation of 92 new sentences.

![](images/23f555f99cbdbd897f5142da492406f9db0c4e0baf12203190ee7153a81626d2.jpg)  
Figure 5: Example of generating new data from an original sentence

After experimenting with a set of data, it was observed that verb types, when using the method of searching for words with shared hypernyms based on sibling, parent-child, and grandparent-grandchild criteria, did not meet semantic requirements. Therefore, only pronouns, nouns, and adjectives were considered. Table 3 presents some sets $T$ and summarizes the number of enriched sentences generated by the proposed algorithm (where $T$ represents the set of words with shared hypernyms based on the applied criteria for each word type, WS represents the number of original data sentences containing a word from the word type being considered, and W’S represents the number of enriched sentences from all original sentences containing a word from the word type being considered).

In the initial dataset of 10,000 sentences, due to the chosen domain of communication, pronouns constitute a significant portion of the vocabulary. Additionally, the categorization of nouns and adjectives is derived from their hypernym groups. This ensures that the replacement of words to generate new sentences maintains semantic similarity.

The similarity of the dataset before and after augmentation can be evaluated based on the language model’s perplexity for each type. Perplexity is a measure used in probability and statistics to assess the effectiveness of a language model. In an $n$ -gram language model, perplexity measures the model’s ability to predict a new text segment based on the probability of n-grams in the model. Perplexity in an n-gram language model is calculated using the following formula

$$
P e r p l e x i t y ( W ) = \sqrt [ n ] { \frac { 1 } { P ( w _ { 1 } , w _ { 2 } , . . . , w _ { N } ) } }
$$

where, $N$ is the order of the $n$ -gram model; $P ( w _ { 1 } , w _ { 2 } , . . . , w _ { N } )$ is the probability of the test text segment in the $n$ -gram language model; $\sqrt [ n ] { \cdots }$ denotes taking the $N t h$ root, where $N$ is the number of words in the test text segment. This formula helps normalize perplexity to make it independent of the size of the text segment.

The smaller the perplexity, the better the model performs, indicating its ability to predict new word sequences. In $n$ -gram language models, perplexity is often used to compare different models and evaluate their effectiveness in language prediction [20]. The lowest perplexity reported was in 1992 on the Brown Corpus dataset (1 million words of American English across various topics and genres), with an actual value of approximately 247, corresponding to a cross-entropy of $\log _ { 2 } ( 2 4 7 ) = 7 . 9 5$ bits per word or 1.75 bits per character using a 3-gram model. Lower perplexity levels can often be achieved with more specialized datasets as they are easier to predict. The perplexity score of a dataset depends on various factors such as the size of the dataset, the complexity of the language structure, the vocabulary richness, and so on. In many cases, perplexity tends to increase with the size of the dataset, especially when the dataset size significantly grows. However, this increase does not always occur and can be limited by the complexity of the language structure or vocabulary richness. Table 4 presents the perplexity scores for the constructed datasets using a 3-gram language model, comparing them with some commonly used datasets.

Table 3: Perplexity scores of the datasets   

<table><tr><td rowspan=1 colspan=1>Dataset</td><td rowspan=1 colspan=1>fAverage perplexity score</td></tr><tr><td rowspan=1 colspan=1>WikiText-103</td><td rowspan=1 colspan=1>109-113</td></tr><tr><td rowspan=1 colspan=1>Penn Treebank</td><td rowspan=1 colspan=1>110-120</td></tr><tr><td rowspan=1 colspan=1>Common Craml</td><td rowspan=1 colspan=1>600-800</td></tr><tr><td rowspan=1 colspan=1>Vie-VSL10k</td><td rowspan=1 colspan=1>300-420</td></tr><tr><td rowspan=1 colspan=1>Vie-VSL10k</td><td rowspan=1 colspan=1>450-250</td></tr></table>

Thus, we can observe that despite the dataset is more than six times larger than the original one, the perplexity score is only slightly higher, by no more than 1.5 times. This indicates that the language model with a 3-gram approach performs well in terms of data efficiency. Additionally, the high similarity between the original and newly generated sentences, which preserves the syntactic structure, further supports this notion. In terms of semantics, the similarity is ensured by the hyponym relationship among words, as defined by the applied standards.

Table 4: Results of the data augmentation algorithm from Vie-VSL10K   

<table><tr><td rowspan=1 colspan=1>Lexical category</td><td rowspan=1 colspan=1>Group</td><td rowspan=1 colspan=1>Example</td><td rowspan=1 colspan=1>T</td><td rowspan=1 colspan=1>WS</td><td rowspan=1 colspan=1>W&#x27;S</td></tr><tr><td rowspan=13 colspan=1>Noun</td><td rowspan=1 colspan=1>Plant 1 (fruits)</td><td rowspan=1 colspan=1>Buoi, cam, nho, táo..(Pomelo, orange, grape, apple, etc.)</td><td rowspan=1 colspan=1>92</td><td rowspan=1 colspan=1>35</td><td rowspan=1 colspan=1>3220</td></tr><tr><td rowspan=1 colspan=1>Plant 2 (flowers)</td><td rowspan=1 colspan=1>Hoa cúc, hoa hòng, hoa ly,. ..(Chrysanthemum, rose, lily, etc.)</td><td rowspan=1 colspan=1>183</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>915</td></tr><tr><td rowspan=1 colspan=1>Plant 3 (general)</td><td rowspan=1 colspan=1>Cày, hoa, co, lá, rau,...(Tree, flower, grass, leaf, vegetable, etc.)</td><td rowspan=1 colspan=1>438</td><td rowspan=1 colspan=1>10</td><td rowspan=1 colspan=1>2628</td></tr><tr><td rowspan=1 colspan=1>Food</td><td rowspan=1 colspan=1>Bánh, keo, bia, thit, rau...(Cake, candy, beer, meat, vegetable, etc.)</td><td rowspan=1 colspan=1>471</td><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>1413</td></tr><tr><td rowspan=1 colspan=1>Animal 1 (pets)</td><td rowspan=1 colspan=1>chó, chó con, chó xiù, gà, mèo,.. .(Dog, puppy, poodle, chicken, cat, etc.)</td><td rowspan=1 colspan=1>25</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>125</td></tr><tr><td rowspan=1 colspan=1>Animal 2 (others)</td><td rowspan=1 colspan=1>Báo, ho, huou,..(Tiger, lion, giraffe, etc.)</td><td rowspan=1 colspan=1>708</td><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>2124</td></tr><tr><td rowspan=1 colspan=1>Object 1 (household items)</td><td rowspan=1 colspan=1>Bàn, ghé, t,..(Table, chair, cabinet, etc.)</td><td rowspan=1 colspan=1>257</td><td rowspan=1 colspan=1>11</td><td rowspan=1 colspan=1>2827</td></tr><tr><td rowspan=1 colspan=1>Object 2 (tools)</td><td rowspan=1 colspan=1>Buá, kéo, máy,..(Hammer, scissors, machine, etc.)</td><td rowspan=1 colspan=1>1564</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>5056</td></tr><tr><td rowspan=1 colspan=1>Object 3 (vehicles)</td><td rowspan=1 colspan=1>Xe máy, ō to, xe cho hàng, …(Motorcycle, car, truck, etc.)</td><td rowspan=1 colspan=1>78</td><td rowspan=1 colspan=1>7</td><td rowspan=1 colspan=1>546</td></tr><tr><td rowspan=1 colspan=1>Weather</td><td rowspan=1 colspan=1>Náng, mua, gio,..(Sun, rain, wind, etc.)</td><td rowspan=1 colspan=1>63</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>315</td></tr><tr><td rowspan=1 colspan=1>Occupation</td><td rowspan=1 colspan=1>Giáo vièn, còng nhan,...(Teacher, worker, etc.)</td><td rowspan=1 colspan=1>21</td><td rowspan=1 colspan=1>8</td><td rowspan=1 colspan=1>168</td></tr><tr><td rowspan=1 colspan=1>Body parts</td><td rowspan=1 colspan=1>Chàn, tay, tóc, má, moi,...(Leg, arm, hair, cheek, lips, etc.)</td><td rowspan=1 colspan=1>231</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>924</td></tr><tr><td rowspan=1 colspan=1>Geometric shapes</td><td rowspan=1 colspan=1>Tam giác, hinh tròn, hinh vuòng,...(Triangle, circle, square, etc.)</td><td rowspan=1 colspan=1>134</td><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>402</td></tr><tr><td rowspan=5 colspan=1>Adjective</td><td rowspan=1 colspan=1>Color</td><td rowspan=1 colspan=1>Dǒ, xanh, vàng, tím,...(Red, green, yellow, purple, etc.)</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>432</td></tr><tr><td rowspan=1 colspan=1>Material property</td><td rowspan=1 colspan=1>Nǎng, nhe, Cúng, mèm,. . .(Heavy, light, hard, soft, etc.)</td><td rowspan=1 colspan=1>45</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>90</td></tr><tr><td rowspan=1 colspan=1>Size</td><td rowspan=1 colspan=1>To, rong, dài, ngán,. . .(Big, wide, long, short, etc.)</td><td rowspan=1 colspan=1>15</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>60</td></tr><tr><td rowspan=1 colspan=1>Emotions</td><td rowspan=1 colspan=1>vui, buòn, lo láng,. ..(Happy, sad, worried, etc.)</td><td rowspan=1 colspan=1>279</td><td rowspan=1 colspan=1>7</td><td rowspan=1 colspan=1>1953</td></tr><tr><td rowspan=1 colspan=1>Personality</td><td rowspan=1 colspan=1>hài huóc, cuc càn, dě thuong...(Funny, grumpy, adorable, etc.)</td><td rowspan=1 colspan=1>23</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>92</td></tr><tr><td rowspan=1 colspan=1>Pronoun</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Toi, ho, chúng ta, …(I, they, we, etc.)</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>3424</td><td rowspan=1 colspan=1>41088</td></tr><tr><td rowspan=1 colspan=5>Total:</td><td rowspan=1 colspan=1>64378</td></tr></table>

# 3. STATE-OF-THE-ART MACHINE TRANSLATION MODELS FOR VSL

# 3.1. Sequence to sequence model

The “sequence to sequence” (Seq2Seq) model is one of the successful models in the field of natural language processing [21]. This model offers several advantages, including its applicability to various tasks, especially in addressing natural language processing problems such as machine translation, text summarization, question answering, and many other applications. It possesses the capability to learn transformations from training data: Seq2Seq enables the learning of converting one type of data into another type. It is easily scalable, allowing the handling of input and output data of different sizes. Seq2Seq exhibits high accuracy, generating precise and natural outputs, particularly in machine translation and text summarization tasks. Furthermore, it can be combined with other models, such as the attention model, to enhance performance and accuracy. Therefore, for the translation of Vietnamese sentences into grammatically correct VSL sentences, utilizing the Seq2Seq model in combination with attention is a feasible approach. The Seq2Seq model consists of two main components: the encoder and the decoder. In the encoder, the input sentence, which is in Vietnamese, is transformed into a semantic vector using an LSTM model to encode information from each word in the sentence. In the decoder, the semantic vector is fed into the model to decode and generate the corresponding VSL output sentence using another LSTM model. The encoder and decoder components of the Seq2Seq model are illustrated in Figure 6.

![](images/b108696fad70deb7bf9ee267b729815ee88513d0cad5ec12b610078f74acb0b9.jpg)  
Figure 6: The encoder-decoder architecture of the Seq2Seq model in the Vietnamese-VSL translation task.

At each time step, the output of the decoder is combined with the weighted sum over the encoded input to predict the next word in the sentence. The decoder utilizes selective attention over parts of the input sequence. Attention takes a sequence of vectors as input and returns an attention vector. To train the Seq2Seq model, we need to use the input and output data in the form of parallel sentence pairs. In this case, with 60,000 sentence pairs, we used a simple yet effective Seq2Seq model with the following basic parameters:

• Batch size: 128;   
• Number of epochs: 10;   
• Learning rate: 0.001-0.01;   
• Model architecture: LSTM with 3 hidden layers with a dimension of 256;   
• Training time: 4.5 hours with a training speed on CPU of approximately 30-40 samples/second;   
• GPU: NVIDIA Tesla T4.

# 3.2. Transformer model

The transformer is a recent and well-known model in the natural language processing community that has made significant breakthroughs in machine translation tasks since its introduction in 2017 [22]. With the ability to leverage the parallel computing power of GPUs to accelerate training speed for language models and overcome the issue of handling long sentences, the transformer model is considered suitable for the automatic VSL translation task. The initial steps in applying this model to the task include data encoding and decoding, applying the translation model, and evaluating the effectiveness of the translations.

# A. Encoding and Decoding

First, the data needs to be transformed into a numerical representation. Typically, the text is converted into an encoded sequence, which is used as input to create an embedding. The training data consists of two tokenized forms of text, one for regular Vietnamese and one for VSL. Both employ similar methods. The encoding process converts a series of sentences into tokens. The decoding process converts these tokens back into human-readable text.

• Setting up the input pipeline: To construct a suitable input pipeline for training, some transformations need to be applied to the dataset. The following function will be used to encode batches of raw text.

def tokenize_pairs(vsl, vi): vi $=$ tokenizers.pt.tokenize(vi) # Convert from ragged to dense, padding with zeros. vsl $=$ vsl.to_tensor() vi $=$ tokenizers.vi.tokenize(vi) # Convert from ragged to dense, padding with zeros. vi $=$ vi.to_tensor() return vsl, vi

Positional Encoding: Attention layers treat the input as a set of unordered vectors. This model does not contain any recurrent layers. Therefore, a “positional encoding” is added to provide the model with information about the relative positions of tokens within a sentence. The positional encoding vector is added to the embedding vector. The embedding vector represents a token in a $d$ -dimensional space, where tokens with similar meanings are closer to each other. However, the embedding does not encode the relative positions of tokens within a sentence. Hence, after adding positional encoding, the tokens will be closer based on both their semantic similarity and their positions within the sentence, in the d-dimensional space. The formula for calculating the positional encoding is as follows

$$
\begin{array} { r } { P E _ { ( p o s , 2 i ) } = \sin ( p o s / 1 0 0 0 ^ { 2 i / d _ { m o d e l } } ) , } \\ { P E _ { ( p o s , 2 i + 1 ) } = \cos ( p o s / 1 0 0 0 ^ { 2 i / d _ { m o d e l } } ) } \end{array}
$$

• Look-ahead mask is used to hide future tokens in a sequence. In other words, the mask indicates which entries should not be used. This means that to predict the third token, only

the first and second tokens will be used. Similarly, to predict the fourth token, only the first, second, and third tokens will be used, and so on.

$\bullet$ The attention function used by the Transformer has three inputs: $Q$ (query), $K$ (key), and $V$ (value). The equation used for computation is as follows

$$
\begin{array} { r } { A t t e n t i o n ( Q , K , V ) = s o f t m a x _ { k } ( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } ) V . } \end{array}
$$

During the softmax normalization process applied to $K$ , its values determine the level of importance for $Q$ . The output represents the weighted sum of attention weights and the $V$ (value) vector. This ensures that tokens of interest are preserved while irrelevant tokens are discarded.

# $B$ . Initializing the transformer model

The transformer consists of an encoder, a decoder, and a final linear layer. The output of the decoder serves as the input to the linear layer, and its output is returned.

• Setting hyperparameters.

• Optimization algorithm: Using the Adam optimization algorithm with customized learning rate scheduling (Adam is an extension of stochastic gradient descent that has been widely adopted for deep learning applications in computer vision and natural language processing) [23].

• Training and testing

Transformer $=$ Transformer( num_layers $=$ num_layers, d_model ${ } = { }$ d_model, num_heads ${ } = { }$ num_heads, dff $=$ dff, input_vocab_size $=$ tokenizers.pt.get_vocab_size().numpy(), target_vocab_size $=$ tokenizers.en.get_vocab_size().numpy(), pe_input $= 1 0 0 0$ , pe_target $= 1 0 0 0$ , rate $=$ dropout_rate)

Next, create a checkpoint path and a checkpoint manager to save checkpoints after every n epochs. The regular Vietnamese sentence is used as the input language, and VSL is the target language.

• The following steps are used for inference:

- Encode the input sentence using the Vietnamese tokenizer (tokenizers.vie). This serves as the input to the encoder.

- Initialize the input to the decoder as a token (Start).

- Compute the padding masks and look-ahead masks.

- The decoder then makes predictions by looking at the output of the encoder and its own output (self-attention).

- Concatenate the predicted tokens with the input to the decoder and pass it through the decoder. In this approach, the decoder predicts the next token based on the previously predicted tokens.

• Display attention: The translator class returns a dictionary mapping that can be used to visualize the inner workings of the model.

![](images/d74278c413973bc82382f5b36f8dcafb552a4bcf44614bfad75ed28dd3e7fdc8.jpg)  
Figure 7: Attention map

Training time and environment:

- Training time: Approximately 8 hours with 30 epochs.   
- Training environment: Configured with a Tesla T4 GPU and 16GB RAM.   
- Batch size: 64.   
- Number of layers in the model: 6.   
- Number of heads in multi-head attention: 8.   
- Embedding size: 512.   
- Dimensionality of the Encoder and Decoder: 512.

# 4. EVALUATION OF RESULTS

With the parameters applied to the transformer translation model presented above, it is considered quite good and suitable for handling translation data with around 60,000 bilingual sentence pairs. The training time of 8 hours with 30 epochs is notably reasonable. The training environment on a Google Colab virtual machine with a Tesla T4 GPU and 16GB RAM is powerful and suitable for model training. A batch size of 64 is a suitable choice given the amount of data and other model parameters. The number of layers in the model of 6 and the number of heads in the multi-head attention of 8 are also appropriate and noteworthy parameters. The embedding size of 512 and the dimensionality of the encoder and decoder of 512 are common and suitable choices to achieve good results for the Transformer translation model. The Seq2seq model with the given parameters, including batch size ranging from 64 to 128, the number of epochs from 30 to 50, and learning rate from 0.001 to 0.01, along with the LSTM architecture consisting of 3 hidden layers with a hidden dimension of 256, has achieved good performance in the Vietnamese-VSL machine translation task. Due to the relatively low complexity of the input data and the high similarity between the two languages, the training time is better compared to other language pairs. Furthermore, to evaluate the experimental performance, we rely on the BLEU score is used to assess the data enrichment on a new dataset compared to the original dataset using various machine translation models. BLEU is a method for evaluating the quality of automatically generated machine translations, originally proposed by IBM and widely used as a primary evaluation metric in machine translation research [24].

Table 5: Comparison of BLEU scores on models training with the original data and augmented data   

<table><tr><td rowspan=1 colspan=1>No</td><td rowspan=1 colspan=1>Translation model</td><td rowspan=1 colspan=1>Original data</td><td rowspan=1 colspan=1>Augmented data</td></tr><tr><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>Rule-based translation</td><td rowspan=1 colspan=1>68.02</td><td rowspan=1 colspan=1>68.02</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>Seq2Seq</td><td rowspan=1 colspan=1>58.5</td><td rowspan=1 colspan=1>81.44</td></tr><tr><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>65.2</td><td rowspan=1 colspan=1>89.23</td></tr></table>

Note: BLEU scores range from 0 to 100, with higher scores indicating better translation quality. The augmented data shows improved BLEU scores across all models, indicating better translation performance compared to the original data. Through the experimentatal process with the mentioned models, we can observe that with a training dataset of 10,000 sentence pairs, rule-based translation yields higher BLEU scores compared to statistical models. However, as the dataset size increases, the performance of statistical models gradually improves. Among the statistical models used in our research, the Transformer model consistently provides better results. However, it is worth noting that the BLEU score is appropriate for evaluation, but may not hold significant value when it comes to sign language translation or other specific language translation tasks. For example, the German sign language translation achieves an 82.87 BLEU score [25], and the Thai sign language translation [26], indicates the need for domain-specific evaluation metrics in such cases.

# 5. CONCLUSION

In this paper, we have addressed the challenges of the Vietnamese sign language translation problem. We proposed a simple and effective method for data augmentation based on Wordnet. The results showed that the augmented data increased sixfold while the perplexity score only increased by up to 1.5 times. This indicates that the language model with a 3-gram approach performs well in capturing semantic similarity. With the available data, we applied modern translation models such as Seq2Seq with attention and the transformer model to experiment with this data. The best achieved BLEU score is 89.23, which is for the transformer model using 60,000 bilingual sentence pairs for training data, outperforming other baseline methods. We observed that the transformer model with a pretrained model can be used effectively even with a small amount of training data, allowing us to apply various techniques designed for the transformer. The higher BLEU score compared to other language translation models is due to the unique characteristics of sign language translation. However, this score is not surprisingly high compared to other sign language translation tasks.

# REFERENCES

[1] S. W and Lillo-Martin, “Sign language and linguistic universals,” Linguist, pp. 738–742, 2006.

[2] A. Othman and M. Jemni, “Statistical sign language machine translation from Englishwritten text to American sign language gloss,” IJCSI International Journal of Computer Science Issues, vol. 8, no. 3, 2021.

[3] Q. LD, Duong-Trung, N. Vu, and A. Nguyen, “Recommending the workflow of vietnamese sign language translation via a comparison of several classification algorithms,” Computational Linguistics, Communications in Computer and Information Science, vol. 1215, 2020.   
[4] S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, and M. Tutt, “Tessa - a system to aid communication with deaf people,” 2002.   
[5] J. A. Bangham, S. J. Cox, R. Elliot, J. R. W. Glauert, I. Marshall, S. Rankov, , and M. Wells, “Virtual signing: Capture, animation, storage and transmission – an overview of the visicast project,” IEEE Seminar on Speech and Language Processing for Disabled and Elderly People, 2000.   
[6] A. Grieve-Smith, “SignSynth: A sign language synthesis application using Web3D and perl,” in Revised Papers from the International Gesture Workshop on Gesture and Sign Languages in Human-Computer Interaction, 2002.   
[7] B. Krieg-Br¨uckner, J. Peleska, E.-R. Olderog, and A. Baer, “The uniform workbench - a universal development environment for formal methods,” Lecture Notes in Computer Science 1709, 1999.   
[8] L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, and M. Palmer, “A machine translation system from English to American sign language,” Envisioning Machine Translation in the Information Future, vol. 1934, pp. 191–193, 2000.   
[9] A. Chintha, I. Nwogu, and S. Sah, “Deep learning methods for sign language translation,” ACM Transactions on Accessible Computing, vol. 14, pp. 1–30, 2021.   
[10] G. S. Mishra, A. K. Sahoo, and K. K. Ravulakollu, “Word based statistical machine translation from English text to indian sign language,” ARPN Journal of Engineering and Applied Sciences, vol. 12, no. 2, pp. 481–488, 2017.   
[11] G. Angelova, E. Avramidis, and S. M¨oller, “Using neural machine translation methods for sign language translation,” in 60th Annual Meeting of the Association for Computational Linguistics Student Research Workshop, 2022, pp. 273–284.   
[12] K. H., H. M., E. S., P. K., M. K., and W. M., “Regression analysis of demographic and technologyexperience factors influencing acceptance of sign language animation,” ACM Transactions on Accessible Computing, vol. 10, no. 1, pp. 1–33, 2017.   
[13] B. D, K. H, and et al, “Sign language recognition, generation, and translation: An interdisciplinary perspective,” in The 21st International ACM SIGACCESS Conference on Computers and Accessibility - ASSETS’19. ACM Press, 2019, pp. 16–31.   
[14] Q. L. Da and N. C. N, “Conversion of the vietnamese grammar into sign language structure using the example-based machine translation algorithm,” in International Conference on Advanced Technologies for Communications, 2018, pp. 27–31.   
[15] Q. L. Da and et al, “Converting the Vietnamese television news into 3D sign language animations for the deaf,” Lecture Notes of the Institute for Computer Sciences - Social Informatics and Telecommunications Engineering, vol. 257, 2019.   
[16] T.-B.-D. Nguyen, T.-N. Phung, and T.-T. Vu, “Some issues on syntax transformation in Vietnamese sign language translation,” Sign Language Studies. IJCSNS International Journal of Computer Science and Network Security, vol. 17, no. 5, pp. 292–297, 2017.   
[17] —, “A rule-based method for text shortening in Vietnamese sign language translation,” in International Conference on Advanced Technologies for Communications, 2018, pp. 655–662.   
[18] T.-B. Ho, P.-T. Nguyen, and et al, “VLSP research topic,” in https://vlsp.hpda.vn/, 2022.   
[19] Soergel and Dagobert, “Wordnet. an electronic lexical database,” 10 1998.   
[20] J. F and M. R. L, “Interpolated estimation of Markov source parameters from sparse data,” in Proceedings of The Workshop on Speech and Natural Language, Association for Computational Linguistics, 1980, pp. 357–366.   
[21] S. I., V. O., and L. Q. V, “Sequence to sequence learning with neural networks,” in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.   
[22] L. Jones, A. N. Gomez, and Lukasz Kaiser, “Attention is all you need,” in 31st Conference on Neural Information Processing Systems USA, 2017.   
[23] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,” in International Conference on Learning Representations, 2015.   
[24] P. K., R. S., W. T., and Z. J, “Bleu: A method for automatic evaluation of machine translation,” in Proceedings of the 20th Annual Meeting of the Association for Computational Linguistics (ACL), 2001, pp. 311–318.   
[25] K. Yin and J. Read, “Better sign language translation with STMC-transformer,” in Proceedings of the 28th International Conference on Computational Linguistics, 2020.   
[26] D. S, N. K, Cercone, and S. B, “Intelligent Thai text – Thai sign translation for language learning,” Computers Education, vol. 51, no. 1, pp. 1125–1141, 2008.

Received on March 25, 2023   
Accepted on May 11, 2023