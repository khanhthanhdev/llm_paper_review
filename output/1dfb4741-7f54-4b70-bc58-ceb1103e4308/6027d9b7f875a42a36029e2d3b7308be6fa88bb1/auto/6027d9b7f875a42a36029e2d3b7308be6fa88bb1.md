# MagicPIG: LSH Sampling for Efficient LLM Generation

Zhuoming Chen $^ \dagger$ , Ranajoy Sadhukhan $^ \dagger$ , Zihao Ye $^ \ddagger$ , Yang Zhou $^ \dagger$ , Jianyu Zhang $\ S \sharp$ , Niklas Nolte $\sharp$ , Yuandong Tian $\sharp$ , Matthijs Douze $\sharp$ , Leon Bottou $\ S \sharp$ , Zhihao Jia $^ \dagger$ , Beidi Chen†

†Carnegie Mellon University, $^ \ddag$ University of Washington, $\ S$ New York University, $\sharp$ Meta AI

Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $) \times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.

Github: https://github.com/Infini-AI-Lab/MagicPIG Website: https://www.lsh-ai.com

# 1 Introduction

Large language models (LLMs) with long context windows, such as GPT (Achiam et al., 2023), Llama (Dubey et al., 2024), and Gemini (Team et al., 2023), have gained significant attention for their ability to enhance applications like chatbots (Chiang et al., 2024), search engines (Wang et al., 2024), and video analysis (Cheng et al., 2024). However, serving long-context LLMs is highly challenging due to the unique bottleneck in auto-regressive generation—the key-value (KV) cache, which stores intermediate attention keys and values to avoid re-computation (Pope et al., 2022; Zhang et al., 2023b). Specifically, the KV cache grows linearly with both the batch size and sequence length, occupying substantial GPU memory and increasing decoding time. Moreover, the KV cache makes LLM generation extremely memory-bound, leading to underutilization of GPU computational power. For instance, an NVIDIA A100-40GB GPU can only handle a single request for Llama with a 128k context length, with nearly half of the decoding time spent accessing the KV cache, and poor GPU utilization (He and Zhai, 2024).

![](images/3b43cb44f90189700fba5339ca85a1245259ed0eb102cdb886a9a184cf437b3d.jpg)  
Figure 1 While TopK attention performs well on retrieval tasks (niah) where the useful information reduces to a few words, it degrades severely in aggregated tasks like word extraction (cwe, fwe). x-axis: proportion of attention keys used for TopK attention.

Leveraging the common insight that attention is naturally sparse, dynamic sparse or TopK-based approximation has been extensively studied (Tang et al., 2024; Singhania et al., 2024; Zhang et al., 2024; Wu et al., 2024), but three major challenges prevent a wide adoption in LLM serving systems. (1) Quality Degradation. They usually propose various strategies to approximate a subset of KV cache that yields the highest attention scores. However, TopK attention itself is a biased attention approximation and lacks theoretical guarantees. Figure 1 shows that even exact TopK attention results significantly degrade the accuracy of certain downstream tasks. (2) High Overhead. There is a large overhead to identify TopK attention, which becomes the bottleneck rather than the attention computation. For example, as studied in Liu et al. (2024a), naively applying a search algorithm like IVF (Douze et al., 2024) requires access over 30% key states to obtain the exact TopK, showing an unsatisfying trade-off between search accuracy and cost. (3) No Memory Saving. Although saving KV cache loading time, they cannot reduce the total memory occupied by the KV cache, which limits the maximum context and batch sizes when VRAM is scarce.

![](images/8b8e30581e8f7324a784bb9f8e6a2b8623e0d4d5ef972752b8430502a8b88d11.jpg)  
Figure 2 Left: Examples of long-tailed distribution in LLM. The x-axis is the fraction (or number of tokens) used in the TopK, a.k.a. the sampling budget. Mid: Sink tokens make attention score look sparser. Right: The geometry of attention. The key of attention sink $k _ { s i n k }$ is almost opposite to other tokens, and its orientation is surprisingly invariant with input tokens. Query states lie close to $k _ { 0 }$ , thus forming attention sink and Figure 2b. $k$ usually lies in a narrow cone that is far away from $q$ . In certain heads, this geometry will result in a long-tailed distribution of attention score and difficulty searching for the TopK keys.

An ideal sparse attention approximation approach should (1) preserve full accuracy for a diverse set of downstream tasks with guarantees, (2) involve low-cost overhead for KV cache selection, and (3) save GPU memory. The following observations, together with the performance drop shown in Figure 1 suggest that to achieve such demanding requirements, we need to go beyond TopK attention:

• Attention is not always sparse. Contradictory to previous belief (Zhang et al., 2023b, 2024; Tang et al., 2024; Liu et al., 2024a), we observe that attention is not always sparse, especially for tasks that leverage the full context. As shown in Figure 2a, in some layers, attention distribution can be very long-tailed, i.e., the Top20% attention can only cover 70% of the total attention scores.   
• Seemingly high sparsity is usually a consequence of an attention sink. Most of the attention scores concentrate on initial tokens (attention sink phenomenon) (Xiao et al., 2023), making the distribution look sparser. However, as shown in Figure 2b, attention scores are distributed more uniformly among tokens except for the sink. According to the geometrical interpretation of sink, keys, and queries shown in Figure 2c, the attention sink, which we found surprisingly almost static regardless of the input token, is just for imposing sparsity on the attention distribution.   
• It is hard to find TopK attention. Figure 2c also shows why searching for the Top-K keys is intrinsically costly. The keys and queries usually lie within two narrow cones with nearly opposite orientations, except for the attention sink. This significant mismatch between query and data distributions causes nearest-neighbor search methods to perform poorly.

These limitations of TopK attention require rethinking the sparse attention approximation. Rather than only using the keys and values with the highest scores, leveraging information on the distribution can make the estimation more accurate. We approach this as a bias correction problem in sampling. Unbiased and efficient sampling has been long studied in biology (Lukacs, 2009), sociology (Chen et al., 2018) as well as machine learning (Backurs et al., 2019; Chen et al., 2019; Zandieh et al., 2023), with theoretical guarantees.

Figure 3 shows that sampling values according to their corresponding attention score (we call this oracle sampling) achieves a much lower (up to 4 $\times$ ) estimation error than the naive TopK selection. Deploying sampling estimation in attention is promising, but three challenges remain. First, how a reduction of the attention error can make a difference in downstream performance is unclear (Backurs et al., 2019, 2018). Second, modeling the attention score distribution is necessary for efficient sampling, but inferring the distribution parameters requires expensive computations. Third, fully leveraging the resources of modern hardware, GPU and CPU, with a theoretically efficient algorithm is non-trivial.

This paper proposes Magic samPlIng for Generation (MagicPIG), which leverages Locality sensitive hashing (LSH) sampling for efficient LLM generation. LSH is employed for sampling to approximate the attention score distribution and estimate attention output. By computing hash functions on GPU and conducting sampling on CPU, MagicPIG can allow massive hash tables and hash functions compared to prior work (Kitaev et al., 2020; Chen et al., 2021), which are of vital importance for accurate estimation (Backurs et al., 2018). Following the practice of Aminabadi et al. (2022); He and Zhai (2024), we offload the KV cache computation, which is memory bound, to CPU to allow a larger batch or longer context. Specifically,

![](images/30b3482f040f9cbe08e97b117435e1e1501caabe2bd7fbfb6f074c6f1692adec.jpg)  
Figure 3 TopK v.s. Sampling, 16k total context

• In Section 3, we analyze the failures of TopK attention. Moreover, we study sampling-based attention estimation assuming an oracle for the key distribution (Oracle Sampling Estimation) and empirically demonstrate that it is consistently more effective both for distribution estimation and downstream tasks. • In Sections 4.1 to 4.3, we present a sampling algorithm to approximate oracle sampling for attention estimation based on locality sensitive hashing and the intuition and motivation from statistic perspectives. To our best knowledge, MagicPIG is the first to leverage LSH sampling in self-attention in decoder-only LLM generation. • In Section 4.4, we present our system design to efficiently offload attention computation on the CPU, breaking the memory limit of the GPU for serving larger batches or longer contexts. We also overcome the new challenges of computation and memory size raised by our sampling algorithm to support a larger scale of hashing tables beyond prior work (Chen et al., 2021; Kitaev et al., 2020).

In Section 5, we show the empirical evaluation results of the performance of MagicPIG, demonstrating the accuracy and efficiency. While maintaining high accuracy for diverse tasks, MagicPIG can improve serving throughput by $1 . 5 \sim 5 \times$ (A100, L20, RTX 4090) and can achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct (Dubey et al., 2024) with 96K context. More importantly, we show that MagicPIG already outperforms TopK attention in the two aggregation tasks in Figure 1, suggesting that sampling indeed goes beyond TopK attention.

# 2 Background

In this section, we formulate the targeted attention estimation problem and related works.

# 2.1 Problem formulation

In LLM decoding phase, self-attention part calculates a weighted average of previous values by

$$
o = \mathrm { S o f t m a x } ( \frac { q K ^ { T } } { \sqrt { d } } ) V = w V \quad q \in \mathbb { R } ^ { 1 \times d } \quad K , V \in \mathbb { R } ^ { n \times d } \quad w \in \mathbb { R } ^ { 1 \times n }
$$

where $d$ is the head dimension and is the context size. $K = [ k _ { 1 } , k _ { 2 } , . . . , k _ { n } ] , V = [ v _ { 1 } , v _ { 2 } , . . . , v _ { n } ] , k _ { i } , v _ { i } \in \mathbb { R } ^ { 1 \times d }$ is KV cache. Normalized attention weighOur target is to find sampling matrix $\begin{array} { r } { w = \mathrm { S o f t m a x } ( \frac { q K ^ { T } } { \sqrt { d } } ) \in \mathbb { R } ^ { 1 \times n } } \end{array}$ is rix ttention (score) distribution. which minimize $\Pi \in \mathbb { R } ^ { n \times m }$ $D \in \mathbb { R } ^ { m \times m }$

$$
\delta = | | w V - w \Pi D \Pi ^ { T } V | |
$$

where $m \ll n$ is computation budget. For TopK attention, suppose $w _ { r _ { 1 } } > . . . > w _ { r _ { m } } > . . . > w _ { r _ { n } }$ , then

$$
\Pi _ { i , j } = { \left\{ \begin{array} { l l } { 1 , } & { { \mathrm { i f ~ } } i = r _ { j } , } \\ { 0 , } & { { \mathrm { o t h e r w i s e . } } } \end{array} \right. } D _ { i i } = { \frac { 1 } { \sum _ { i = 1 } ^ { m } w _ { r _ { i } } } }
$$

# 2.2 Related works

Efficient Attention. Attention approximation has been long studied. Reformer (Kitaev et al., 2020), KDEformer (Zandieh et al., 2023) and ScatterBrain (Chen et al., 2021) tackle the problem via locality sensitive hashing. These methods work in training and encoder models like BigGAN (Brock et al., 2019). Theoretically, the error bounds and minimal workload required are continuously improved (Brand et al., 2023; Alman and Song, 2023) but have not proven to be practical for wall-clock acceleration in LLM decoding. Besides, flashattention (Dao et al., 2022b; Dao, 2023; Dao et al., 2022a), flash-decoding (Ye et al., 2024; Hong et al., 2024) and SlimAttention (He et al., 2024) losslessly accelerate scaled product attention operator by maximizing the utilization of hardware, which is orthogonal to our approach.

Locality sensitive hashing. Locality sensitive hashing (LSH) (Backurs et al., 2019, 2018) is a family of hashing functions which assigns the same hash codes for similar inputs with higher probability than others (Chen et al., 2020b; Jafari et al., 2021). LSH uses two hyper-parameters, $( K , L )$ . $L$ hash tables are independently built. Each hash table has its own function $H$ which projects a high-dimension vector to an integer by concatenating $K$ random independent hash functions. In the sampling process, all vectors that share hash codes in at least one hash table with a query will be collected. SimHash (Charikar, 2002) is the LSH family based on cosine similarity. For a vector $x \in \mathbb { R } ^ { d }$ , SimHash generates a random hyperplane $w$ and returns $\mathrm { S i g n } ( w ^ { T } x )$ . Vectors share the same sign if and only if the random projection is not in between them. For a random projection, all angles are equally likely, thus the probability that two vectors $x$ , $y$ share the same sign is $\begin{array} { r } { p = 1 - \frac { \theta } { \pi } } \end{array}$ , where $\begin{array} { r } { \theta = \operatorname { a r c c o s } \frac { x y ^ { T } } { | | x | | \cdot | | y | | } } \end{array}$ . If we have $L$ hash tables each with $K$ random hash functions, the probability of $y$ to be retrieved by query $x$ is $1 - ( 1 - p ^ { K } ) ^ { L }$ .

KV Cache reduction. To get rid of memory bound introduced by KV cache thus enabling a larger batch size or serving a longer prompt, many methods are proposed to reduce the volume of KV cache. For example, $\mathrm { H _ { 2 } }$ O (Zhang et al., 2023b), SnapKV (Li et al., 2024) and Keyformer (Adnan et al., 2024) calculate heuristics during the prefilling phase to decide which tokens to preserve for decoding phase. Quest (Tang et al., 2024) and Loki (Singhania et al., 2024) do not evict KV cache but apply dynamic sparsity to reduce KV Cache loading at inference time. Besides the reduction along the dimension of sequence length, methods like KIVI (Liu et al., 2024b) and QServe (Lin et al., 2024) reduce the size of KV Cache by quantization.

# 3 Rethinking attention sparsity

In this section, we examine TopK attention, which is the theoretical upper bound of prior search-based algorithms, including both static methods (Zhang et al., 2023b; Li et al., 2024) and dynamic methods (Tang et al., 2024; Singhania et al., 2024; Mao et al., 2024). We show that TopK is sub-optimal and present another attention approximation based on sampling and estimation with an oracle that improves the accuracy and/or the computation cost.

# 3.1 Achilles’ heel of TopK attention

As it is defined, TopK attention only computes the weighted average on elements with the highest attention scores. To quantify its performance, the computation budget of TopK attention is defined as the number of selected tokens, i.e., the K of TopK. Searching-based sparse attention algorithms, like (Tang et al., 2024; Singhania et al., 2024; Wu et al., 2024), are approximations for TopK attention by replacing the true TopK keys with the ones found by approximate searching algorithms.

![](images/a9f0d7f5a4ffa0e874b2511bdc4a960b239f7a5338e95632e384c8e64723e3f8.jpg)  
Figure 4 TopK estimation error for a KV-cache of 16k tokens.

However, we find significant performance degradation in downstream tasks caused by TopK attention as shown in Figure 1. Although TopK attention preserves accuracy for retrieval tasks that only require a minimal subset of the context (needle-in-a-haystack single/multikey (Hsieh et al., 2024)), it severely degrades for aggregation tasks that leverage the full context (common word extraction and frequent word extraction (Hsieh et al., 2024)). Intuitively, the information is distributed more broadly for aggregation tasks, which results in less peak attention score distribution.

![](images/d2782fe1b19124ff423c9d89014f4073916a886c6fcf3a5fde4919d90aa9de2f.jpg)  
Figure 5 Geometric information of attention. Left: With arbitrary input, the orientation of $k _ { s i n k }$ almost remains the same, with a minimum similarity $> 0 . 9 9$ across sampled inputs. Mid: The orientation of $k _ { a v g }$ is stable across various input sentences with a similarity $> 0 . 9$ observed. Right: $k _ { s i n k }$ and $k _ { a v g }$ are almost opposite with similarity between $- 0 . 9 \sim - 0 . 8$ .

TopK attention is biased and inaccurate, especially when the distribution of attention scores is long-tailed and the computation budget or density (i.e., $K$ ) is limited. Unfortunately, long-tailed phenomena do occur in LLMs across all layers (prior works (Xiao et al., 2023; Tang et al., 2024; Sun et al., 2024) usually skip the first two layers to maintain accuracy) as presented in Figure 2a. Top20% tokens can only cover $7 0 \sim 8 0 \%$ attention scores, leaving a large proportion of keys and values not considered, which is translated into a non-negligible $1 5 \sim 2 0 \%$ ) estimation error in Figure 4.

To better understand the attention distribution, we study the geometry of $q , k$ and make the following three observations. (1) Key states of the initial token (also known as attention sink, denoted by $k _ { s i n k }$ ) remain almost the same for arbitrary input. In Figure 5a, we randomly draw 32 samples from the vocabulary and measure the mutual cosine similarity of key states. Surprisingly, we find that the orientations of the key states of different input tokens are almost identical with a similarity $> 0 . 9 9$ . (2) The orientation of the center of key states (i.e. $\begin{array} { r } { k _ { a v g } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } k _ { i } ) } \end{array}$ remains stable for different input sentences. In Figure 5b, we measure the mutual cosine similarity of $k _ { a v g }$ of 50 different input sentences. Although variance exists, the similarity of $k _ { a v g }$ is over 0.9. (3) The orientations of $k _ { a v g }$ and $k _ { s i n k }$ are almost opposite. In Figure 5c, we find that for each head, $k _ { s i n k }$ and $k _ { a v g }$ has a cosine similarity between $- 0 . 9 \sim - 0 . 8$ .

These observations shape the geometry as shown in Figure 2c. The attention sink, which is static regardless of input, produces high sparsity in the attention distribution, whereas other parts are more uniformly distributed. Simply applying TopK will place even more weight on the sink token, thus losing contextual information. In addition, misaligning $q$ and $k$ also causes difficulty in search (Liu et al., 2024a).

# 3.2 Estimate attention with sampling

Existing TopK attention mechanisms ignore tokens in the KV cache with low attention scores, which introduces a bias since the ignored tokens comprise a large proportion of attention scores (Figure 2a). As a result, TopK attention achieves suboptimal performance for long-context tasks, such as information aggregation (Figure 1). Increasing the computation budget for TopK attention does help reduce the estimation error (Figure 4) since it will involve more elements in computing. However, the following question is posed:

Can we improve the estimation quality with low computational budgets?

Inspired by mark and recapture (Lukacs, 2009; Owen, 2013; Lohr, 2021; Chen et al., 2018), we show in the following that attention output can be estimated with sampling. Using notations from Section 2.1 we can re-write attention output $o$ as the expectation of $v _ { i } , 1 \leq i \leq n$ from distribution $w$ , i.e. $o = \mathbb { E } _ { i \sim w } ( v _ { i } )$ , which can be estimated by the following method.

![](images/56c3001685befcf259f96a6dfa3920922cdf2fe49ca129a1a6486b2836682289.jpg)  
Figure 6 Left and Middle: Oracle sampling estimation can significantly reduce numerical error compared to TopK attention. The evaluated context size is 16k. The $_ x$ -axis is sampling budget for oracle sampling and computation budget for TopK attention. Notice that the estimation error of TopK attention will cross oracle sampling after a certain large budget (12k in figures). This is because oracle sampling will repetitively sample the same subset of tokens with a high probability while TopK will not. Theorem 3.3 further explains this. Right: Downstream comparison for oracle sampling estimation and TopK attention. The $_ x$ -axis for both methods is computation budget ratio, i.e. the fraction of selected/sampled tokens.

Definition 3.1 (Oracle Sampling Estimation). Given a sampling budget $\boldsymbol { B }$ and normalized attention score $w$ , $\boldsymbol { \beta }$ elements are sampled independently from $w$ (i.e. $i _ { 1 } , i _ { 2 } , . . . , i _ { B } \overset { \mathrm { i i d } } { \sim } w )$ ). Then the attention output is estimated as

$$
\bar { o } = \frac { 1 } { \mathcal { B } } \sum _ { j = 1 } ^ { B } v _ { i _ { j } }
$$

This is not the lowest variance estimator but has better downstream performance (see Appendix B). We call it “oracle” because it assumes that the exact attention vector $w$ is known, which is not true for sparse attention approximations.

Theorem 3.2. Oracle sampling estimation is unbiased, and the trace of covariance monotonically decreases with $\boldsymbol { B }$ .

This theorem (proved in Appendix A) theoretically guarantees a low estimation error of oracle sampling. We also present an empirical comparison between oracle sampling estimation and TopK attention in Figures 6a and 6b. In summary, oracle sampling estimation can reduce relative error by up to 4 $\times$ .

Note that the sampling budget $\boldsymbol { B }$ is not the actual computation cost for oracle sampling estimation: duplicate $X _ { i }$ need to be computed/loaded only once, so $o$ can be computed by

$$
\bar { o } = \sum _ { i \in \mathcal { S } } \frac { f _ { i } } { B } v _ { i } \quad S = \mathrm { U n i q u e } ( \{ i _ { 1 \leq i \leq \mathcal { B } } \} )
$$

where $f _ { i }$ is the number of duplicates of $X _ { i }$ . Intuitively, if $w$ has an peaked distribution (e.g. $w _ { i } > 9 9 \%$ ), then almost all samples in $\{ i _ { 1 } , . . . , i _ { B } \}$ are identical to $i$ . The actual computation cost of oracle sampling estimation is $| S |$ , the number of unique samples, which we bound in the following:

Theorem 3.3. The expected computation budget $( \mathbb { E } ( | S | )$ ) has an upper bound of $1 + B \epsilon$ , where $\epsilon = 1 - \operatorname* { m a x } _ { i } w _ { i }$

This theorem (proved in Appendix A) shows that the computation cost of oracle sampling is usually far less than the sampling budget. In Figure 6c, we present the downstream accuracy comparison between oracle sampling estimation and TopK attention. The former preserves high accuracy for both tasks, even with a very small computation cost (0.002% out of 16k context, which is approximately 32). In Appendix F, we provide an intuitive example to explain why sampling outperforms TopK in estimation.

# 4 MagicPIG

Section 3.2 demonstrates the potential of sampling-based estimation. In Sections 4.1 and 4.2, we present how we arrive at Locality sensitive hashing to unleash this potential from a statistical perspective. In Section 4.3,

we show the practical algorithm. Finally, in Section 4.4, we demonstrate our system co-design for accurate and efficient LLM decoding through GPU-CPU collaboration.

Note that most of the derivations in this section might be classical and can even be found in textbooks, but our goal is to leverage them to motivate MagicPIG design and precisely demonstrate the power of a rigorously sound algorithm with system co-design in deep generative models.

# 4.1 Self-normalized importance sampling for attention estimation

Oracle sampling estimation cannot go beyond $2 \times$ wall clock speed up because obtaining distribution $w$ requires full computation of all $q k _ { i } ^ { T }$ , thereby only saving the $w V$ computation.

Fortunately, importance sampling (Kloek and Van Dijk, 1978; Owen, 2013; Lohr, 2021) allows us to estimate unknown distribution $w$ by sampling from a proposed distribution $u$ . In our problem setting, the normalization factor of $w$ , i.e. $\begin{array} { r } { Z = \sum _ { i = 1 } ^ { n } \exp { \frac { q k _ { i } ^ { T } } { \sqrt { d } } } } \end{array}$ is also unknown because computing it requires evaluating all $q k _ { i } ^ { T }$ . However, we do have access to unnormalized weights $\widetilde { w _ { i } } = e ^ { \frac { q k _ { i } ^ { T } } { \sqrt { d } } }$ qkTi for sampled indices $i$ . Hence, by employing a variant of fimportance sampling, self-normalized importance sampling (Owen, 2013), we sample indices $i _ { 1 } , i _ { 2 } , . . . , i _ { B }$ from a proposed distribution $u$ and the resulting estimator is

$$
X ^ { \mathrm { I S } } = \frac { 1 } { \widetilde Z } \sum _ { j = 1 } ^ { { \cal B } } \frac { \widetilde { w _ { i _ { j } } } } { u _ { i _ { j } } } v _ { i _ { j } } \quad \mathrm { w h e r e } \quad \widetilde Z = \sum _ { j = 1 } ^ { { \cal B } } \frac { \widetilde { w _ { i _ { j } } } } { u _ { i _ { j } } }
$$

which has a very nice property for accurately estimating attention output that $\mathbb { P } [ \operatorname* { l i m } _ { B \to \infty } X ^ { \mathrm { 1 S } } = o ] = 1$

Its variance1 is related to the distribution $u$ , and can be approximated by

$$
\widetilde { \mathrm { V a r } } ( X ^ { \mathrm { I S } } ) = \frac { 1 } { \mathcal { B } } \mathbb { E } _ { i \sim u } [ \frac { w _ { i } ^ { 2 } } { u _ { i } ^ { 2 } } ( v _ { i } - o ) ^ { 2 } ] = \frac { 1 } { \mathcal { B } Z ^ { 2 } } \mathbb { E } _ { i \sim u } [ \frac { \widetilde { w _ { i } } ^ { 2 } } { u _ { i } ^ { 2 } } ( v _ { i } - o ) ^ { 2 } ]
$$

To minimize the variance, $u$ should satisfy $u \propto \widetilde { w _ { i } } | v _ { i } - o |$ (Hesterberg, 2003). The variance will be high if $u _ { i }$ and $\widetilde { w _ { i } } | v _ { i } - o |$ assign a high probability mass to different regions of the sample space or have different modes. fTherefore, the challenge is computing a distribution $u$ aligned with $\widetilde { w _ { i } } | v _ { i } - o |$ without accessing too many $\widetilde { w _ { i } }$ . Besides, Equation (6) requires that sampling probability $u$ f can be computed and $u _ { i } > 0$ f, which is not satisfied by many deterministic approximations like TopK.

# 4.2 Variance reduction with LSH

We decompose $\begin{array} { r } { \widetilde { w _ { i } } | v _ { i } - o | = \exp ( \frac { q k _ { i } ^ { T } } { \sqrt { d } } + \log | v _ { i } - o | ) } \end{array}$ . We observe empirically (Figure 10 in the appendix) that $\log \left| v _ { i } - o \right|$ does not fluctuate significantly compared to $\frac { q k _ { i } ^ { T } } { \sqrt { d } }$ . Hence, we simplify the requirement of $u$ to share the same peaks with $q k _ { i } ^ { I }$ . By the following transformation,

$$
r = \operatorname* { m a x } _ { 1 \leq i \leq n } \left| k _ { i } \right| \quad \bar { q } = [ q , 0 ] \quad \bar { k _ { i } } = [ k _ { i } , \sqrt { r ^ { 2 } - | k _ { i } | ^ { 2 } } ]
$$

we further transfer the inner product $q k _ { i } ^ { I }$ to cosine similarity between $q$ and $\bar { k _ { i } }$ (which is a common practice in Maximum Inner Product Search (Shrivastava and Li, 2014)).

Inspired by prior work (Spring and Shrivastava, 2017; Chen et al., 2020a), we leverage Locality sensitive hashing-based sampling for this estimation problem. Specifically, leveraging a hash function $h$ in the LSH family that preserves cosine similarity such as SimHash (Sadowski, 2007), we can sample from probability distribution $u _ { i } = \mathbb { P } [ h ( q ) = h ( k _ { i } ) ]$ which is monotonic to cos $\frac { q k _ { i } ^ { T } } { | q | \cdot | k _ { i } | }$ .

# 4.3 Algorithm Design

To make this estimation practical, MagicPIG is implemented by the following specific design.

Estimator approximation. Self-normalized important sampling Equation (6) requires $i _ { 1 } , i _ { 2 } , . . . , i _ { k }$ iid sampled, but the probabilities provided by hashing are not normalized. Hence we adapt our estimator: After obtaining $S$ with probability $u$ , MagicPIG computes

$$
X = \frac { \sum _ { i = 1 } ^ { n } \frac { \widetilde { w _ { i } } } { u _ { i } } v _ { i } \mathbf { 1 } _ { i \in S } } { \sum _ { i = 1 } ^ { n } \frac { \widetilde { w _ { i } } } { u _ { i } } \mathbf { 1 } _ { i \in S } } = \frac { \sum _ { i \in S } \frac { \widetilde { w _ { i } } } { u _ { i } } v _ { i } } { \sum _ { i \in S } \frac { \widetilde { w _ { i } } } { u _ { i } } }
$$

Hash function selection. MagicPIG leverages SimHash (Sadowski, 2007), that draws with $K \times L$ random vectors. For each of the $L$ hash tables, the $q$ and $k _ { i }$ s vectors are projected on $K$ directions, and only the sign of the projection is kept, which yields a $K$ -bit hash value. Key $k _ { i }$ is sampled only if there exist at least two $^ 2$ hash tables where $k _ { i }$ shares the hash value with $q$ . The corresponding probability is

$$
= \mathbb { P } [ k _ { i } { \mathrm { ~ i s ~ s a m p l e d } } ] = 1 - ( 1 - p ^ { K } ) ^ { L } - L p ^ { K } ( 1 - p ^ { K } ) ^ { L - 1 } \quad { \mathrm { w h e r e } } \quad p = 1 - { \frac { 1 } { \pi } } \operatorname { a r c c o s } { \frac { q k _ { i } ^ { T } } { | q | \cdot | k _ { i } | } }
$$

Data pre-processing. Before building hash tables, MagicPIG centers the $k _ { i }$ vectors. As shown in Figure 2c, keys are almost always concentrated on one side of the queries, except the initial token. In this case, random projections cannot effectively distinguish keys, resulting in uniform sampled probabilities. Softmax is translation invariant. Centering $\begin{array} { r } { ( \bar { k _ { i } } = k _ { i } - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } k _ { i } ) } \end{array}$ distributed the keys better and remains computationally equivalent.

Combining Equations (9) and (10) gives a closed form of the MagicPIG attention estimation. Assuming sample set $S$ is obtained with LSH,

$$
\begin{array} { r l } & { \bar { o } = \displaystyle \sum _ { i \in S } \frac { \exp { ( \frac { q k _ { i } ^ { T } } { \sqrt { d } } - \log { u _ { i } } ) } } { \sum _ { i \in S } \exp { ( \frac { q k _ { i } ^ { T } } { \sqrt { d } } - \log { u _ { i } } ) } } v _ { i } } \\ & { u _ { i } = 1 - ( 1 - p _ { i } ^ { K } ) ^ { L } - L p _ { i } ^ { K } ( 1 - p _ { i } ^ { K } ) ^ { L - 1 } } \\ & { p _ { i } = 1 - \displaystyle \frac { 1 } { \pi } \operatorname { a r c c o s } { \frac { q k _ { i } ^ { T } } { | q | \cdot | k _ { i } | } } } \end{array}
$$

Input: $K , V \in R ^ { n \times d }$ , $\pmb q \in R ^ { 1 \times d }$ , random projectors $W \in R ^ { d \times ( K \times L ) }$ , hash tables $_ { H T }$ , static KV cache KT , VT ∈ Rt×d.   
Compute hash code for new query   
$\begin{array} { r } { { \pmb q } _ { \mathrm { c o d e } } = { \bf E } { \bf n } { \bf c o d e } ( { \pmb q } , { \pmb W } ) } \end{array}$   
Query hash tables to sample $S$ in Equation (9)   
S = Query(HT , qcode), KS = K[S], VS = V [S] Compute inner product for $\mathbf { \pmb { q } }$ and sampled $\pmb { K }$   
${ \pmb w } _ { S } = { \pmb q } { \pmb K } _ { S } ^ { T }$ , ${ \pmb w } _ { T } = { \pmb q } { \pmb K } _ { T } ^ { T }$   
Compute collision probability for each hash function $\textstyle p = 1 - { \frac { 1 } { \pi } }$ arccos(w/(||q|| · ||KS||))   
Compute sampling probability   
${ \pmb u } = 1 - ( 1 - { \pmb p } ^ { K } ) ^ { L } - L { \pmb p } ^ { K } ( 1 - { \pmb p } ^ { K } ) ^ { L - 1 }$   
Compute attention output estimation   
$\begin{array} { r } { \bar { o } = \mathbf { S o f t m a x } ( \frac { [ w _ { S } , w _ { T } ] } { \sqrt { d } } - \log ( [ { \pmb u } , \mathbf { 1 } _ { t } ] ) ) [ V _ { S } , V _ { T } ] } \end{array}$   
Return ¯o

# 4.4 System co-design

The memory size of KV cache remains a bottleneck for long-context LLM decoding, especially when GPU VRAM is limited. DRAM on the CPU side offers sufficient memory capacity with $1 0 0 - 2 0 0 \mathrm { G B / s }$ bandwidth, which is usually $1 0 - 2 0 \%$ of GPU VRAM bandwidth (see Figure 7a). Ideally, this gap can be mitigated by $5 - 1 0 \times$ sparsity. To make CPU DRAM an aggregated memory for GPU, the workload must be partitioned. In our experiments, $K = 9$ or 10, and $L$ is a few hundred.

Our system design extends prior work (He and Zhai, 2024; Aminabadi et al., 2022) by splitting LLM decoding into three parts. (1) Parameter computations, i.e., all linear projectors including MLP and $W _ { Q } , W _ { K } , W _ { V } , a n d W _ { O }$ in the self-attention module run on GPU. (2) Attention computation, which involves $\begin{array} { r } { o = \mathrm { S o f t m a x } ( \frac { q K ^ { T } } { \sqrt { d } } ) V } \end{array}$ , runs on CPU. (3) Random projections. At generation time, for each $q$ , $K \times L$ random projections are conducted to obtain the hash codes. Since all heads can share the same random projectors, the memory overhead is limited (400 KB in our implementation), so this step is compute-bound. Therefore, the projection is placed on GPU.

![](images/0a7417db907f7720d501a03ff8e8ccd4e5cb0e299700a003acc54d28c1c6d691.jpg)  
Figure 7 Left: Memory hierarchy of hardware. GPU VRAM has high bandwidth but is limited. CPU DRAM is sufficient but is relatively slow. The limited bandwidth of PCIE forbids large-scale data transfer. Right: Workload partition of MagicPIG. Linear projections and hash function computation (by random projection) are done on GPU, while sampling with hash tables and attention are done on CPU. The execution order is $\textcircled { 1 } \textcircled { 3 } \textcircled { 4 } \textcircled { 2 }$ at decoding time.

(4) Retrieval. The hash codes of $q$ , need to be looked up in $L$ hash tables, which is negligible computationally. However, the pre-built hash tables for $k _ { i }$ s can occupy considerable memory, making it a better fit for the CPU. With the above partition, we are able to support hash tables with $K$ and $L$ beyond the scale of prior work (Kitaev et al., 2020; Chen et al., 2021; Zandieh et al., 2023) without worrying about computation for hash codes as well as the storage of hash tables.

On-device cache. Sink tokens (the first several tokens) and local tokens are more likely to be sampled according to their high similarity to the query. To further reduce CPU workload, MagicPIG stores these tokens on GPU and does not apply LSH sampling to them. We leverage the recursive attention technique (Ye et al., 2024) to merge the attention output from CPU and GPU.

Our algorithm applies to a single attention head, see Algorithm 1. The details of Encode, Query, as well as the hash table construction, are described in prior work (Sadowski, 2007; Chen et al., 2020b). In Appendix E, we discuss the selection of LSH hyper-parameter (K, L).

# 5 Evaluation

In this section, we aim to demonstrate that MagicPIG can speed up LLM decoding while preserving high accuracy. We first present MagicPIG’s accuracy in downstream tasks, followed by our end-to-end system results showing wall-clock performance.

• In Section 5.1, we demonstrate MagicPIG preserves high accuracy (less than 2% degradation) across moderate to long context tasks with computation cost $2 \% \sim 5 \%$ of full attention. • In Section 5.2, we demonstrate the system performance of MagicPIG, which achieves up to 5 $\times$ throughput improvement and 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct with 96K context. • In Section 5.3, we verify the effectiveness of centering, which is of vital importance for the success of sampling. Also, we demonstrate that MagicPIG already outperforms TopK attention in the two aggregation tasks in Figure 1, indicating that sampling indeed goes beyond TopK attention.

# 5.1 MagicPIG Preserves Accuracy

We demonstrate that MagicPIG can preserve accuracy in diverse tasks with less than 5% computation.

Setup. Our experiments are based on Llama (AI@Meta, 2024; Dubey et al., 2024; Touvron et al., 2023) models. Three types of tasks are included, which are 3 mid-context comprehensive tasks from lm-eval-harness (Gao et al., 2021) (GSM8K-CoT (Cobbe et al., 2021), MMLU-Flan-Cot-Fewshot (Hendrycks et al., 2020) and COQA (Reddy et al., 2019)), and 6 long context tasks from (Bai et al., 2023) (QASPER (Dasigi et al., 2021), LCC, Repobench-P (Liu et al., 2023), TriviaQA (Joshi et al., 2017), PRE and TREC (Li and Roth, 2002; Hovy et al., 2001)) and 13 synthetic tasks from RULER (Hsieh et al., 2024) (with 50 examples per task).

Baselines. Besides full attention, Quest (Tang et al., 2024) and its variants are used as baselines. In its default setting, Quest uses a “page size” of 16, i.e. 1/16 of the full attention cost. To compare the methods fairly in the low computation budget regime, we also evaluate Quest with page size 32 and 64 and make sure at least one page is selected in every test example. The initial 4 tokens and local 64 (for LongBench (Bai et al., 2023) and RULER (Hsieh et al., 2024)) or 24 (for lm-eval-harness (Gao et al., 2021)) tokens as well as layer- $\{ 0 , 1 6 \}$ are statically preserved. We do not use the theoretical transformations in Equation (8) in our implementations, as we do not find them to contribute to accuracy improvements.

Table 1 Comprehensive tasks on lm-eval-harness (Gao et al., 2021). MagicPIG significantly outperforms other methods with lower computation. The config (K, L) is a hyper-parameter of LSH for MagicPIG or page size and ratio of selected pages for Quest (Tang et al., 2024). $\mathrm { C o s t _ { 1 } }$ , $\mathrm { C o s t _ { 2 } }$ represents the cost for searching/sampling and sparse attention computation.   

<table><tr><td>Methods</td><td>Config</td><td>GSM</td><td>COQA</td><td>MMLU|i</td><td>|Avg.</td><td>. |Cost1</td><td>Cost2|i</td><td>Costtotal.</td></tr><tr><td>Llama-2-7b-chat</td><td>Full</td><td>22.4</td><td>75.8</td><td>49.2</td><td>49.1d</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,220)</td><td>17.3</td><td>76.4</td><td>48.6</td><td>47.4</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,90)</td><td>18.7</td><td>75.0</td><td>47.9</td><td>47.2</td><td>0.00</td><td>0.08</td><td>0.08</td></tr><tr><td>Quest</td><td>(16,0.05)</td><td>13.0</td><td>69.4</td><td>41.4</td><td>41.3</td><td>0.06</td><td>0.05</td><td>0.11</td></tr><tr><td>Quest</td><td>(32,0.1)d)</td><td>15.7</td><td>70.2</td><td>44.0</td><td>43.3</td><td>0.03</td><td>0.10</td><td>0.13</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>Full</td><td>77.6</td><td>78.5</td><td>65.2</td><td>73.7</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,220)</td><td>72.7</td><td>78.1</td><td>62.7</td><td>71.2</td><td>0.00</td><td>0.03</td><td>0.03</td></tr><tr><td>MAGICPIG</td><td>(8,90)</td><td>71.0</td><td>78.0</td><td>61.3</td><td>70.1</td><td>0.00</td><td>0.07</td><td>0.07</td></tr><tr><td>Quest</td><td>(16,0.05)</td><td>57.9</td><td>64.6</td><td>42.5</td><td>55.0</td><td>0.06</td><td>0.05</td><td>0.11</td></tr><tr><td>Quest</td><td>(32,0.1)d)</td><td>64.5</td><td>65.0</td><td>48.0</td><td>59.2</td><td>0.03</td><td>0.10</td><td>0.13</td></tr></table>

Table 2 Long context tasks on LongBench (Bai et al., 2023). MagicPIG preserves high accuracy with low computation. Config and cost are defined as in Table 1. Code models are only evaluated by Repobench-P and LCC.   

<table><tr><td>Methods</td><td>Config</td><td>QaS</td><td>RbP</td><td>LCC</td><td>PrE</td><td>TrC</td><td>TrQ|</td><td>Avg.</td><td>Cost1</td><td>Cost2</td><td>|c Costtotal.</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>Full</td><td>44.9</td><td>52.1</td><td>66.8</td><td>100.0</td><td>71.3</td><td>91.8</td><td>71.2</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>43.2</td><td>50.2</td><td>64.4</td><td>100.0</td><td>71.3</td><td>92.2</td><td>70.3</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>43.5</td><td>50.4</td><td>67.0</td><td>100.0</td><td>71.7</td><td>91.7</td><td>70.7</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.05)</td><td>45.7</td><td>49.7</td><td>64.9</td><td>100.0</td><td>71.7</td><td>91.5</td><td>70.6</td><td>0.06</td><td>0.05</td><td>0.11</td></tr><tr><td>Quest</td><td>(32,0.1)</td><td>44.4</td><td>50.5</td><td>65.1</td><td>100.0</td><td>71.3</td><td>391.6</td><td>70.5</td><td>0.03</td><td>0.10</td><td>0.13</td></tr><tr><td>Code-Llama-13b-16K</td><td>Full</td><td></td><td>58.5</td><td>74.7</td><td></td><td></td><td></td><td>66.6</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td></td><td>56.9</td><td>74.0</td><td></td><td></td><td></td><td>65.5</td><td>0.00</td><td>0.03</td><td>0.03</td></tr><tr><td>Quest</td><td>(16,0.05)</td><td></td><td>56.4</td><td>74.4</td><td></td><td></td><td></td><td>65.4</td><td>0.06</td><td>0.05</td><td>0.11</td></tr></table>

Cost. The cost for the attention approximation consists of two parts: Cost $\bot$ is the sampling/search cost to obtain $S$ in Equation (11), Cost $^ 2$ is the attention computation cost, see Equation (11). We report the ratio of the number of FLOPs compared to the full attention computation. For MagicPIG, $\mathrm { { C o s t } _ { 1 } \simeq 0 }$ and $\mathrm { C o s t _ { 2 } }$ is empirically measured for different LSH hyper-parameters. For Quest with page size $K$ , $\textstyle \mathrm { C o s t } _ { 1 } = { \frac { 1 } { K } }$ and $\mathrm { C o s t _ { 2 } }$ is controlled manually.

Analysis. From Tables 1 to 3, (1) MagicPIG preserves high accuracy (degradation less than $2 \%$ ) for all kinds of tasks, with a computation cost of $2 \% \sim 5 \%$ . (2) Compared with Quest, which also shows reasonable performance on long context tasks, MagicPIG also demonstrates good performance on tasks with moderate context sizes in lm-eval-harness (Gao et al., 2021), indicating a more robust performance in general serving. (3) With LSH sampling, which introduces an order of magnitude lower sampling/searching cost (Cost $^ { 1 }$ ), MagicPIG can achieve equivalent or better accuracy with only half of the computation cost.

# 5.2 MagicPIG Shows Impressive Efficiency across Various Hardware Settings

We show MagicPIG can bring up to $5 \times$ wall clock speed up and reduce GPU memory consumption on different models and hardware settings (A100, L20, RTX4090).

Setup. We evaluate our system performance on 3 serving settings. (1) 80GB GPU (A100) and 34B model (CodeLlama-34B) (Rozi\`ere et al., 2024) with 16K contexts; (2) 48GB GPU (L20) and 13B model (CodeLlama13B) (Rozi\`ere et al., 2024) with 16K contexts; (3) 24GB GPU $^ 3$ (e.g. RTX 4090) and 8B model (Llama-3.1-

Table 3 Synthesized tasks on RULER (Hsieh et al., 2024). MagicPIG preserves high accuracy with low computation. Config and cost are defined as in Table 1.   

<table><tr><td>Methods</td><td>Config</td><td>16Kd</td><td>32K</td><td>64K</td><td>96K(id:)</td><td>|Avg.|d</td><td>(ct1d</td><td>Cost2 |</td><td>(c stotal.</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>Full</td><td>94.2</td><td>91.5</td><td>86.1</td><td>83.0</td><td>88.7</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>91.8</td><td>88.9</td><td>84.8</td><td>80.0</td><td>86.4</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>93.4</td><td>90.6</td><td>84.7</td><td>81.5</td><td>87.6</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>92.9</td><td>90.2</td><td>84.9</td><td>81.7</td><td>87.4</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.04)</td><td>86.3</td><td>85.4</td><td>81.9</td><td>74.9</td><td>82.1</td><td>0.06</td><td>0.04</td><td>0.10</td></tr><tr><td>Quest</td><td>(32,0.06)</td><td>84.3</td><td>84.0</td><td>80.1</td><td>74.4</td><td>80.7</td><td>0.03</td><td>0.06</td><td>0.09</td></tr><tr><td>Quest</td><td>(64,0.08)</td><td>85.2</td><td>84.3</td><td>77.0</td><td>74.2</td><td>80.2</td><td>0.02</td><td>0.08</td><td>0.10</td></tr><tr><td>MegaBeam-Mistral-7B-512K</td><td>Full</td><td>91.7</td><td>88.1</td><td>83.5</td><td>83.7</td><td>86.8</td><td>0.00</td><td>1.00i</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>89.8</td><td>86.5</td><td>81.7</td><td>80.7</td><td>84.7</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>90.7</td><td>88.5</td><td>82.9</td><td>82.4</td><td>86.1</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>90.6</td><td>86.4</td><td>82.8</td><td>81.6</td><td>85.4</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.04)</td><td>83.3</td><td>83.2</td><td>79.3</td><td>78.6</td><td>81.1</td><td>0.06</td><td>0.04</td><td>0.10</td></tr><tr><td>Quest</td><td>(32,0.06)</td><td>81.5</td><td>80.8</td><td>76.7</td><td>74.4</td><td>78.4</td><td>0.03</td><td>0.06</td><td>0.09</td></tr><tr><td>Quest</td><td>(64,0.08)</td><td>79.6</td><td>77.5</td><td>73.8</td><td>73.7</td><td>76.1</td><td>0.02</td><td>0.08</td><td>0.10</td></tr><tr><td>Llama3-8B-Prolong-512K</td><td>Full</td><td>93.5</td><td>90.8</td><td>85.1</td><td>83.5</td><td>88.2</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>88.0</td><td>86.4</td><td>81.3</td><td>78.8</td><td>83.6</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(10,170)</td><td>89.0</td><td>88.7</td><td>82.8</td><td>80.0</td><td>85.1</td><td>0.00</td><td>0.025</td><td>0.025</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>91.4</td><td>88.2</td><td>82.4</td><td>80.4</td><td>85.6</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>91.4</td><td>88.6</td><td>83.1</td><td>80.5</td><td>85.9</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.04)</td><td>84.9</td><td>83.7</td><td>78.7</td><td>78.6</td><td>81.5</td><td>0.06</td><td>0.04</td><td>0.10</td></tr></table>

![](images/1ef15962bfe1952668230bdd3d1456ac54a67e949329f618629fa0fd179f3dc2.jpg)  
Figure 8 We evaluate MagicPIG on three serving scenarios. Left: A100 serves 34B model with 16K context. MagicPIG achieves $1 . 5 \times$ throughput improvement. Mid: L20 serves 13B model with 16K context. MagicPIG achieves $5 . 0 \times$ throughput improvement. Right: Simulated RTX 4090 serves 8B model with 96K context. MagicPIG achieves a latency of 54ms in a single request serving and can improve the throughput of baseline by up to $3 . 3 \times$ . The dashed lines denote the highest throughput of baselines. With KV cache offloading, MagicPIG can fit a much larger batch size compared with full attention on GPU, which contributes to the throughput improvement.

![](images/c73a04cfe65993386f791d2b3a4da60defa8a63d4a0bca98efd0ede8835424dd.jpg)  
Figure 9 Left: Accuracy comparison for with and without centering. Here we fix $K$ and vary $L$ for the two settings. Mid and Right: Comparison between TopK attention and MagicPIG. In the two aggregated tasks, sampling-based MagicPIG can even beat the exact TopK attention. The experiments are done on RULER (Hsieh et al., 2024) with a 16K context size.

# 8B) (Dubey et al., 2024) with 96K contexts.

Baselines. Our baselines for (1) and (2) are full attention on GPU, and for (3) is full attention on CPU with theoretical estimated bandwidth. Our system’s GPU part is implemented in native Pytorch (Paszke et al., 2019) and the CPU part in FBGEMM (Khudia et al., 2021) in bfloat16 precision. Our CPU is Intel Platinum $8 4 8 0 +$ for A100 and Intel 8563C for L20. In the last setting, the CPU bandwidth is estimated at 150GB/s, above the empirical bandwidth we measure when running a group query attention of size 4.

Analysis. In Figures 8a to 8c, we demonstrate (1) MagicPIG significantly improves decoding throughput for all three scenarios (A100: 1.5 $\times$ , L20: $5 . 0 \times$ , RTX 4090: 3.3 $\times$ ) and can achieve a latency of 54ms for single request generation with 96K context for RTX 4090. (2) With KV cache offloading, MagicPIG can fit much larger batches than GPU full attention baselines (over $1 2 \times$ ). The ablation study of decoding throughput with different LSH hyper-parameters is presented in Table 7.

# 5.3 Ablation Study

In this section, we empirically validate our two previous observations.

Centering is important for good performance. In Section 4.3, we use a translation to center the keys before applying LSH sampling. Empirical results show this to be important for downstream tasks as shown in Figure 9a. Without centering, the accuracy drops to almost zero in retrieval (NIAH) and degrades to $6 5 \%$ in FWE. We find almost no keys (less than $0 . 1 \%$ ) can be sampled by the query without centering, as their orientation is almost opposite, as shown in Figure 2c.

Sampling goes beyond TopK. In Figures 9b and 9c, We compare the performance of MagicPIG and TopK attention in two aggregated tasks (CWE, FWE) where TopK attention experiences significant performance degradation (Figure 1). MagicPIG can even beat exact TopK attention in these two tasks by a margin up to $3 \%$ and 8% respectively, demonstrating that sampling improves the ceiling of TopK, which is impossible for a search-only algorithm.

# 6 Conclusion

In this work, we first present the limitation of TopK attention approximation for addressing the computational and memory challenges of long-context LLM generation. Then we show oracle sampling can go beyond TopK and introduce MagicPIG, a novel approach that leverages LSH sampling to approximate the oracle sampling. MagicPIG significantly reduces the workload of attention computation while preserving high accuracy across diverse tasks. MagicPIG relies on LSH sampling and a system co-design that offloads hash tables and reduced attention computation to the CPU. Our experimental results demonstrate that MagicPIG substantially improves throughput and latency across multiple hardware configurations, outperforming traditional TopK attention mechanisms. The theoretical soundness, robustness, and scalability of MagicPIG open up new opportunities in both attention approximation methods and algorithm-hardware co-design.

References   
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,   
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114–127, 2024.   
AI@Meta. Llama 3 model card. 2024. https://github.com/meta-llama/llama3/blob/main/MODEL CARD.md.   
Josh Alman and Zhao Song. Fast attention requires bounded entries. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 63117–63135. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper files/paper/2023/file/ c72861451d6fa9dfa64831102b9bb71a-Paper-Conference.pdf.   
Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale. arXiv preprint arXiv:2207.00032, 2022.   
Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approximate near neighbors. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 793–801, 2015.   
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal LSH for angular distance. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pages 1225–1233, 2015.   
Arturs Backurs, Moses Charikar, Piotr Indyk, and Paris Siminelakis. Efficient density evaluation for smooth kernels. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 615–626, 2018. doi: 10.1109/FOCS.2018.00065.   
Arturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efficient kernel density estimation in high dimensions. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper files/   
paper/2019/file/a2ce8f1706e52936dfad516c23904e3e-Paper.pdf.   
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.   
Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. arXiv preprint arXiv:2304.02207, 2023.   
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis, 2019. https://arxiv.org/abs/1809.11096.   
Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing, STOC ’02, page 380–388, New York, NY, USA, 2002. Association for Computing Machinery. ISBN 1581134959. doi: 10.1145/509907.509965. https://doi.org/10.1145/509907.509965.   
Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with application to the syrian conflict. The Annals of Applied Statistics, 12(2):1039–1067, 2018.   
Beidi Chen, Yingchen Xu, and Anshumali Shrivastava. Fast and accurate stochastic gradient estimation. Advances in Neural Information Processing Systems, 32, 2019.   
Beidi Chen, Tharun Medini, James Farwell, sameh gobriel, Charlie Tai, and Anshumali Shrivastava. Slide : In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning and Systems, volume 2, pages 291–306, 2020a. https://proceedings. mlsys.org/paper files/paper/2020/file/ca3480d82599b9b9b7040655483825c1-Paper.pdf.   
Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems, 2:291–306, 2020b.   
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R´e. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413–17426, 2021.   
Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024.   
Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024.   
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.   
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.   
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. https://doi.org/10.48550/arXiv.2307.08691.   
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022a.   
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b.   
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021.   
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar´e, Maria Lomeli, Lucas Hosseini, and Herv´e J´egou. The faiss library. arXiv preprint arXiv:2401.08281, 2024.   
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. https: //doi.org/10.5281/zenodo.5371628.   
Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024.   
Jiaao He and Jidong Zhai. Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024.   
Pujiang He, Shan Zhou, Wenhuan Huang, Changqing Li, Duyi Wang, Bin Guo, Chen Meng, Sheng Gui, Weifei Yu, and Yi Xie. Inference performance optimization for large language models on cpus, 2024. https://arxiv.org/abs/2407.07304.   
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   
Timothy Hesterberg. Advances in importance sampling. 01 2003.   
Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus, 2024. https://arxiv.org/abs/2311.01282.   
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. https://www.aclweb.org/anthology/H01-1069.   
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.   
Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker Mushfiqul Islam, and Chidambaram Crushev. A survey on locality sensitive hashing algorithms and their applications. arXiv preprint arXiv:2102.08942, 2021.   
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017. https://arxiv.org/abs/1705.03551.   
Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.   
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.   
Teun Kloek and Herman K Van Dijk. Bayesian estimates of equation system parameters: an application of integration by monte carlo. Econometrica: Journal of the Econometric Society, pages 1–19, 1978.   
Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. https://www.aclweb.org/anthology/C02-1150.   
Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024.   
Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024.   
Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, et al. Retrievalattention: Accelerating long-context llm inference via vector retrieval. arXiv preprint arXiv:2409.10516, 2024a.   
Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023. https://arxiv.org/abs/2306.03091.   
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024b.   
Sharon L Lohr. Sampling: design and analysis. Chapman and Hall/CRC, 2021.   
Paul Lukacs. Closed population capture-recapture models. Program MARK: a gentle introduction, 8, 2009.   
Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. Intelligent probing for locality sensitive hashing: multi-probe lsh and beyond. Proc. VLDB Endow., 10(12):2021–2024, August 2017. ISSN 2150-8097. doi: 10.14778/ 3137765.3137836. https://doi.org/10.14778/3137765.3137836.   
Yuzhen Mao, Martin Ester, and Ke Li. Iceformer: Accelerated inference with long-sequence transformers on cpus. arXiv preprint arXiv:2405.02842, 2024.   
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.   
Toan Nguyen Mau and Yasushi Inoguchi. Locality-sensitive hashing for information retrieval system on multiple gpgpu devices. Applied Sciences, 10(7), 2020. ISSN 2076-3417. doi: 10.3390/app10072539. https://www.mdpi.com/ 2076-3417/10/7/2539.   
Art B. Owen. Monte Carlo theory, methods and examples. https://artowen.su.domains/mc/, 2013.   
Zaifeng Pan, Feng Zhang, Hourun Li, Chenyang Zhang, Xiaoyong Du, and Dong Deng. G-slide: A gpu-based sub-linear deep learning engine via lsh sparsification. IEEE Transactions on Parallel and Distributed Systems, 33(11):3015–3027, 2022. doi: 10.1109/TPDS.2021.3132493.   
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.   
Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019.   
Baptiste Rozi\`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. https://arxiv.org/abs/2308.12950.   
Caitlin Sadowski. Simhash : Hash-based similarity detection. 2007. https://api.semanticscholar.org/CorpusID: 199497165.   
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Advances in Neural Information Processing Systems (NeurIPS), pages 2321–2329, 2014.   
Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. arXiv preprint arXiv:2406.02542, 2024.   
Malcolm Slaney, Yury Lifshits, and Junfeng He. Optimal parameters for locality-sensitive hashing. Proceedings of the IEEE, 100(9):2604–2623, 2012. doi: 10.1109/JPROC.2012.2193849.   
Ryan Spring and Anshumali Shrivastava. A new unbiased and efficient class of lsh-based samplers and estimators for partition function computation in log-linear models. arXiv preprint arXiv:1703.05160, 2017.   
Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024.   
Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024.   
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.   
Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. arXiv preprint arXiv:2406.17419, 2024.   
Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024.   
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.   
Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze. Cascade inference: Memory bandwidth efficient shared prefix batch decoding, February 2024. https://flashinfer.ai/2024/02/ 02/cascade-inference.html.   
Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In International Conference on Machine Learning, pages 40605–40623. PMLR, 2023.   
Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, and Bin Cui. Pqcache: Product quantization-based kvcache for long context llm inference. arXiv preprint arXiv:2407.12820, 2024.   
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding. CoRR, abs/2309.08168, 2023a. doi: 10.48550/ARXIV.2309. 08168. https://doi.org/10.48550/arXiv.2309.08168.   
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R´e, Clark Barrett, Zhangyang ”Atlas” Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 34661–34710. Curran Associates, Inc., 2023b. https: //proceedings.neurips.cc/paper files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf.   
Yang Zhou. Yangzhoumill/infini igsm 4k noise close. https://huggingface.co/datasets/YangZhoumill/infini igsm 4k noise close, 2024a. Accessed: 2024-10-20.   
Yang Zhou. Yangzhoumill/infini igsm 8k noise close. https://huggingface.co/datasets/YangZhoumill/infini igsm 8k noise close, 2024b. Accessed: 2024-10-20.

# Appendix

# A Proofs for theorems

# A.1 Proof for Theorem 3.2

Proof.

$$
\mathbb { E } ( \bar { o } ) = \frac { 1 } { \mathcal { B } } \sum _ { j = 1 } ^ { \mathcal { B } } \mathbb { E } [ v _ { i _ { j } } ] = \frac { 1 } { \mathcal { B } } \sum _ { i = 1 } ^ { n } w _ { i } v _ { i } = o
$$

Assume $\Sigma _ { 1 }$ is the covariance matrix of $o$ , $\Sigma _ { 2 }$ is the covariance matrix of $v _ { i }$

$$
\mathrm { T r } ( \Sigma _ { 1 } ) = \frac { 1 } { B } \mathrm { T r } ( \Sigma _ { 2 } ) = \frac { 1 } { B } ( \mathbb { E } [ | | v _ { i } | | ^ { 2 } ] - | | \mathbb { E } [ v _ { i } ] | | ^ { 2 } ) = \frac { 1 } { B } ( \mathbb { E } [ | | v _ { i } | | ^ { 2 } ] - | | o | | ^ { 2 } )
$$

$\mathbb { E } [ | | v _ { X } | | ^ { 2 } ] - | | o | | ^ { 2 }$ is a constant, so the trace of covariance matrix monotonically decreases with $\boldsymbol { \beta }$ .

# A.2 Proof for Theorem 3.3

Proof.

$$
\mathbb { E } [ | S | ] = \mathbb { E } \Big [ \sum _ { i = 1 } ^ { n } \mathbf { 1 } _ { i \in S } \Big ] = \sum _ { i = 1 } ^ { n } \mathbb { E } [ \mathbf { 1 } _ { i \in S } ] = \sum _ { i = 1 } ^ { n } ( 1 - ( 1 - w _ { i } ) ^ { B } ) = n - \sum _ { i = 1 } ^ { n } ( 1 - w _ { i } ) ^ { B }
$$

Without loss of generality, let $a _ { i } = 1 - w _ { i }$ and $a _ { 1 } = \operatorname* { m i n } _ { 1 \leq i \leq n } a _ { i } = \epsilon$ , then

$$
\begin{array} { l } { \displaystyle \mathbb { E } [ | S | ] = n - \sum _ { i = 1 } ^ { n } a _ { i } ^ { \mathcal { B } } = n - a _ { 1 } ^ { \mathcal { B } } - \sum _ { i = 2 } ^ { n } a _ { i } ^ { \mathcal { B } } } \\ { \displaystyle = n - \epsilon ^ { \mathcal { B } } - \sum _ { i = 2 } ^ { n } a _ { i } ^ { \mathcal { B } } } \end{array}
$$

$f ( x ) = x ^ { B }$ is convex function with $B \geq 1$ and $x \geq 0$ . Then with Jensen’s inequality, we have

$$
\begin{array} { c } { \displaystyle \sum _ { i = 2 } ^ { n } a _ { i } ^ { \mathfrak { s } } \geq ( n - 1 ) \Big ( \frac { \sum _ { i = 2 } ^ { n } a _ { i } } { n - 1 } \Big ) ^ { \mathfrak { s } } = ( n - 1 ) \Big ( \frac { ( \sum _ { i = 1 } ^ { n } a _ { i } ) - a _ { 1 } } { n - 1 } \Big ) ^ { \mathfrak { s } } } \\ { = ( n - 1 ) ( \frac { n - 1 - \epsilon } { n - 1 } ) ^ { \mathfrak { s } } = ( n - 1 ) ( 1 - \frac { \epsilon } { n - 1 } ) ^ { \mathfrak { s } } } \end{array}
$$

Let $g ( x ) = ( 1 - x ) ^ { B } + B x - 1$ . We can prove $g ( x ) \geq 0$ for any $x \in ( 0 , 1 ) , B \geq 1$ . Then we have

$$
\sum _ { i = 2 } ^ { n } a _ { i } ^ { \mathcal { B } } \geq ( n - 1 ) ( 1 - \frac { \epsilon \mathcal { B } } { n - 1 } ) = n - 1 - \epsilon \mathcal { B }
$$

Then we finally have

$$
\mathbb { E } [ | S | ] = n - \epsilon ^ { B } - \sum _ { i = 2 } ^ { n } a _ { i } ^ { B } \le 1 + \epsilon B
$$

# B Oracle sampling

The optimal sampling probability to guarantee estimation is unbiased in terms of lowest variance is not directly using attention score distribution $w _ { i }$ , but $u _ { i } ^ { \prime } \propto w _ { i } | | v _ { i } | |$ . However, this sampling probability is not optimal in terms of downstream accuracy and efficiency. We attribute this to two reasons. First, we observe the value norm of the sink token is significantly smaller than others (Figure 11), given its lower probability of being sampled, which may influence the functionality of attention. Second, due to the same reason, $u _ { i } ^ { \prime } \propto w _ { i } | | v _ { i } | |$ is flatter than $w _ { i }$ , resulting larger computation cost (as analyzed by Theorem 3.3).

# C Supplementary analysis

![](images/e9c6607814ba811c3435ce7e4ff0e92b93eecb0fc62a18c275a410703276fe28.jpg)  
Figure 10 The range of fluctuation of log |vi − o| and qkT√id in a single decoding step. Compared to $\frac { q k _ { i } ^ { T } } { \sqrt { d } }$ \$√\，\$ , $\log \left| v _ { i } - o \right|$ is stable, hence we do not consider $\log \left| v _ { i } - o \right|$ in our proposed sampling probability.

![](images/3b6ec96bd0b1da5742317cf7ecb3e35f4b09dac0aa0bf9f19c3f3216e1c38cb8.jpg)  
Figure 11 The $_ y$ -axis is the norm of values states $\lVert v _ { i } \rVert$ for token $_ i$ (on the x-axis). We observe that the value norm $\| v _ { 0 } \|$ of the attention sink is significantly smaller than others.

Figure 10 shows that compared to qkT√id , $\log \left| v _ { i } - o \right|$ is stable in a decoding step. Figure 11 shows that the norm of the value states of attention sink is smaller than others.

# D Additional evaluation

In this section, we provide additional experimental results to demonstrate that

• MagicPIG can support longer context lengths and a wide range of LLMs (Appendix D.1).   
• MagicPIG can scale up with 70B level LLM (Appendix D.2).   
• MagicPIG can perform well in reasoning benchmarks (Appendix D.3).   
• MagicPIG improves decoding throughput with various hyper-parameters (K, L). (Appendix D.4).

# D.1 Longer Contexts

Following the setups of Table 3, we evaluate two additional models, MegaBeam-Mistral-7B-512K $^ 4$ and Llama3- 8B-Prolong-512K (Gao et al., 2024) with context lengths extended to 256K. The results are shown in Table 4.

Table 4 Synthesized tasks on RULER (Hsieh et al., 2024). MagicPIG preserves high accuracy with extended context lengths and different models. Config and cost are defined as in Table 1.   

<table><tr><td>Methods</td><td>Config</td><td>16K</td><td>32K</td><td></td><td>64K96K</td><td>128K</td><td>256K|</td><td>|Avg.</td><td>Cost1</td><td>Cost2</td><td>(Cstotal.</td></tr><tr><td>MegaBeam-Mistral-7B-512K</td><td>Full</td><td>91.7</td><td>88.1</td><td>83.5</td><td>83.7</td><td>83.5</td><td>82.5</td><td>85.5</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>89.8</td><td>86.5</td><td>81.7</td><td>80.7</td><td>81.6</td><td>79.0</td><td>83.2</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>90.7</td><td>88.5</td><td>82.9</td><td>82.4</td><td>82.3</td><td>80.1</td><td>84.5</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>90.6</td><td>86.4</td><td>82.8</td><td>81.6</td><td>82.3</td><td>80.8</td><td>84.1</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.04)</td><td>83.3</td><td>83.2</td><td>79.3</td><td>78.6</td><td>78.5</td><td>78.5</td><td>80.2</td><td>0.06</td><td>0.04</td><td>0.10</td></tr><tr><td>Llama3-8B-Prolong-512K</td><td>Full</td><td>93.5</td><td>90.8</td><td>85.1</td><td>83.5</td><td>81.7</td><td>78.4</td><td>85.5</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>88.0</td><td>86.4</td><td>81.3</td><td>78.8</td><td>77.3</td><td>71.1</td><td>80.5</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(10,170)</td><td>89.0</td><td>88.7</td><td>82.8</td><td>80.0</td><td>77.7</td><td>73.7</td><td>82.0</td><td>0.00</td><td>0.025</td><td>0.025</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>91.4</td><td>88.2</td><td>82.4</td><td>80.4</td><td>79.2</td><td>75.2</td><td>82.8</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td>MAGICPIG</td><td>(8,75)</td><td>91.4</td><td>88.6</td><td>83.1</td><td>80.5</td><td>79.1</td><td>73.9</td><td>82.8</td><td>0.00</td><td>0.05</td><td>0.05</td></tr><tr><td>Quest</td><td>(16,0.04)</td><td>84.9</td><td>83.7</td><td>78.7</td><td>78.6</td><td>76.3</td><td>72.3</td><td>79.2</td><td>0.06</td><td>0.04</td><td>0.10</td></tr></table>

# D.2 Scaling up to larger models

We evaluate MagicPIG for meta-llama/Llama-3.1-70B-Instruct (Dubey et al., 2024) to demonstrate that our approach can work well with larger LLMs in Table 5.

Table 5 Synthesized tasks from RULER (Hsieh et al., 2024). MagicPIG preserves high accuracy with low computatio for 70B level models. 4 layers $\{ 0 , 1 6 , 3 2 , 4 8 \}$ are preserved. Config and cost are defined as in Table 1.   

<table><tr><td>Methods</td><td>Config</td><td>16K</td><td>32K</td><td>64K</td><td>96K</td><td>Avg.</td><td>Cost1</td><td>Cost2</td><td>Costtotal.</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>Full</td><td>96.4</td><td>94.6</td><td>89.2</td><td>80.8</td><td>90.3</td><td>0.00</td><td>1.00</td><td>1.00</td></tr><tr><td>MAGICPIG</td><td>(10,150)</td><td>94.7</td><td>93.5</td><td>87.5</td><td>79.3</td><td>88.8</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>MAGICPIG</td><td>(9,110)</td><td>95.7</td><td>93.5</td><td>88.4</td><td>79.4</td><td>89.3</td><td>0.00</td><td>0.034</td><td>0.034</td></tr><tr><td>MAGICPIG</td><td>(9,120)</td><td>95.5</td><td>94.1</td><td>88.8</td><td>80.6</td><td>89.8</td><td>0.00</td><td>0.04</td><td>0.04</td></tr></table>

# D.3 Reasoning

In mathematical reasoning tasks infini igsm (Zhou, 2024a,b), MagicPIG consistently outperforms Quest (Tang et al., 2024) across all complexity (in terms of operators). We also find TopK attention suffers from significant performance degradation while Oracle Sampling can maintain high accuracy.

Table 6 Tasks from infini igsm (Zhou, 2024a,b). MagicPIG preserves high accuracy for reasoning tasks. Config and cost for MagicPIG and Quest are defined as in Table 1. Config denotes the ratio of selected tokens for TopK and sampled tokens for oracle sampling. For oracle sampling, massive duplication exists in sampled tokens, so Cost2 is significantly lower than the ratio of sampled tokens Theorem 3.3.   

<table><tr><td>Task</td><td>|Methods</td><td>Config</td><td>2-Ops</td><td>4-Ops</td><td>5-Ops |</td><td>|Cost1</td><td>Cost2</td><td>Costtotal.</td></tr><tr><td></td><td>Llama-3.1-8B-Instruct MAGICPIG</td><td>Full</td><td>87.4</td><td>71.4</td><td>26.8</td><td>0.00</td><td>1.00d</td><td>1.00</td></tr><tr><td></td><td>MAGICPIG</td><td>(10,300) (10,220)</td><td>83.1 79.8</td><td>67.2 58.9</td><td>20.7 17.9</td><td>0.00 0.00</td><td>0.06 0.04</td><td>0.06 0.04</td></tr><tr><td></td><td>MAGICPIG</td><td>(10,150)</td><td>68.3</td><td>43.5</td><td>11.7</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td></td><td>T0pK</td><td>0.06</td><td>78.6</td><td>62.9</td><td>20.8</td><td>0.50</td><td>0.06</td><td>0.56</td></tr><tr><td>4K close (Zhou, 2024a)</td><td>TopK</td><td>0.04</td><td>76.2</td><td>59.0</td><td>19.2</td><td>0.50</td><td>0.04</td><td>0.54</td></tr><tr><td></td><td>TopK</td><td>0.02</td><td>71.5</td><td>44.0</td><td>11.3</td><td>0.50</td><td>0.02</td><td>0.52</td></tr><tr><td></td><td>Oracle Sampling</td><td>0.3</td><td>88.1</td><td>72.4</td><td>27.6</td><td>0.50</td><td>0.02</td><td>0.52</td></tr><tr><td></td><td>Oracle Sampling</td><td>0.1</td><td>88.5</td><td>69.2</td><td>26.2</td><td>0.50</td><td>0.01</td><td>0.51</td></tr><tr><td></td><td>Oracle Sampling</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Quest</td><td>0.02 (16,0.06)</td><td>83.1 55.8</td><td>57.9 23.2</td><td>11.9 5.2</td><td>0.50 0.06</td><td>0.005 0.06</td><td>0.505 0.12</td></tr><tr><td></td><td>Llama-3.1-8B-Instruct</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>MAGICPIG</td><td>Full (10,300)</td><td>80.2 78.6</td><td>68.8 61.5</td><td>26.0|id) 25.2</td><td>0.00 0.00</td><td>1.00d 0.06</td><td>1.00 0.06</td></tr><tr><td></td><td>MAGICPIG</td><td>(10,220)</td><td>72.2</td><td>60.7</td><td>20.4</td><td>0.00</td><td>0.04</td><td>0.04</td></tr><tr><td></td><td>MAGICPIG</td><td>(10,150)</td><td>67.1</td><td>44.0</td><td>11.9</td><td>0.00</td><td>0.02</td><td>0.02</td></tr><tr><td>8K close (Zhou, 2024b)</td><td>TopK</td><td>0.06</td><td>70.2</td><td>61.1</td><td>22.3</td><td>0.50</td><td>0.06</td><td>0.56</td></tr><tr><td></td><td>TopK</td><td>0.04</td><td>66.9</td><td>55.2</td><td>20.6</td><td>0.50</td><td>0.04</td><td>0.54</td></tr><tr><td></td><td>Kd</td><td>0.02</td><td>64.7</td><td>47.2</td><td>15.9</td><td>0.50</td><td>0.02</td><td>0.52</td></tr><tr><td></td><td>Oracle Sampling</td><td>0.3</td><td>80.0</td><td>67.3</td><td>26.2</td><td>0.50</td><td>0.02</td><td>0.52</td></tr><tr><td></td><td>Oracle Sampling</td><td>0.1</td><td>76.6</td><td>64.1</td><td>25.4</td><td>0.50</td><td>0.01</td><td>0.51</td></tr><tr><td></td><td>Oracle Sampling</td><td>0.02</td><td>79.0</td><td>60.3</td><td>20.4</td><td>0.50</td><td>0.005</td><td>0.505</td></tr><tr><td></td><td>Quest</td><td>(16,0.06)</td><td>54.8</td><td>30.0</td><td>11.1</td><td>0.06</td><td>0.06</td><td>0.12</td></tr></table>

Table 7 System performance for MagicPIG using Llama-3.1-8B-Instruct with a 96K context length under varying hyper-parameter configurations. We report the decoding latency (time between tokens, TBT) when the batch size is 1, the maximum throughput, and the throughput with a latency constraint of $2 0 0 \mathrm { m s }$ (Throughpu $\mathrm { { ' } 2 0 0 m s }$ in the table). Config and cost are defined as in Table 1. The number with ∗ means hit the memory limit of CPU.   

<table><tr><td>Config</td><td></td><td>| TBT (ms) Max Throughput (tokens/sec) Throughput200ms</td><td>(tokens/sec)</td><td>|Costotal.</td></tr><tr><td>(11,300)</td><td>17.38</td><td>41.68*</td><td>40.84</td><td>0.02</td></tr><tr><td>(10,220)</td><td>14.07</td><td>32.29*</td><td>26.66</td><td>0.04</td></tr><tr><td>(10,170)</td><td>16.79</td><td>46.52*</td><td>39.90</td><td>0.025</td></tr><tr><td>(10,150)</td><td>18.31</td><td>53.78</td><td>48.89</td><td>0.02</td></tr><tr><td>(9,120)d)</td><td>13.93</td><td>32.50</td><td>26.60</td><td>0.04</td></tr><tr><td>(8,75)</td><td>12.47</td><td>27.43</td><td>21.17</td><td>0.05</td></tr></table>

# D.4 System performance

In this section, we evaluate the system performance (latency, throughput) of MagicPIG under different hyper-parameter configurations. We use Llama-3.1-8B-Instruct (Dubey et al., 2024) with 96K contexts as an example.

# E Selection of hyper-parameter (K, L)

In this section, we discuss the impact of the LSH hyper-parameter (K, L) and how to select it. First, we briefly explain what hyper-parameter (K, L) does for LSH sampling. Then, we explain the relations between (K, L) and attention computation cost and accuracy. Finally, we show how we decide the parameters by ablation studies.

# E.1 (K, L) in LSH

In each hash table, we use K hash functions to compute the hash code of $k$ and $q$ . In Simhash (Charikar, 2002), the hashing we use in MagicPIG, the hash functions are random projections. With K random projections, we are able to partition the space (in our problem, the space is $R ^ { 1 2 8 }$ ) into $2 ^ { K }$ subspace. If and only if $k$ and $q$ fall in the same subspace, we say they collide in this hash table. We have L hash tables in total. In MagicPIG, if and only if $k$ and $q$ collide in at least two hash tables, $k$ is sampled by $q$ . Here are some intuitions about how (K, L) will influence the LSH sampling in MagicPIG.

• If K is too small, then we cannot partition the space well; we will sample too many $k$ s, which might be far away from $q$ (in the attention problem, this means their inner production is small), increasing computation cost.   
• On the other hand, if K is too large, although the quality of sampled ks will be better, the collision probability in each table will be small; thus, the number of the sampled ks will be reduced. We need to increase L to ensure that a certain number of keys are sampled and involved in the computation. However, increasing (K, L) too much will bring more memory overhead on CPU DRAM since we build L hash tables for each key-value head.

Thus, (K, L) is important because it balances computation cost, overhead, and sampling quality (which determines accuracy). Tuning (K, L) is necessary in LSH (Lv et al., 2017; Slaney et al., 2012).

# E.2 (K, L) and memory overhead

(K, L) will change two overheads brought by MagicPIG: the memory occupied by hash tables on the CPU and extra computation for random projections (hash functions) on the GPU (as shown in Table 8).

Table 8 The overhead of Locality sensitive hashing during decoding. We report the size of random projectors (on GPU) and hash tables (on CPU), the computation overhead CO (refers to the ratio between computation introduced by random projections in LSH and the computation of the original model’s linear projections (e.g., $W _ { Q } , W _ { K } , W _ { V }$ , and MLP)). Notice that when the context length exceeds 64K, we need to use 32-bit integers to store the indices for the KV cache in hash tables. Llama-3.1-8B/70B-Instruct (Dubey et al., 2024) and Code-Llama-34b-16K Rozi\`ere et al. (2024) use group query attention, thus the sizes of hash tables are reduced.

<table><tr><td>Models</td><td>(K, L)</td><td>Context lengthi</td><td>Projectors</td><td>Hash tables</td><td>CO</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>(10, 150)</td><td>96K</td><td>384KB</td><td>14GB</td><td>3.8%</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>(11, 300)</td><td>96K</td><td>825KB</td><td>28GB</td><td>8.5%</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>(10, 150)</td><td>64K</td><td>384KB</td><td>4.7GB</td><td>3.8%</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>(10, 150)</td><td>64K</td><td>384KB</td><td>11.8GB</td><td>1.8%</td></tr><tr><td>Code-Llama-13b-16K</td><td>(10, 150)</td><td>16K</td><td>384KB</td><td>7.3GB</td><td>5.2%</td></tr><tr><td>Code-Llama-34b-16K</td><td>(10, 150)</td><td>16K</td><td>384KB</td><td>1.8GB</td><td>2.2%</td></tr></table>

LLM decoding is a memory-bandwidth-bound process and the majority of time is spent loading the data (parameters/KV cache) to GPU cores rather than actually doing the computation (Miao et al., 2023; Zhang et al., 2023a; Chen et al., 2024). Besides, the time-consuming part, i.e., the long-context attention computation, is moved to the CPU. Thus, the $1 . 8 \% \sim 8 . 5 \%$ extra computation on GPU will only make a minor difference in execution time. However, the enlarged size of hash tables prevents us from always increasing (K, L) to get more accurate results.

As shown in Table 8, under the same (K, L), the memory overhead of hash tables grows linearly with contex length and the total number of key-value heads in models (which is determined by model sizes).

# E.3 (K, L) and computation cost/budget

In summary, increasing K will make the budget5 smaller, and increasing L will increase the budget.

Theoretically, as introduced in Section 4.3, in our approach, the key $k _ { i }$ is sampled only if at least two hash tables exist where $k _ { i }$ shares the hash value with query q. With the assumption that $k _ { i }$ is well-distributed (In

each hash table out of $\mathrm { L }$ , each hash value corresponds to roughly the same number of $k _ { i } \mathrm { s }$ ), the ratio of retrieved $k _ { i }$ s can be estimated with

$$
\mathcal { B } / n = 1 - ( 1 - 0 . 5 ^ { K } ) ^ { L } - L \times 0 . 5 ^ { K } ( 1 - 0 . 5 ^ { K } ) ^ { L - 1 }
$$

where $n$ is the context length. Here, we estimate the collision probability of $k _ { i }$ and $q$ in a single hash table as $0 . 5 ^ { K }$ .

Empirically, the ratio of retrieved keys and values $( B / n )$ might differ from the above estimation since the data is not perfectly distributed. We present the empirically measured budget in Table 9.

Table 9 Empirical measured budget/cost for different (K, L).   

<table><tr><td>K/L</td><td>75</td><td>100</td><td>120</td><td>150</td><td>200</td><td>300</td></tr><tr><td>7</td><td>14%</td><td>21%</td><td>27%</td><td>35%</td><td>48%</td><td>66%</td></tr><tr><td>8</td><td>5%</td><td>8%</td><td>11%</td><td>15%</td><td>22%</td><td>36%</td></tr><tr><td>9</td><td>1.6%</td><td>2.7%</td><td>4%</td><td>5.4%</td><td>8.5%</td><td>15.4%</td></tr><tr><td>10</td><td>0.5%</td><td>0.9%</td><td>1.5%</td><td>2%</td><td>3%</td><td>6%</td></tr><tr><td>11</td><td>0.15%</td><td>0.3%</td><td>0.5%</td><td>0.6%</td><td>1%</td><td>2%</td></tr></table>

# E.4 (K, L) and accuracy

There are no naive relations between (K, L) and downstream accuracies since (K, L) not only influences sampling quality but also the computation budget. One safe way to discuss the relation between (K, L) and accuracy is: Fixing the computation budget, larger (K, L) will potentially produce higher accuracy, since the sampling quality is higher. Our experimental results show that,

• Increasing (K, L) can significantly improve accuracy in relatively longer contexts Table 10.

Table 10 We show the effectiveness of larger hash tables for longer contexts by evaluating MegaBeam-Mistral-7B-512K on RULER (Hsieh et al., 2024). With the same computation cost $\sim 2 \%$ ), config (11, 300) achieves higher accuracy compared to (10, 150).

<table><tr><td>(K, L)</td><td>16K</td><td>128K</td><td>256K</td></tr><tr><td>Full</td><td>91.7</td><td>83.7</td><td>82.5</td></tr><tr><td>(10, 150)</td><td>89.8</td><td>80.7</td><td>79.0</td></tr><tr><td>(11, 300)</td><td>90.6</td><td>83.3</td><td>81.9</td></tr></table>

• Same set of (K, L) can generalize to larger LLMs Table 11.

Table 11 8B and 70B models on RULER (Hsieh et al., 2024) 64K.   

<table><tr><td>Models/Config</td><td>Full</td><td>(10, 150)</td><td>(10, 135)</td><td>(9, 120)</td><td>(9, 110)</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>86.1</td><td>84.8</td><td>83.6</td><td>84.7</td><td>84.7</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>89.2</td><td>87.5</td><td>86.7</td><td>88.8</td><td>88.4</td></tr></table>

# E.5 How to select (K, L)

Finding the optimal (K, L) for high accuracy as well as efficiency is a long-standing problem in LSH. Similar to the traditional hyper-parameter tuning process in machine learning, K, and L are configured offline based on data subsets. In LSH, K is a more sensitive hyper-parameter than L. A slight change of K can drastically influence the number of retrieved items (i.e., budget/cost) and quality. In MagicPIG, K=8-10 is manually determined by ablations on small-scale tasks and found to be effective across various models and tasks; then, we adjust $\mathrm { L }$ to obtain the wanted computation cost/budget.

Here, we present two ablations to demonstrate the selection of K in Tables 12 and 13.

Table 12 Fixing the budget/cost to 4%, we ablation the performance of different (K, L) on RULER (Hsieh et al., 2024) 16K.   

<table><tr><td>Models/Config</td><td>Full </td><td>(10,240)(9, 120)</td><td></td><td>)(8,65) </td><td>(7,35)</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>94.2</td><td>94.2</td><td>92.8</td><td>92.3</td><td>88.5</td></tr></table>

Table 13 Fixing L as 120, we ablation the performance of different K on RULER (Hsieh et al., 2024) 16K for Llama-3.1-8B-Instruct.   

<table><tr><td>(K, L)</td><td>Full</td><td>(10, 120)</td><td>(9, 120)</td><td>(8, 120)</td><td>(7, 120)</td></tr><tr><td>Cost</td><td>1.0</td><td>0.012</td><td>0.04</td><td>0.11</td><td>0.27</td></tr><tr><td>Accuracy</td><td>94.2</td><td>92.8</td><td>92.8</td><td>94.1</td><td>94.3</td></tr></table>

If we want the computation cost to be below 5% and L below 200 (to reduce memory overhead in the CPU), then K=8-10 is a reasonable choice. Unlike K, L is not that sensitive. We select L based on the following principle after determining K: we can allow the computation cost to be smaller for larger K since the sampling is more precise. This is why we choose to use (8, 75), (9, 120), and (10, 150).

It’s worth pointing out that tuning (K, L) is a challenging and long-standing problem in LSH, and we only give an example of practice in MagicPIG. More advanced hashing algorithms (such as Cross-polytope (Andoni et al., 2015) or data-dependent ones (Andoni and Razenshteyn, 2015)) can improve the trade-off between memory overhead and accuracy. We leave it as a future direction.

# F TopK vs. Sampling

In this section, we provide an intuitive understanding of how sampling can work better than TopK. TopK only captures the ranking information when estimating attention output. In contrast, sampling considers the entire data distribution (i.e., the attention score after Softmax).

Here is an example. Imagine a zoo with 100 animals: 10 elephants, 10 pigs, 10 tigers, and 70 other unique animals. The daily food consumption for each group is as follows:

• Elephants: 50 lb/day each   
• Pigs: 20 lb/day each   
• Tigers: 10 lb/day each   
• Other unique animals: 1 lb/day each

To compute the true average daily food consumption per animal in the zoo:

$$
\mathrm { T r u e \ A v e r a g e } = { \frac { ( 1 0 \times 5 0 ) + ( 1 0 \times 2 0 ) + ( 1 0 \times 1 0 ) + ( 7 0 \times 1 ) } { 1 0 0 } } = 8 . 7 \mathrm { l b } .
$$

If we use a Top- $\mathbf { K }$ approach (e.g., selecting the top 10 animals based on the numbers of animals), we include elephants, pigs, tigers, and 7 randomly selected animals from the unique ones. The estimated average is:

$$
\mathrm { T o p K ~ A v e r a g e } = { \frac { ( 1 0 \times 5 0 ) + ( 1 0 \times 2 0 ) + ( 1 0 \times 1 0 ) + ( 7 \times 1 ) } { 3 7 } } = 2 2 \mathrm { l b } .
$$

This overestimates the average because it disproportionately weights high-consumption animals.

Instead, we perform sampling with replacement from the animal distribution, proportional to their numbers. The probabilities for each group are:

$$
\mathrm { S a m p l i n g ~ P r o b a b i l i t i e s } = [ 0 . 1 , 0 . 1 , 0 . 1 , 0 . 0 1 \times 7 0 ] .
$$

where 0.1 represents the probabilities for elephants, pigs, and tigers (10/100 each), and 0.01 corresponds to each unique animal (1/100).

Perform 10 random draws. A possible sampling outcome could be: [elephant, pig, tiger, other, other, other, other, other, other, other]. The corresponding daily food estimate is:

$$
{ \mathrm { S a m p l e ~ E s t i m a t e } } = { \frac { 5 0 + 2 0 + 1 0 + ( 7 \times 1 ) } { 1 0 } } = 8 . 7 1 { \mathrm { b } } .
$$

This estimate is unbiased, meaning the expected value of the estimates equals the true average (8.7 lb). While there is variance across individual trials, the standard deviation (std) can be calculated as 4.7 lb for a 10-sample budget.

Increasing the sampling budget reduces variance. For example, with 20 samples, the std decreases to 3.4 lb. Meanwhile, Top-K with a budget of 20 adds 17 unique animals, yielding:

$$
\mathrm { T o p K ~ A v e r a g e ~ ( K { = } 2 0 ) } = \frac { ( 1 0 \times 5 0 ) + ( 1 0 \times 2 0 ) + ( 1 0 \times 1 0 ) + ( 1 7 \times 1 ) } { 4 7 } = 1 7 \mathrm { l b } .
$$

Again, the Top-K estimate remains biased, significantly overestimating the average.

Note that this is intended as an intuitive example. For a detailed and formal derivation of the sampling methodology, please refer to Kloek and Van Dijk (1978); Owen (2013); Lohr (2021).

# G Limitations and future work

MagicPIG stores the offloaded KV cache and hash tables on CPU DRAM, which is unsuitable for serving scenarios with insufficient DRAM. KV cache quantization methods like QServe (Lin et al., 2024) and KIVI (Liu et al., 2024b) can help to reduce the KV cache memory. Currently, another limitation is that, we have not implemented MagicPIG in prefilling stage, which is also an important direction in long context LLM serving. Applying more advanced LSH algorithms, such as Cross-polytope hash (Andoni et al., 2015), can reduce the size of hash tables while improving estimation accuracy. Building CPU-GPU pipelines (He and Zhai, 2024) and leveraging the new avx512 bf16 features of CPUs will improve efficiency. For higher-end GPUs with sufficient HBM, leveraging LSH to accelerate GPU attention computation is also an interesting topic, as GPU-friendly LSH algorithms and efficient GPU kernels (Nguyen Mau and Inoguchi, 2020; Pan et al., 2022) are required to do sampling. Besides, how to automatically tune the LSH hyper-parameter (K, L) (Lv et al., 2017) is also an interesting future work.