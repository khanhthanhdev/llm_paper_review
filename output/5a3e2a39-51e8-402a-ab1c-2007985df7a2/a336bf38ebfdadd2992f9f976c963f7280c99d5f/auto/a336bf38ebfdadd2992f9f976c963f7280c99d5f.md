# TIDALDECODE: FAST AND ACCURATE LLM DECODING WITH POSITION PERSISTENT SPARSE ATTENTION

Lijie Yang∗ Carnegie Mellon University lijiey@andrew.cmu.edu

Zhihao Zhang∗ Carnegie Mellon University zhihaoz3@cs.cmu.edu

Zhuofu Chen Carnegie Mellon University zhuofuc@cs.cmu.edu

Zikun Li Carnegie Mellon University zikunl@cs.cmu.edu

Zhihao Jia Carnegie Mellon University zhihao@cmu.edu

# ABSTRACT

Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to $2 . 1 \times$ .

# 1 INTRODUCTION

Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance on various applications. As LLMs evolve, they are increasingly being adapted to manage tasks with long contexts, such as Chain-of-Thought reasoning (Wei et al., 2023), document summarization (Huang et al., 2021), and retrieval-augmented generation (Ram et al., 2023; Zhang et al., 2024b). However, quickly and efficiently serving long-context LLMs is challenging due to the inherent memory and compute bottlenecks in the Transformer architectures (Vaswani et al., 2023).

LLM inference involves two separate stages: prefilling and decoding. The prefilling stage computes the activations for all input tokens and stores the keys and values for all tokens in the key-value (KV) cache, allowing the LLM to reuse these keys and values to compute attention for future tokens. In each decoding stage, the LLM decodes one new token using all input tokens and previously generated tokens. The KV cache size grows linearly in the sequence length (Kwon et al., 2023). For instance, with a context length of 128K tokens, the KV cache of LLama2-7B with half-precision can easily reach $6 4 \mathrm { G B }$ , creating substantial memory pressure for LLM serving. In addition, the LLM decoding stage is memory-bounded since decoding one new token requires accessing all previous tokens in the KV cache, making KV cache access the primary bottleneck for long-context LLM decoding. This memory-bound nature severely limits the scalability and efficiency of LLM serving.

![](images/32a84a1aa8b8fa1b0bb95b349b97ca26715b3f96d146c64c99544033842f3bfd.jpg)  
Figure 1: The heatmap for one decoding step of Llama3-8B-Instruct (AI, 2024a), where columns and rows indicate different Transformer layers and tokens in the KV cache, respectively. For each layer, the 5 tokens $10 \%$ sparsity) with the highest attention scores of the first attention head are highlighted in yellow, which are the tokens used for sparse attention. We feed an input prompt “Use only the provided search results to write a high-quality, concise answer to the question. $\backslash \mathrm { n } <$ —begin of text— $\cdot > \backslash { \mathtt { n } }$ The magic number is: 15213. $\backslash \boldsymbol { \mathrm { n } } \backslash \boldsymbol { \mathrm { n } } \backslash \boldsymbol { \mathrm { n } }$ Question: What is the magic number? Keep the response short and direct. Answer: ”, and the LLM outputs “15213”. The results show strong spatial coherence of tokens chosen for sparse attention in the decoding step.

To address this problem, recent work has introduced sparse attention, which approximates full attention using a small portion of tokens with the highest attention scores. Compared to full attention, sparse attention reduces computation cost and memory access while preserving the LLM’s generative performance (Ge et al., 2024; Zhang et al., 2023). Existing sparse attention techniques can be classified into two categories: eviction- and selection-based methods.

First, eviction-based sparse attention reduces memory usage for the KV cache by selectively discarding less relevant tokens from the KV cache, therefore reducing the number of tokens computed in attention mechanisms (Xiao et al., 2023; Zhang et al., 2023). While these methods decrease the size of the KV cache, they can be inadequate for tasks where critical information is carried by tokens that are prematurely evicted, such as the needle-in-the-haystack tasks (Peng et al., 2023). On the other hand, selection-based sparse attention maintains all tokens in the KV cache, estimates their attention scores, and selects a small subset of tokens to participate in each LLM decoding step. This approach is prone to issues related to distribution shifts caused by appending sparsely attended, biased KV representations back into the cache.

This paper presents TidalDecode, an algorithm and system for fast and precise LLM decoding, utilizing position persistent sparse attention (PPSA). A key insight behind TidalDecode is the observation that tokens chosen for sparse attention — based on their highest attention scores — exhibit significant overlap across consecutive Transformer layers within each decoding phase. Figure 1 illustrates this overlap in a single decoding step of LLaMA-3-8B instruct AI (2024a) with an input of 51 tokens. Each column in the figure corresponds to a Transformer layer, and each row indicates one token in the KV cache. Selection-based sparse attention methods select the 5 tokens with the highest attention scores (highlighted in yellow) for attention computation in each head. As the figure depicts, there is a recurring pattern where consecutive layers consistently focus on the same set of tokens, indicating a spatial coherence in the selection of tokens for sparse attention.

Instead of independently selecting tokens for sparse attention at each layer, TidalDecode introduces a few token selection layers, which perform full attention to identify the tokens with the highest attention scores. All remaining layers implement position persistent sparse attention, where only the tokens selected by the token selection layers are retrieved from the KV cache for attention. Consequently, all other layers between two token selection layers operate on the same set of tokens, reducing the overhead of token selection. Experiments across a diverse set of LLMs and datasets demonstrate that using just two token selection layers — one at the beginning and one in the middle — is sufficient to achieve high generative performance while minimizing computation and memory overheads.

This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Additionally, to address the KV cache distribution shift, TidalDecode introduces a cache-correction mechanism that periodically refills the KV cache using full attention for all sparsely decoded tokens to mitigate bias in the KV representations.

Comprehensive evaluation with the LongChat-7b-v1.5-32k, Llama-3-8B, Llama-3-70B, and Llama3.1-8B models on the Needle-in-the-Haystack, PG-19, and LongBench tasks demonstrates that TidalDecode can consistently achieve the best performance efficiency trade-off compared with the best existing sparse attention methods. We have implemented custom GPU kernels for PPSA and an end-to-end system for TidalDecode. Compared with existing full and sparse attention implementations, our system reduced the end-to-end inference latency by up to $2 . 1 \times$ and $1 . 2 \times$ , respectively. In conclusion, our contributions are:

• We propose TidalDecode, a streamlined and efficient algorithm and system for fast and high-quality LLM decoding, utilizing position persistent sparse attention. • To address KV cache distribution shifts, we introduce a cache-correction mechanism that periodically refills the KV cache with using full attention for sparsely decoded tokens. • Empirically, we demonstrate the effectiveness and efficiency of TidalDecode through comprehensive evaluation, showing that TidalDecode significantly outperforms existing sparse attention methods.

# 2 RELATED WORK

Long-context model. Efficiently handling long-context inputs is essential for various LLM tasks in real-world applications such as document summarization, question answering, and dialogue systems (Wang et al., 2024). Recent advancements, including rotary positional encoding (RoPE) (Su et al., 2023), have enabled models to manage extended context lengths effectively. The LLaMA-3 model series supports up to 8K tokens, with enhanced versions such as Gradient-AI-Llama3 (AI, 2024a) and LLaMA 3.1 (AI, 2024b) extending this limit to 128K tokens. Additionally, proprietary LLMs such as GPT-4 Turbo and GPT-4o (OpenAI, 2024) support up to 128K tokens, and Claude 3.5 Sonnet allows up to 200K tokens (Anthropic, 2024). While recent work has introduced efficient attention kernel implementation (Dao et al., 2022; Dao, 2023), processing long-context inputs continues to be constrained by significant memory usage and computational costs from the extended KV cache. TidalDecode is designed to mitigate these challenges by reducing latency and memory overhead through an efficient strategy for selecting tokens with the highest attention scores and one-time intermediate re-calibration, ensuring both efficiency and high-quality output.

To alleviate the intrinsic computational and memory bottleneck in long-context LLM inference, recent works on sparse attention have approached this problem from two main perspectives: eviction- and selection-based methods.

![](images/1c741b0376ca4b3f5cebc84ecd76848fecc1c2ee98abcd62552cb6070dc557fb.jpg)  
Figure 2: An overview of the decoding step in TidalDecode, which performs full attention for the first two layers, full attention with token selection for the third layer and a middle layer, and position persistent sparse attention for all other layers.

Eviction-based sparse attention. Xiao et al. (2023); Zhang et al. (2024a) propose to reduce KV cache memory usage by evicting tokens that are considered less relevant during inference. These suffer from potential performance degradation, especially in tasks where every token may carry crucial information (e.g., needle-in-the-haystack tasks), since tokens with high importance for a future decoding step can be mistakenly evicted as the generation proceeds, which makes selection-based methods more popular choices in latest sparse attention works.

Selection-based sparse attention. Instead of evicting past tokens in the KV cache, Child et al. (2019); Kitaev et al. (2020); Choromanski et al. (2020); Tang et al. (2024); Ribar et al. (2023) preserve the full KV cache and only select important tokens to attend with the attention module on the fly. More specifically, Child et al. (2019) leverages a fixed attention mask to select tokens while Tang et al. (2024); Ribar et al. (2023); Choromanski et al. (2020); Kitaev et al. (2020) aim to identify and retain the most relevant tokens at each layer by approximating attention scores. Although these methods are more selective, they operate independently at each layer and are not guaranteed to obtain the ground-truth tokens with the highest attention scores, failing to capture token relevance patterns that persist across layers. Moreover, attention score estimation algorithms sometimes introduce unnecessary complexity, diminishing the practical efficiency gains they are designed to achieve. Improving upon prior works, TidalDecode leverages a shared pattern of most important tokens across consecutive layers to further reduce the computational overhead and memory access required for token selection.

# 3 METHODOLOGY

This section introduces TidalDecode, an efficient algorithm and system for fast LLM decoding using position persistent sparse attention and KV cache correction. Figure 2 shows an overview of TidalDecode. TidalDecode uses the same prefilling mechanism as existing systems and performs full attention to compute the key-value (KV) cache for all prompt tokens. In each decoding step, TidalDecode uses three types of attention layers: full attention, full attention with token selection, and position persistent sparse attention. First, TidalDecode performs full attention for the initial Transformer layers to avoid early performance degradation as identified by prior work (Tang et al., 2024). Second, the layer immediately after full attention and a single middle layer (e.g., layer 2 and 13 in Figure 2) perform full attention with token selection, where TidalDecode stores the inner product3 between the current query and key vectors of all tokens in KV cache during full attention and then selects $k$ tokens contributing to the highest attention scores. Third, all other layers perform position persistent sparse attention, where only tokens selected from the previous token selection layer are loaded from the KV cache to perform attention computation.

# 3.1 POSITION PERSISTENT SPARSE ATTENTION (PPSA)

Attention mechanisms have been widely used in today’s LLMs. For each attention head, the output is computed via scaled multiplicative formulation as follows.

$$
A _ { i } = Q _ { i } K _ { i } / \sqrt { d } , \quad H _ { i } = \mathrm { s o f t m a x } ( A _ { i } ) V _ { i }
$$

where $Q _ { i } , K _ { i }$ , and $V _ { i }$ are the query, key, and value tensors for the $i$ -th attention head. $A _ { i }$ is a matrix representing the attention scores between tokens, and $H _ { i }$ is the output of the $i$ -th attention head. Instead of attending to all input tokens, existing sparse attention methods approximate attention computation by attending the query $Q _ { i }$ to a subset of previous tokens the highest attention scores. Prior work generally performs token selection for individual attention heads and Transformer layers, introducing high runtime overhead. For example, selecting the tokens with highest attention scores using top- $\mathbf { \nabla } \cdot \mathbf { k }$ can take longer than computing full attention (see Figure 7), thus diminishing the benefits of performing sparse attention.

The key insight behind TidalDecode’s position persistent sparse attention is an observation that tokens with highest attention scores for consecutive Transformer layers highly overlap. We use the LLaMA-3-8B model and the needle-in-the-haystack test on PG-19-mini dataset with a context length of 100K tokens to quantify this observation. We randomly select 100 requests from the dataset, compute full attention, and analyze the top 256 tokens with the highest attention scores for each Transformer layer. Figure 3a shows the overlap ratios for all pairs of transformer layers, where an overlap ratio of 1 indicates that the tokens with highest attention scores are always identical in these layers, and an overlap ratio of 0 means the top tokens do not overlap in the two layers. Note that we select top 256 tokens from 100K tokens in the KV cache, so randomly selected tokens hardly overlap. In Figure 3b, we compute average recall rates of selected tokens by choosing different re-selection layers. We observe that without re-selection layers, where all layers possess a low overlap ratio with Layer 3 shown in the purple box in Figure 3a, the average recall rates are less than $20 \%$ . When we choose Layer 13 to perform re-selection, the average recall rates boost to almost $40 \%$ due to higher overlap ratios between Layer 13 and its subsequent layers, shown by red boxes in Figure 3a.

![](images/aaa58ccf8e77a800b815beff9e5d1ecd19fa2aa7428dfdcd96daf40182ea3f17.jpg)

(a) Overlap of Tokens with the Highest Attention Scores between Layers

![](images/4e5553d3f1282bf6853d0ca7280e7835cbc041a524d5decbe5262a3ac5250263.jpg)  
Figure 3: By retrieving the top-256 tokens from a 100K-context-length Needle-in-the-Haystack test conducted on PG-19-mini, 3a shows the overlap ratio of tokens with the highest attention scores across layers, showing that consecutive layers tend to share a large number of critical tokens. 3b depicts the recall rates, indicating that different choices of re-selection layers have a high impact on the recall rates — there is a clear peak, delineating the optimal layers for token re-selection.

Based on this observation, we design position persistent sparse attention to maximally leverage the token overlaps between consecutive Transformer layers to reduce the computation cost for token selection while achieving high predictive performance. Algorithm 1 shows the TidalDecode algorithm for interleaving full attention and PPSA layers. After the initial full attention layers, TidalDecode uses a token selection layer that computes full attention and selects tokens with the highest attention scores. To select tokens, TidalDecode stores the inner product $\langle Q , K \rangle$ on the fly together with full attention

# Algorithm 1 TidalDecode

1: Input: Current embedding $h$ , KV cache $\mathcal { C }$ , token budget $m$   
2: Output: Logits   
3: Initialize: $\rho = \left[ \begin{array} { l l l } \end{array} \right]$ ▷ Initialize the token buffer to store selected tokens   
4: for each decoder layer $i$ do   
5: $\begin{array} { l } { { q , k , v = f ( W _ { q k v } , h ) } } \\ { { \mathcal { C } . \mathrm { a p p e n d } ( k , v ) } } \end{array}$   
6:   
7: if $i$ is Full Attention Layer then   
8: $o = \mathrm { F u l l A t t e n t i o n } ( q , \mathcal { C } [ : ] )$ ▷ Dense attention with the full KVCache   
9: else if $i$ is Token Selection Layer then   
10: $o =$ FullAttention $( q , { \mathcal { C } } [ : ] )$ ▷ Dense attention with the full KVCache   
11: $K  { \mathcal { C } }$ .getKey, $\rho : = \mathrm { a r g T o p K } ( \langle q , K \rangle , m )$ ▷ Update token buffer   
12: else   
13: o = SparseAttention $( q , \mathcal { C } [ \rho ] )$ ▷ Sparse attention with the tokens in the token buffer   
14: end if   
15: $h = \operatorname { F F N } ( o )$   
16: end for   
17: logits = lm head(h)   
18: return logits

calculation. TidalDecode then selects the top $k$ tokens with the highest inner product values to form a token set $\tau$ . Note that using the inner product to select top- $k$ is equivalent to the post-softmax attention score as the softmax operator is ordering invariant. All PPSA layers after a token selection layer computes sparse attention by only loading the keys and values for tokens in $\tau$ , thus limiting the number of tokens participating in attention computations and reducing memory access.

A straightforward approach to designing TidalDecode is to select the tokens $\tau$ once after full attention and perform PPSA using the same set of tokens for all subsequent layers. However, our preliminary experimentation shows that using a single token set for all Transformer layers reduces the LLM’s predictive performance by a large margin since distant Transformer layers are less correlated compared to consecutive layers, as shown in Figure 3a. To address this issue, TidalDecode performs token re-selection in a middle layer, where TidalDecode recalibrates the selected tokens with the highest attention scores by applying full attention and re-selecting top- $k$ token to update $\tau$ , ensuring that token selection remains optimal for the remaining layers. This re-selection mechanism significantly boosts the model performance and promotes accurate and efficient PPSA throughout the model.

Extensive evaluation on both small and large models on a wide range of datasets shows that using a single middle layer for token re-selection is sufficient to preserve the LLM’s generative performance, while introducing small runtime overhead. However, deciding which layer to perform token reselection is critical to model performance. As shown in Figure 3, choosing different layers for token re-selection results in different recall rates, where layer 11 and 13 achieve optimal performance. Introducing a one-time token re-selection at an optimal layer ensures the selected tokens are recalibrated, effectively mitigating the drift in token importance and elevates accuracy from $15 \%$ (without re-selection) to almost $40 \%$ .

# 3.2 KV CACHE CORRECTION

For tokens decoded by sparse attention methods, their key/value representations can deviate from the original representation of full attention decoded ones, which we refer to as polluted tokens. The problem can be further exacerbated as their KV pairs are added to the KV cache, resulting in the error accumulation or distribution shift of the KV cache. This can lead to model performance drop in scenarios where the generation length is fairly long. To this end, TidalDecode uses a cache-correction mechanism as shown in Figure 4 to periodically correct the polluted tokens in the KV cache. For every $T$ decoding step performed by TidalDecode, there will be a cache correction step through a prefill over all polluted tokens to update their KV representations in the cache. The choice of $T$ can be at the level of thousands of decoding steps but also depend on different models and tasks. Notice that the cache correction step can be performed concurrently with the sparse decoding step. Nevertheless, we haven’t used cache correction in our evaluations to make it a fair comparison against existing methods.

![](images/f149ae5903ade19c5c24096ae74f6f78318816cc5783ba5b22e27060438f95b1.jpg)  
Figure 4: Cache Correction

# 4 EXPERIMENTS

# 4.1 EXPERIMENT SETTING

In this section, we conduct extensive experiments to assess both the performance and efficiency of TidalDecode. Our evaluations are performed on widely used open-source models, including Llama-2- 7B Touvron et al. (2023) and Llama-3-8/70B. Both models are pretrained decoder-only transformers, exhibiting similar yet distinct architectural features. For instance, Llama 3-8B incorporates group query attention (GQA), a feature not present in Llama 2-7B. In Section 4.2, we evaluate TidalDecode’s performance on various tasks, including needle-in-the-haystack, language modeling on PG-19, and LongBench. In Section 4.3, we write customized attention kernels and compare TidalDecode’s kernel efficiency against existing state-of-the-art sparse attention methods. Finally, in Section 4.4, we conclude our evaluations with a detailed sensitivity analysis on the choice of different token selection layers. We use $\mathrm { T D } { \cdot } { \mathrm { L X } }$ to denote TidalDecode with layer X selected as the token re-selection layer throughout this section.

# 4.2 PERFORMANCE EVALUATION

To evaluate the effectiveness of TidalDecode, we conduct two key downstream NLP experiments: the needle-in-the-haystack test and perplexity evaluation on the PG-19 dataset (Rae et al., 2019). These tasks provide robust benchmarks for measuring both sparse attention models’ ability to retrieve critical information in challenging scenarios and their performance on long-context language modeling tasks.

# 4.2.1 NEEDLE-IN-THE-HAYSTACK

Table 1: Results of 10k-context-length Needle-in-the-Haystack test on LongChat-7b-v1.5-32k. TidalDecode achieves the same or better results than Quest and significantly better results than cache eviction algorithms such as H2O, TOVA, and StreamingLLM. TidalDecode achieves full accuracy with only a 512 token budget.   

<table><tr><td>Method / Budget</td><td>K=32</td><td>K=64</td><td>K=128</td><td>K=256</td><td>K=512</td></tr><tr><td>H2O</td><td>0%</td><td>1%</td><td>1%</td><td>1%</td><td>3%</td></tr><tr><td>TOVA</td><td>0%</td><td>1%</td><td>1%</td><td>3%</td><td>8%</td></tr><tr><td>StreamingLLM</td><td>1%</td><td>1%</td><td>1%</td><td>3%</td><td>5%</td></tr><tr><td>Quest</td><td>65%</td><td>99%</td><td>99%</td><td>99%</td><td>100%</td></tr><tr><td>TD+L7(Ours)</td><td>73%</td><td>92%</td><td>98%</td><td>99%</td><td>100%</td></tr></table>

The Needle-in-the-Haystack test assesses LLMs’ ability to handle long-dependency tasks, which is particularly critical for sparse attention algorithms. Eviction-based methods Xiao et al. (2023); Zhang et al. (2023) may discard essential tokens, while selection-based approaches often fail to consistently identify the ground-truth tokens with the highest attention scores in long contexts. Since

Table 2: Comprehensive results of 10K-, 32K-, and 100K-context-length Needle-in-the-Haystack test on Llama-3-8B-Instruct-Gradient- $1 0 4 8 \mathrm { k }$ , Llama-3.1-8B-Instruct, and Llama-3-70B-InstructGradient-1048k with PG-19-mini dataset. Across all models, TidalDecode consistently outperforms Quest, showing that TidalDecode with only two token selection layers can effectively retain critical information. TidalDecode achieves full accuracy with 64, 64, and 128 tokens in 10K-, 32K-, and 100K-context-length tests, which is only $0 . 6 \%$ , $0 . 2 \%$ , and $0 . 1 \%$ of total input lengths, respectively.   

<table><tr><td>Model (context length)</td><td>Method / Budget</td><td>K=32</td><td>K=64</td><td>K=128</td><td>K=256</td><td>K=512</td></tr><tr><td rowspan="3">LLaMA-3-8B (10K)</td><td>Quest</td><td>74%</td><td>84%</td><td>99%</td><td>98%</td><td>100%</td></tr><tr><td>TD+L13(Ours)</td><td>88%</td><td>98%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L15(Ours)</td><td>92%</td><td>88%</td><td>94%</td><td>94%</td><td>100%</td></tr><tr><td rowspan="3">LLaMA-3-8B (100K)</td><td>Quest</td><td>38%</td><td>50%</td><td>65%</td><td>87%</td><td>98%</td></tr><tr><td>TD+L13(Ours)</td><td>86%</td><td>92%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L15(Ours)</td><td>84%</td><td>90%</td><td>92%</td><td>98%</td><td>100%</td></tr><tr><td rowspan="3">LLaMA-3.1-8B (10K)</td><td>Quest</td><td>74%</td><td>86%</td><td>94%</td><td>100%</td><td>98%</td></tr><tr><td>TD+L13(Ours)</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L14(Ours)</td><td>98%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td rowspan="3">LLaMA-3.1-8B (32K)</td><td>Quest</td><td>78%</td><td>88%</td><td>92%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L13(Ours)</td><td>98%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L14(Ours)</td><td>80%</td><td>98%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td rowspan="3">LLaMA-3-70B (10K)</td><td>Quest</td><td>68%</td><td>72%</td><td>90%</td><td>98%</td><td>100%</td></tr><tr><td>TD+L14(Ours)</td><td>87%</td><td>93%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L31(Ours)</td><td>90%</td><td>97%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td rowspan="3">LLaMA-3-70B (32K)</td><td>Quest</td><td>50%</td><td>80%</td><td>88%</td><td>92%</td><td>78%</td></tr><tr><td>TD+L14(Ours)</td><td>82%</td><td>98%</td><td>98%</td><td>100%</td><td>100%</td></tr><tr><td>TD+L31(Ours)</td><td>80%</td><td>82%</td><td>92%</td><td>98%</td><td>100%</td></tr></table>

Quest is the current state-of-the-art approach on this task, we first run TidalDecode on the same test as Quest on the LongChat-7b-v1.5-32k model and obtained Table 1 with competitive performance. To demonstrate the effectiveness of TidalDecode on long-dependency tasks, we further evaluate TidalDecode on tasks with 10K-, 32K-, and 100K-context-window lengths with the LLaMA-3-70B, LLaMA-3-8B, LLaMA-3.1-8B model using the PG-19-mini dataset, shown in Table 2. To ensure fairness, both TidalDecode and Quest use dense attention in the first two layers. In each test, we inserted a random password within the text and tested whether the specific method could retrieve the password correctly.

From Table 2, TidalDecode consistently outperforms Quest and achieves full accuracy with an extremely low sparsity (about $0 . 5 \%$ across all context lengths and models). These results demonstrate TidalDecode can achieve state-of-the-art performance with only two token selection layers. While Quest relies on page-level importance estimation for token selection, TidalDecode’s exact selection with token reuse approach proves more effective for this task. Also, note that TidalDecode can reduce the token budget by up to $8 \times$ when achieving a $100 \%$ accuracy compared with Quest. This further demonstrates that TidalDecode’s exact token selection layer can obtain more relevant tokens than Quest.

# 4.2.2 LANGUAGE MODELING

Perplexity measures the negative likelihood of how well a model predicts the next word in a sequence, with lower values indicating better performance. We evaluate TidalDecode on Llama-3-8B-InstructGradient-1048k with the PG-19 dataset, which includes up to 100 books, providing a comprehensive long-context benchmark.

As shown in Figure 5, TidalDecode+L9/13/15 consistently achieves lower perplexity than Quest across all token budget options (2048, 4096). This indicates that TidalDecode’s position persistent sparse attention mechanism can effectively retain critical information without significantly sacrificing model accuracy, even as the sequence length grows, demonstrating its robustness for long-context inputs.

![](images/d7b440f697b8c40570c6e9adba420481816639bbb6c0d9ff669422d2652d89e0.jpg)  
Figure 5: Perplexity evaluation on the PG-19 dataset from 0 to 32K tokens. The results compare TidalDecode with different token re-selection layers (L9, L13, L15) to Quest across token budgets (2048 5a, 4096 5b). Lower perplexity indicates better model performance. Full refers to dense attention as baseline.

# 4.2.3 LONGBENCH EXPERIMENT

Table 3: Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3-8B-Instruct-Gradient-1048k. TidalDecode outperforms Quest at a 4096 token budget and achieves an average score higher than full-weight attention. The maximum F1-score for each task is in bold.   

<table><tr><td>Method (K)/Task</td><td></td><td>MFQA</td><td>NrtQA</td><td>Qasp</td><td>2Wiki</td><td>HotQA</td><td>QMSm</td><td>TrQA</td><td>PRe</td><td>Avg</td></tr><tr><td>Full</td><td></td><td>30.76</td><td>5.52</td><td>14.56</td><td>13.32</td><td>11.50</td><td>19.43</td><td>86.56</td><td>77.00</td><td>32.33</td></tr><tr><td>Quest</td><td>(1024)</td><td>26.21</td><td>4.08</td><td>12.19</td><td>12.61</td><td>10.75</td><td>19.56</td><td>83.47</td><td>63.84</td><td>29.09</td></tr><tr><td>TD+L13</td><td>(1024)</td><td>28.57</td><td>7.63</td><td>11.11</td><td>13.56</td><td>9.82</td><td>20.37</td><td>79.78</td><td>75.17</td><td>30.75</td></tr><tr><td>Quest</td><td>(4096)</td><td>28.92</td><td>3.74</td><td>13.63</td><td>12.83</td><td>12.15</td><td>19.36</td><td>85.91</td><td>72.50</td><td>31.13</td></tr><tr><td>TD+L13</td><td>(4096)</td><td>30.94</td><td>6.19</td><td>13.85</td><td>14.40</td><td>13.71</td><td>19.48</td><td>86.30</td><td>78.00</td><td>32.86</td></tr></table>

We also evaluate TidalDecode on LongBench, a benchmark designed to test LLMs on long-context tasks across diverse NLP domains (Bai et al., 2023). We focus on eight tasks: MultiFieldQA (MFQA), NarrativeQA (NrtQA), Qasper (Qasp), 2WikiMQA (2Wiki), HotpotQA (HotQA), QMSum (QMSm), TriviaQA (TrQA), and Passage Retrieval (PRe), which collectively composite a comprehensive evaluation benchmark in single/multi-document QA, summarization, and retrieval.

We evaluate all methods with LLaMA-3-8B-Instruct-Gradient- $1 0 4 8 \mathrm { k }$ . TidalDecode is compared against full-weight attention and Quest at token budgets of 1024 and 4096. As shown in Table 3, TidalDecode consistently outperforms Quest on all tasks at $K = 4 0 9 6$ and on five tasks at $K = 1 0 2 4$ Surprisingly, TidalDecode, in most cases, matches or exceeds full attention baseline with notable sparsity: $14 \%$ on NrtQA, $50 \%$ on MFQA, $80 \%$ on Qasp, $50 \%$ on 2WikiMQA, $32 \%$ on HotQA, $29 \%$ on QMSm, $3 5 \%$ on TrQA, and $33 \%$ on PRe. We hypothesize this is because TidalDecode’s token selection process can filter out irrelevant information, thus leading to higher performance.

These results demonstrate TidalDecode’s generic ability to select tokens with the highest attention scores, achieving competitive or superior performance while significantly reducing token usage, making it ideal for long-context scenarios.

![](images/7d610ddd9fe11b3e636486aae11d9d2dec517a0fef9c9381a894dd2a807d376d.jpg)  
Figure 6: End-to-end latency results on LLaMA-2-7B model for Full attention baseline(Full), Quest, and TidalDecode(TD) when context length is 10K, 32K, and 100K, respectively.

# 4.3 EFFICIENCY EVALUATION

To show the efficiency of TidalDecode, we write customized kernels for our approach and measure the end-to-end decoding latency. We conduct evaluation under the configuration of Llama-2-7B on one Nvidia A100 (80 GB HBM, SXM4) with CUDA 12.2. We compare TidalDecode with state-of-the-art full attention serving library FlashInfer (Ye et al., 2024) and also the Quest implementation. As shown in Figure 6, we can observe that TidalDecode can consistently outperform full attention baseline and Quest by a large margin under all token budgets and context lengths. TidalDecode achieves this through token pattern reuse to minimize the token selection overhead. Notice that the latest LLaMA-3 model shares the same architecture as LLaMA-2, except it uses Group-Query-Attention instead of Multi-Head-Attention. However, this does not affect the relative efficiency comparison against Quest and full attention.

![](images/02ce68291fbecef16ce4193d23b1b9adbdbb024b0f99ec176138f7963c054060.jpg)  
Figure 7: Overall attention latency results for different methods on the LLaMA model with (a) 32 and (b) 64 layers. We use the full attention model as a reference and show TidalDecode and Quest’s overall attention latency ratio. For each group of the bar plots, the left/middle/right bar denotes the full attention baseline, Quest, and TidalDecode, respectively.

In Figure 7, we compare the overall attention latency between different methods on the LLaMA model with 32/64 layers. For the 32-layer LLaMA model, we have 2 full attention layers $^ { + 2 }$ token selection layers $+ 2 8$ sparse attention layers, while Quest has 2 full attention layers $+ \ 3 0$ Quest attention layers. For the 64-layer LLaMA model, we have 2 full attention layers $^ { + 2 }$ token selection layers $+ ~ 6 0$ sparse attention layers, while Quest has 2 full attention layers $+ 6 2$ Quest attention layers. Thus, by completely removing the token estimation overhead in the sparse attention layers, for the 32-layer and 64-layer LLaMA model under all context lengths, TidalDecode can consistently achieve the lowest serving latency while bringing up to $5 . 5 6 \times$ speed-up ratio against the full attention baseline and $2 . 1 7 \times$ speed-up ratio against Quest. When the context length is 10K, Quest has a higher latency due to the token selection overhead, which aligns with the end-to-end results in Figure 6. In contrast, TidalDecode still achieves significant speed-up by utilizing the position persistent sparse attention mechanism.

![](images/0d086a94abdc3e56a6274f4ef221673ea209ae9bc19ba4b0ec0f3c27c2c398bd.jpg)  
Figure 8: The breakdown latency results for the full attention, token selection attention, sparse attention, and Quest attention kernels over 10K, 32K, and $1 0 0 \mathrm { K }$ context length. We use full attention latency as a reference and report other kernels’ relative latency ratio. We use a token budget of $_ { \mathrm { K = } 5 1 2 }$ for TidalDecode and Quest across all evaluations.

In Figure 8, we further break down the latency comparison for different attention modules to show why TidalDecode can bring significant speed-up consistently. We compare different attention modules, namely, the full attention layer, the token selection layer, TidalDecode’s sparse attention layer, and the Quest attention layer over the 10K, 32K, and 100K context length. We can observe that, as TidalDecode’s sparse attention kernel can directly reuse previous token patterns, it completely removes the important token estimation overhead in the Quest attention kernel, resulting in up to $3 . 3 6 \times$ speed-up compared with the Quest implementation. On the other hand, even though TidalDecode’s token selection layer has a slightly higher latency, we only have two token selection layers even in the 70B LLaMA model that has 64 layers in total.

# 4.4 SENSITIVITY ANALYSIS ON TOKEN RE-SELECTION LAYER

In this section, we conduct sensitivity studies for different choices of the token re-selection layer. As TidalDecode only has one token re-selection layer in the middle, it is critical to choose the best-performed one. As shown in Figure 9, we have two interesting findings: (1). Different choices of token re-selection layers can significantly affect the accuracy of the results (2). For models within the same model family, the optimal token re-selection layer is consistent over different tasks. In our setup, the optimal token re-selection layer for the LLaMA-2-7B model is layer 7, while for the LLaMA-3-8B/LLaMA-3.1-8B model is layer 13. A concurrent KV cache compression work also identifies that layer 13 is surprisingly important for their approach as well (Shi et al., 2024). For a more detailed sensitivity results on the choice of different token re-selection layers, please refer to the appendix for more results.

# 5 CONCLUSION

To conclude, we introduce TidalDecode, an efficient LLM decoding framework with sparse attention. On observing the correlation of the pattern of tokens with the highest attention scores across different consecutive layers, TidalDecode proposes only to select tokens twice: once at the beginning layers and once in the middle layer to serve as a token re-selection layer. We find that using two token selection layers is necessary and sufficient to achieve high-generation quality. Additionally, by reusing the token patterns throughout the sparse attention layer, TidalDecode greatly reduces the token selection overhead, resulting in a significant end-to-end speed-up ratio against existing methods. More interestingly, the optimal choice of the token re-selection layer is consistent across different tasks if the model is in the same model family.

![](images/7d8e50935b9cc1f6e9ded205302d41a138b82462b70391495a4450268119889d.jpg)  
Figure 9: Sensitivity study on the choice of different token re-selection layer. We evaluate LLaMA2-7B-LongChat, LLaMA-2-7B-Yarn, LLaMA-3-8B, and LLaMA-3.1-8B with TidalDecode with a token budget of 256.

# ACKNOWLEDGMENT

This research is supported by NSF awards CNS-2147909, CNS-2211882, CNS-2239351, and research awards from Amazon, Cisco, Google, Meta, Oracle, Qualcomm, and Samsung.

# REFERENCES

Gradient AI. Llama-3-8b-instruct-gradient-1048k. https://huggingface.co/ gradientai/Llama-3-8B-Instruct-Gradient-1048k, 2024a. Accessed: 2024-09- 26.   
Meta AI. Llama 3.1: Advanced open-source language model. https://ai.meta.com/blog/ meta-llama-3-1/, 2024b. Accessed: 2024-09-26.   
Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/ claude-3-5-sonnet, 2024. [Accessed 20-06-2024].   
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023.   
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.   
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.   
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.   
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and ´ memory-efficient exact attention with io-awareness, 2022.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419–1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112.

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

OpenAI. Introducing gpt-4o: our fastest and most affordable flagship model. https://platform. openai.com/docs/models, 2024. [Accessed 28-05-2024].

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https: //arxiv.org/abs/1911.05507.

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models, 2023. URL https://arxiv. org/abs/2302.00083.

Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference, 2023.

Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty. Discovering the gems in early layers: Accelerating long-context llms with $1 0 0 0 \mathrm { x }$ input token reduction. arXiv preprint arXiv:2409.17422, 2024.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv. org/abs/2406.10774.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee´ Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand \` Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762.

Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in large language models, 2024. URL https://arxiv.org/abs/2402.02244.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023.

Zihao Ye, Ruihang Lai, Roy Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze. Cascade inference: Memory bandwidth efficient shared prefix batch decoding. https:// flashinfer.ai/2024/01/08/cascade-inference.html, Jan 2024. URL https: //flashinfer.ai/2024/01/08/cascade-inference.html. Accessed on 2024-02- 01.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen.´ $\mathrm { H _ { 2 } o }$ : Heavyhitter oracle for efficient generative inference of large language models, 2023.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient genera- ´ tive inference of large language models. Advances in Neural Information Processing Systems, 36, 2024a.

Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, and Zhihao Jia. Accelerating iterative retrieval-augmented language model serving with speculation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 60626–60643. PMLR, 21–27 Jul 2024b. URL https://proceedings.mlr.press/v235/zhang24cq.html.

# A APPENDIX

# A.1 LONGBENCH FOR LLAMA-3.1-8B-INSTRUCT

Table 4: Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3.1-8B-Instruct. The maximum F1-score for each task is in bold.

<table><tr><td>Method</td><td>MFQA</td><td>NrtQA</td><td>Qasp</td><td>2Wiki</td><td>HotQA</td><td>QSm</td><td>TrQA</td><td>Pre</td><td>Avg</td></tr><tr><td>Full</td><td>27.02</td><td>25.59</td><td>13.05</td><td>16.64</td><td>16.86</td><td>23.88</td><td>91.48</td><td>97.67</td><td>39.02</td></tr><tr><td>Quest (1024)</td><td>22.35</td><td>14.89</td><td>12.44</td><td>14.24</td><td>14.12</td><td>23.86</td><td>81.71</td><td>95.73</td><td>34.92</td></tr><tr><td>TD+13 (1024)</td><td>23.70</td><td>23.25</td><td>11.14</td><td>13.53</td><td>13.72</td><td>22.69</td><td>92.35</td><td>92.15</td><td>36.57</td></tr><tr><td>Quest (4096)</td><td>26.34</td><td>21.17</td><td>11.99</td><td>15.61</td><td>16.26</td><td>23.61</td><td>90.73</td><td>96.35</td><td>37.76</td></tr><tr><td>TD+13 (4096)</td><td>25.89</td><td>26.29</td><td>12.65</td><td>16.86</td><td>15.94</td><td>23.27</td><td>90.22</td><td>95.47</td><td>38.32</td></tr></table>

TidalDecode and full-weight attention share the maximum F1 scores for all tasks, achieving the best scores in three tasks (NrtQA, 2Wiki, and TrQA). TidalDecode significantly outperforms Quest in 4/8 tasks (NrtQA, Qasp, 2Wiki, and TrQA) and full-attention in 3/8 tasks (NrtQA, 2Wiki, and TrQA). For other tasks, we stay close to the full attention and also obtains a higher average score than Quest.

# A.2 END-TO-END EFFICIENCY EVALUATION RESULTS

Table 5: TidalDecode end-to-end efficiency results on LLaMA-2-7B   

<table><tr><td rowspan="2">Context Length</td><td rowspan="2">Full Attention (ms)</td><td colspan="7">TidalDecode(ms)</td></tr><tr><td>K=32 K=641</td><td>K=128</td><td>K=256</td><td>K=512</td><td>K=10241</td><td></td><td>K=2048 K=4096</td></tr><tr><td>10K</td><td>19.22</td><td>16.94 17.18</td><td>17.15</td><td>16.98</td><td>16.96</td><td>17.32</td><td>17.19</td><td>17.63</td></tr><tr><td>32K</td><td>25.71</td><td>17.89 17.92</td><td>17.64</td><td>17.70</td><td>17.91</td><td>17.97</td><td>18.48</td><td>18.98</td></tr><tr><td>100K</td><td>45.70</td><td>21.26 21.09</td><td>21.38</td><td>21.19</td><td>21.13</td><td>21.38</td><td>21.65</td><td>22.34</td></tr></table>

Table 6: Quest end-to-end efficiency results on LLaMA-2-7B   

<table><tr><td rowspan="2">Context Length</td><td rowspan="2">Full Attention (ms)</td><td colspan="7">TidalDecode(ms)</td></tr><tr><td>K=32</td><td>K=64K=1281</td><td>K=256</td><td>K=512</td><td></td><td></td><td>K=1024 K=2048 K=4096</td></tr><tr><td>10K</td><td>19.22</td><td>20.39 19.86</td><td>19.44</td><td>19.35</td><td>20.18</td><td>19.91</td><td>20.23</td><td>21.09</td></tr><tr><td>32K</td><td>25.71</td><td>20.47 20.85</td><td>20.73</td><td>21.06</td><td>20.62</td><td>20.94</td><td>21.35</td><td>22.11</td></tr><tr><td>100K</td><td>45.70</td><td>24.93 25.18</td><td>24.77</td><td>24.90</td><td>24.84</td><td>25.10</td><td>25.77</td><td>26.17</td></tr></table>

Table 7: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.2-3B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold.   

<table><tr><td>RL/K</td><td>32</td><td>64</td><td>128</td><td>256</td><td>512</td></tr><tr><td>TidalDecode+L2</td><td>2%</td><td>12%</td><td>14%</td><td>16%</td><td>30%</td></tr><tr><td>TidalDecode+L3</td><td>2%</td><td>6%</td><td>8%</td><td>10%</td><td>24%</td></tr><tr><td>TidalDecode+L4</td><td>6%</td><td>14%</td><td>16%</td><td>20%</td><td>28%</td></tr><tr><td>TidalDecode+L5</td><td>2%</td><td>10%</td><td>22%</td><td>26%</td><td>36%</td></tr><tr><td>TidalDecode+L6</td><td>18%</td><td>26%</td><td>26%</td><td>32%</td><td>46%</td></tr><tr><td>TidalDecode+L7</td><td>6%</td><td>10%</td><td>16%</td><td>18%</td><td>28%</td></tr><tr><td>TidalDecode+L8</td><td>20%</td><td>20%</td><td>46%</td><td>60%</td><td>84%</td></tr><tr><td>TidalDecode+L9</td><td>6%</td><td>12%</td><td>32%</td><td>58%</td><td>66%</td></tr><tr><td>TidalDecode+L10</td><td>44%</td><td>58%</td><td>50%</td><td>60%</td><td>64%</td></tr><tr><td>TidalDecode+L11</td><td>4%</td><td>12%</td><td>16%</td><td>22%</td><td>28%</td></tr><tr><td>TidalDecode+L12</td><td>50%</td><td>84%</td><td>96%</td><td>98%</td><td>98%</td></tr><tr><td>TidalDecode+L13</td><td>42%</td><td>80%</td><td>94%</td><td>98%</td><td>100%</td></tr><tr><td>TidalDecode+L14</td><td>28%</td><td>44%</td><td>54%</td><td>60%</td><td>72%</td></tr><tr><td>TidalDecode+L15</td><td>2%</td><td>8%</td><td>16%</td><td>22%</td><td>36%</td></tr><tr><td>TidalDecode+L16</td><td>4%</td><td>16%</td><td>12%</td><td>22%</td><td>34%</td></tr><tr><td>TidalDecode+L17</td><td>2%</td><td>6%</td><td>16%</td><td>20%</td><td>32%</td></tr><tr><td>TidalDecode+L18</td><td>2%</td><td>10%</td><td>12%</td><td>18%</td><td>28%</td></tr><tr><td>TidalDecode+L19</td><td>2%</td><td>6%</td><td>10%</td><td>18%</td><td>32%</td></tr><tr><td>TidalDecode+L20</td><td>6%</td><td>10%</td><td>12%</td><td>18%</td><td>24%</td></tr><tr><td>TidalDecode+L21</td><td>6%</td><td>8%</td><td>10%</td><td>16%</td><td>26%</td></tr><tr><td>TidalDecode+L22</td><td>6%</td><td>0%</td><td>12%</td><td>12%</td><td>26%</td></tr><tr><td>TidalDecode+L23</td><td>4%</td><td>14%</td><td>14%</td><td>18%</td><td>26%</td></tr><tr><td>TidalDecode+L24</td><td>2%</td><td>10%</td><td>16%</td><td>20%</td><td>28%</td></tr><tr><td>TidalDecode+L25</td><td>4%</td><td>8%</td><td>14%</td><td>16%</td><td>22%</td></tr><tr><td>TidalDecode+L26</td><td>2%</td><td>10%</td><td>10%</td><td>22%</td><td>26%</td></tr><tr><td>TidalDecode+L27</td><td>0%</td><td>10%</td><td>12%</td><td>22%</td><td>26%</td></tr><tr><td>Quest</td><td>46%</td><td>56%</td><td>72%</td><td>88%</td><td>96%</td></tr></table>

Table 8: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.1-8B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 13 and Layer 14 are the best two re-selection layers for accuracy.   

<table><tr><td>RL/K</td><td>32</td><td>64</td><td>128</td><td>256</td></tr><tr><td>TidalDecode+L2</td><td>36%</td><td>38%</td><td>46%</td><td>58%</td></tr><tr><td>TidalDecode+L3</td><td>8%</td><td>10%</td><td>14%</td><td>34%</td></tr><tr><td>TidalDecode+L4</td><td>0%</td><td>10%</td><td>16%</td><td>34%</td></tr><tr><td>TidalDecode+L5</td><td>14%</td><td>30%</td><td>52%</td><td>52%</td></tr><tr><td>TidalDecode+L6</td><td>8%</td><td>12%</td><td>28%</td><td>40%</td></tr><tr><td>TidalDecode+L7</td><td>6%</td><td>10%</td><td>10%</td><td>18%</td></tr><tr><td>TidalDecode+L8</td><td>34%</td><td>44%</td><td>50%</td><td>66%</td></tr><tr><td>TidalDecode+L9</td><td>64%</td><td>78%</td><td>82%</td><td>90%</td></tr><tr><td>TidalDecode+L10</td><td>56%</td><td>74%</td><td>84%</td><td>94%</td></tr><tr><td>TidalDecode+L11</td><td>52%</td><td>76%</td><td>82%</td><td>86%</td></tr><tr><td>TidalDecode+L12</td><td>8%</td><td>10%</td><td>28%</td><td>40%</td></tr><tr><td>TidalDecode+L13</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TidalDecode+L14</td><td>98%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>TidalDecode+L15</td><td>56%</td><td>78%</td><td>88%</td><td>96%</td></tr><tr><td>TidalDecode+L16</td><td>18%</td><td>46%</td><td>54%</td><td>72%</td></tr><tr><td>TidalDecode+L17</td><td>64%</td><td>74%</td><td>86%</td><td>98%</td></tr><tr><td>TidalDecode+L18</td><td>64%</td><td>70%</td><td>74%</td><td>84%</td></tr><tr><td>TidalDecode+L19</td><td>58%</td><td>50%</td><td>60%</td><td>68%</td></tr><tr><td>TidalDecode+L20</td><td>68%</td><td>60%</td><td>62%</td><td>76%</td></tr><tr><td>TidalDecode+L21</td><td>40%</td><td>48%</td><td>48%</td><td>62%</td></tr><tr><td>TidalDecode+L22</td><td>28%</td><td>38%</td><td>46%</td><td>56%</td></tr><tr><td>TidalDecode+L23</td><td>40%</td><td>46%</td><td>52%</td><td>64%</td></tr><tr><td>TidalDecode+L24</td><td>30%</td><td>46%</td><td>54%</td><td>66%</td></tr><tr><td>TidalDecode+L25</td><td>40%</td><td>54%</td><td>50%</td><td>66%</td></tr><tr><td>TidalDecode+L26</td><td>34%</td><td>48%</td><td>62%</td><td>64%</td></tr><tr><td>TidalDecode+L27</td><td>38%</td><td>50%</td><td>54%</td><td>70%</td></tr><tr><td>TidalDecode+L28</td><td>30%</td><td>40%</td><td>56%</td><td>58%</td></tr><tr><td>TidalDecode+L29</td><td>32%</td><td>48%</td><td>56%</td><td>68%</td></tr><tr><td>TidalDecode+L30</td><td>36%</td><td>48%</td><td>52%</td><td>70%</td></tr><tr><td>TidalDecode+L31</td><td>30%</td><td>36%</td><td>42%</td><td>56%</td></tr></table>

Table 9: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3-70B-Instruct-Gradient-1048k; we first run top $\mathbf { k } = 5 1 2$ and filter out those layers that do not achieve full accuracy with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 14 and Layer 31 are the best two Re-selection layers for accuracy.   

<table><tr><td>RL/K</td><td>32</td><td>64</td><td>128</td><td>256</td><td>512 RL/K</td><td></td><td>32</td><td>64</td><td>128</td><td>256</td><td>512</td></tr><tr><td>L2</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>6% L33</td><td>50%</td><td>87%</td><td>90%</td><td>93%</td><td>100%</td></tr><tr><td>L3</td><td>-</td><td>-</td><td>-</td><td></td><td></td><td>37% L34</td><td>-</td><td>-</td><td>-</td><td>-</td><td>97%</td></tr><tr><td>L4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>23%</td><td>L35</td><td>70%</td><td>83%</td><td>100%</td><td>100%</td><td>100%</td></tr><tr><td>L5</td><td></td><td>-</td><td>-</td><td></td><td>63%</td><td>L36</td><td>50%</td><td>83%</td><td>97%</td><td>97%</td><td>100%</td></tr><tr><td>L6</td><td>-</td><td></td><td></td><td></td><td>70%</td><td>L37</td><td>-</td><td>-</td><td>-</td><td>-</td><td>90%</td></tr><tr><td>L7</td><td>-</td><td></td><td>-</td><td></td><td>90%</td><td>L38</td><td>37%</td><td>83%</td><td>83%</td><td>80%</td><td>100%</td></tr><tr><td>L8</td><td>-</td><td></td><td>-</td><td></td><td>70%</td><td>L39</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87%</td></tr><tr><td>L9</td><td>-</td><td></td><td>-</td><td></td><td>30%</td><td>L40</td><td>-</td><td>-</td><td>-</td><td>-</td><td>50%</td></tr><tr><td>L10</td><td></td><td>-</td><td>-</td><td></td><td>83%</td><td>L41</td><td></td><td>-</td><td>-</td><td>-</td><td>97%</td></tr><tr><td>L11</td><td>-</td><td></td><td>-</td><td></td><td>70%</td><td>L42</td><td></td><td>-</td><td></td><td></td><td>53%</td></tr><tr><td>L12</td><td>-</td><td>-</td><td>-</td><td></td><td>63%</td><td>L43</td><td></td><td>-</td><td>-</td><td>-</td><td>67%</td></tr><tr><td>L13</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>50%L44</td><td></td><td></td><td></td><td></td><td>83%</td></tr><tr><td>L14</td><td>87%</td><td>93%</td><td>100%</td><td>100%</td><td></td><td>100% L45</td><td></td><td></td><td></td><td></td><td>70%</td></tr><tr><td>L15</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>97%L46</td><td></td><td></td><td></td><td></td><td>63%</td></tr><tr><td>L16</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>63%L47</td><td></td><td></td><td></td><td></td><td>77%</td></tr><tr><td>L17</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>87%L48</td><td></td><td></td><td></td><td>-</td><td>97%</td></tr><tr><td>L18</td><td>50%</td><td>70%</td><td>83%</td><td>97%</td><td>100% L49</td><td></td><td></td><td></td><td></td><td></td><td>77%</td></tr><tr><td>L19</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>93%L50</td><td></td><td></td><td></td><td></td><td>70%</td></tr><tr><td>L20</td><td>-</td><td>-</td><td>-</td><td></td><td>87%L51</td><td></td><td></td><td></td><td></td><td></td><td>93%</td></tr><tr><td>L21</td><td>53%</td><td>80%</td><td>93%</td><td>97%</td><td>100% L52</td><td></td><td></td><td></td><td></td><td></td><td>77%</td></tr><tr><td>L22</td><td>-</td><td>-</td><td>-</td><td>-</td><td>97%L53</td><td></td><td></td><td></td><td></td><td></td><td>70%</td></tr><tr><td>L23</td><td>53%</td><td>93%</td><td>97%</td><td>100%</td><td>100% L54</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>L24</td><td>33%</td><td>60%</td><td>77%</td><td>93%</td><td>100%L55</td><td></td><td></td><td></td><td></td><td></td><td>60%</td></tr><tr><td>L25</td><td></td><td></td><td></td><td></td><td></td><td>80%L56</td><td></td><td></td><td></td><td></td><td>53%</td></tr><tr><td>L26</td><td>- -</td><td>- -</td><td>- -</td><td>- -</td><td></td><td>87%L57</td><td></td><td></td><td></td><td></td><td>87%</td></tr><tr><td>L27</td><td>50%</td><td>87%</td><td>93%</td><td>93%</td><td>100%L58</td><td></td><td></td><td></td><td></td><td></td><td>57% 50%</td></tr><tr><td>L28</td><td>80%</td><td>83%</td><td>93%</td><td>87%</td><td>100% L59</td><td></td><td></td><td></td><td></td><td></td><td>50%</td></tr><tr><td>L29</td><td></td><td>-</td><td>-</td><td>-</td><td></td><td>97%L60</td><td></td><td></td><td></td><td></td><td>57%</td></tr><tr><td>L30</td><td>- 33%</td><td>67%</td><td>80%</td><td>90%</td><td></td><td>100%L61</td><td></td><td></td><td></td><td></td><td>30%</td></tr><tr><td></td><td></td><td></td><td>100%</td><td>100%</td><td></td><td>100% L62</td><td></td><td></td><td></td><td></td><td>43%</td></tr><tr><td>L31 L32</td><td>90% 27%</td><td>97% 73%</td><td>80%</td><td>97%</td><td></td><td>100% L63</td><td></td><td></td><td></td><td></td><td>43%</td></tr></table>

Table 10: Sensitivity study of re-selection layer (RL) on $1 0 \mathrm { k \Omega }$ -context-length Needle-in-the-Haystack test for Llama-3-8B-Instruct-Gradient-1048k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 9, Layer 13, and Layer 14 are the best three re-selection layers for accuracy.   

<table><tr><td rowspan=1 colspan=1>RL/K</td><td rowspan=1 colspan=1>16</td><td rowspan=1 colspan=1>32</td><td rowspan=1 colspan=1>64</td><td rowspan=1 colspan=1>128</td><td rowspan=1 colspan=1>256</td><td rowspan=1 colspan=1>512</td></tr><tr><td rowspan=30 colspan=1>TidalDecode+L2TidalDecode+L3TidalDecode+L4TidalDecode+L5TidalDecode+L6TidalDecode+L7TidalDecode+L8TidalDecode+L9TidalDecode+L10TidalDecode+L11TidalDecode+L12TidalDecode+L13TidalDecode+L14TidalDecode+L15TidalDecode+L16TidalDecode+L17TidalDecode+L18TidalDecode+L19TidalDecode+L20TidalDecode+L21TidalDecode+L22TidalDecode+L23TidalDecode+L24TidalDecode+L25TidalDecode+L26TidalDecode+L27TidalDecode+L28TidalDecode+L29TidalDecode+L30TidalDecode+L31</td><td rowspan=1 colspan=1>78%</td><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>76%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>6%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>28%</td><td rowspan=1 colspan=1>64%</td></tr><tr><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>28%</td><td rowspan=1 colspan=1>68%</td><td rowspan=1 colspan=1>84%</td></tr><tr><td rowspan=2 colspan=1>10%4%</td><td rowspan=1 colspan=1>12%</td><td rowspan=1 colspan=1>32%</td><td rowspan=1 colspan=1>52%</td><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>80%</td></tr><tr><td rowspan=1 colspan=1>6%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>14%</td><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>24%</td></tr><tr><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>14%</td><td rowspan=1 colspan=1>28%</td></tr><tr><td rowspan=1 colspan=1>26%</td><td rowspan=1 colspan=1>64%</td><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>96%</td></tr><tr><td rowspan=1 colspan=1>52%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>76%</td><td rowspan=1 colspan=1>86%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>56%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>8%</td><td rowspan=1 colspan=1>14%</td><td rowspan=1 colspan=1>22%</td><td rowspan=1 colspan=1>44%</td><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>94%</td></tr><tr><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>68%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>100%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>44%</td><td rowspan=1 colspan=1>50%</td><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>82%</td><td rowspan=1 colspan=1>94%</td></tr><tr><td rowspan=1 colspan=1>42%</td><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>82%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>96%</td></tr><tr><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>58%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>82%</td><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>96%</td></tr><tr><td rowspan=1 colspan=1>64%</td><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>78%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>38%</td><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>94%</td></tr><tr><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>70%</td><td rowspan=1 colspan=1>68%</td><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>82%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>58%</td><td rowspan=1 colspan=1>78%</td><td rowspan=1 colspan=1>70%</td><td rowspan=1 colspan=1>86%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>98%</td></tr><tr><td rowspan=1 colspan=1>62%</td><td rowspan=1 colspan=1>58%</td><td rowspan=1 colspan=1>76%</td><td rowspan=1 colspan=1>70%</td><td rowspan=1 colspan=1>78%</td><td rowspan=1 colspan=1>92%</td></tr><tr><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>86%</td><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>82%</td><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>54%</td><td rowspan=1 colspan=1>64%</td><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>94%</td></tr><tr><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>66%</td><td rowspan=1 colspan=1>76%</td><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>94%</td></tr><tr><td rowspan=1 colspan=1>72%</td><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>96%</td></tr><tr><td rowspan=1 colspan=1>80%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>86%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>96%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>74%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>88%</td><td rowspan=1 colspan=1>84%</td><td rowspan=1 colspan=1>90%</td><td rowspan=1 colspan=1>96%</td></tr></table>

Table 11: Sensitivity study of re-selection layer (RL) on $3 \mathrm { k }$ -context-length Needle-in-the-Haystack test for LongChat-7b-v1.5-32k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy.   

<table><tr><td>RL/K</td><td>32</td><td>64</td><td>128</td><td>256</td></tr><tr><td>TidalDecode+L2</td><td>2%</td><td>2%</td><td>6%</td><td>54%</td></tr><tr><td>TidalDecode+L3</td><td>10%</td><td>52%</td><td>67%</td><td>78%</td></tr><tr><td>TidalDecode+L4</td><td>4%</td><td>36%</td><td>65%</td><td>76%</td></tr><tr><td>TidalDecode+L5</td><td>17%</td><td>87%</td><td>94%</td><td>99%</td></tr><tr><td>TidalDecode+L6</td><td>70%</td><td>96%</td><td>99%</td><td>99%</td></tr><tr><td>TidalDecode+L7</td><td>80%</td><td>98%</td><td>100%</td><td>100%</td></tr><tr><td>TidalDecode+L8</td><td>58%</td><td>82%</td><td>96%</td><td>96%</td></tr><tr><td>TidalDecode+L9</td><td>7%</td><td>31%</td><td>59%</td><td>71%</td></tr><tr><td>TidalDecode+L10</td><td>16%</td><td>59%</td><td>71%</td><td>78%</td></tr><tr><td>TidalDecode+L11</td><td>34%</td><td>61%</td><td>68%</td><td>77%</td></tr><tr><td>TidalDecode+L12</td><td>17%</td><td>32%</td><td>53%</td><td>77%</td></tr><tr><td>TidalDecode+L13</td><td>5%</td><td>10%</td><td>28%</td><td>48%</td></tr><tr><td>TidalDecode+L14</td><td>24%</td><td>41%</td><td>57%</td><td>64%</td></tr><tr><td>TidalDecode+L15</td><td>37%</td><td>47%</td><td>62%</td><td>69%</td></tr><tr><td>TidalDecode+L16</td><td>16%</td><td>24%</td><td>28%</td><td>46%</td></tr><tr><td>TidalDecode+L17</td><td>4%</td><td>4%</td><td>10%</td><td>34%</td></tr><tr><td>TidalDecode+L18</td><td>2%</td><td>3%</td><td>8%</td><td>15%</td></tr><tr><td>TidalDecode+L19</td><td>0%</td><td>1%</td><td>7%</td><td>19%</td></tr><tr><td>TidalDecode+L20</td><td>0%</td><td>3%</td><td>6%</td><td>20%</td></tr><tr><td>TidalDecode+L21</td><td>0%</td><td>2%</td><td>10%</td><td>19%</td></tr><tr><td>TidalDecode+L22</td><td>0%</td><td>4%</td><td>4%</td><td>18%</td></tr><tr><td>TidalDecode+L23</td><td>0%</td><td>2%</td><td>5%</td><td>13%</td></tr><tr><td>TidalDecode+L24</td><td>0%</td><td>2%</td><td>6%</td><td>21%</td></tr><tr><td>TidalDecode+L25</td><td>0%</td><td>2%</td><td>7%</td><td>16%</td></tr><tr><td>TidalDecode+L26</td><td>0%</td><td>1%</td><td>7%</td><td>19%</td></tr><tr><td>TidalDecode+L27</td><td>0%</td><td>2%</td><td>4%</td><td>21%</td></tr><tr><td>TidalDecode+L28</td><td>1%</td><td>2%</td><td>10%</td><td>17%</td></tr><tr><td>TidalDecode+L29</td><td>0%</td><td>3%</td><td>7%</td><td>16%</td></tr><tr><td>TidalDecode+L30</td><td>1%</td><td>1%</td><td>9%</td><td>15%</td></tr><tr><td>TidalDecode+L31</td><td>1%</td><td>2%</td><td>5%</td><td>16%</td></tr></table>

Table 12: Sensitivity study of re-selection layer (RL) on $3 \mathrm { k }$ -context-length Needle-in-the-Haystack test for Yarn-Llama-2-7b-128k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy.   

<table><tr><td rowspan=1 colspan=1>RL/K</td><td rowspan=1 colspan=1>32</td><td rowspan=1 colspan=1>64</td><td rowspan=1 colspan=1>128</td><td rowspan=1 colspan=1>256</td><td rowspan=1 colspan=1>512</td></tr><tr><td rowspan=1 colspan=1>TidalDecode+L2</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>3%</td><td rowspan=1 colspan=1>25%</td></tr><tr><td rowspan=29 colspan=1>TidalDecode+L3TidalDecode+L4TidalDecode+L5TidalDecode+L6TidalDecode+L7TidalDecode+L8TidalDecode+L9TidalDecode+L10TidalDecode+L11TidalDecode+L12TidalDecode+L13TidalDecode+L14TidalDecode+L15TidalDecode+L16TidalDecode+L17TidalDecode+L18TidalDecode+L19TidalDecode+L20TidalDecode+L21TidalDecode+L22TidalDecode+L23TidalDecode+L24TidalDecode+L25TidalDecode+L26TidalDecode+L27TidalDecode+L28TidalDecode+L29TidalDecode+L30TidalDecode+L31</td><td rowspan=1 colspan=1>11%</td><td rowspan=1 colspan=1>26%</td><td rowspan=1 colspan=1>39%</td><td rowspan=1 colspan=1>65%</td><td rowspan=1 colspan=1>85%</td></tr><tr><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>17%</td><td rowspan=1 colspan=1>34%</td><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>92%</td></tr><tr><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>42%</td><td rowspan=1 colspan=1>65%</td><td rowspan=1 colspan=1>87%</td><td rowspan=1 colspan=1>96%</td></tr><tr><td rowspan=1 colspan=1>73%</td><td rowspan=1 colspan=1>83%</td><td rowspan=1 colspan=1>89%</td><td rowspan=1 colspan=1>95%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>73%</td><td rowspan=1 colspan=1>95%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>98%</td><td rowspan=1 colspan=1>100%</td></tr><tr><td rowspan=1 colspan=1>87%</td><td rowspan=1 colspan=1>92%</td><td rowspan=1 colspan=1>97%</td><td rowspan=1 colspan=1>94%</td><td rowspan=1 colspan=1>99%</td></tr><tr><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>21%</td><td rowspan=1 colspan=1>43%</td><td rowspan=1 colspan=1>60%</td><td rowspan=1 colspan=1>95%</td></tr><tr><td rowspan=1 colspan=1>12%</td><td rowspan=1 colspan=1>31%</td><td rowspan=1 colspan=1>58%</td><td rowspan=1 colspan=1>69%</td><td rowspan=1 colspan=1>93%</td></tr><tr><td rowspan=1 colspan=1>20%</td><td rowspan=1 colspan=1>21%</td><td rowspan=1 colspan=1>46%</td><td rowspan=1 colspan=1>68%</td><td rowspan=1 colspan=1>97%</td></tr><tr><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>15%</td><td rowspan=1 colspan=1>28%</td><td rowspan=1 colspan=1>51%</td><td rowspan=1 colspan=1>92%</td></tr><tr><td rowspan=1 colspan=1>4%</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>20%</td><td rowspan=1 colspan=1>34%</td><td rowspan=1 colspan=1>88%</td></tr><tr><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>20%</td><td rowspan=1 colspan=1>49%</td><td rowspan=1 colspan=1>53%</td><td rowspan=1 colspan=1>91%</td></tr><tr><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>25%</td><td rowspan=1 colspan=1>44%</td><td rowspan=1 colspan=1>56%</td><td rowspan=1 colspan=1>90%</td></tr><tr><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>13%</td><td rowspan=1 colspan=1>21%</td><td rowspan=1 colspan=1>43%</td><td rowspan=1 colspan=1>86%</td></tr><tr><td rowspan=1 colspan=1>3%</td><td rowspan=1 colspan=1>4%</td><td rowspan=1 colspan=1>9%</td><td rowspan=1 colspan=1>16%</td><td rowspan=1 colspan=1>85%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>15%</td><td rowspan=1 colspan=1>84%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>3%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>80%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>79%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>77%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>8%</td><td rowspan=1 colspan=1>76%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>7%</td><td rowspan=1 colspan=1>74%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>9%</td><td rowspan=1 colspan=1>73%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>71%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>5%</td><td rowspan=1 colspan=1>70%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>3%</td><td rowspan=1 colspan=1>8%</td><td rowspan=1 colspan=1>68%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>9%</td><td rowspan=1 colspan=1>67%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>8%</td><td rowspan=1 colspan=1>65%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>1%</td><td rowspan=1 colspan=1>8%</td><td rowspan=1 colspan=1>64%</td></tr><tr><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>0%</td><td rowspan=1 colspan=1>2%</td><td rowspan=1 colspan=1>10%</td><td rowspan=1 colspan=1>62%</td></tr></table>