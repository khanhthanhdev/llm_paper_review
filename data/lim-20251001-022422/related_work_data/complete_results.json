{
  "submission_id": "lim-20251001-022422",
  "source_paper": {
    "paper_id": "lim-20251001-022422",
    "title": "LESS IS MORE: TRAINING-FREE SPARSE ATTENTION WITH GLOBAL LOCALITY FOR EFFICIENT REASONING",
    "abstract": "Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves-and in some cases improves-accuracy while achieving a 1.1\u00d7 average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2\u00d7 fewer tokens without accuracy loss, achieving a 1.13\u00d7 end-to-end speed-up compared to existing sparse attention methods. 1",
    "publication_date": "2024-10-01",
    "venue": "",
    "year": "2024",
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  "cited_papers": [
    {
      "paper_id": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
      "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
      "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.",
      "publication_date": "2024-03-14",
      "venue": "Conference on Machine Learning and Systems",
      "year": "2024",
      "citation_count": 82,
      "authors": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "48d73af1a5820c5c4fa56a7dc310ee6de7421a3f",
      "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": null,
      "citation_count": 1,
      "authors": "Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6027d9b7f875a42a36029e2d3b7308be6fa88bb1",
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.",
      "publication_date": "2024-10-21",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 40,
      "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, L\u00e9on Bottou, Zhihao Jia, Beidi Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "publication_date": "2025-01-22",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 3517,
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, R. Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, A. Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, C. Deng, Chenyu Zhang, C. Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. Cai, J. Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, K. Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, T. Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, W. Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, X. Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu-mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Ren, Z. Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen-guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "04b2f3d742f33c372df81d8af2ea34c8fec629fb",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "publication_date": "2025-06-10",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 4,
      "authors": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bd6f57cfd1d597fc107995df36601ec969d329a2",
      "title": "OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs",
      "abstract": null,
      "publication_date": null,
      "venue": "International Conference on Learning Representations",
      "year": "2025",
      "citation_count": 10,
      "authors": "Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, Sheng Guo",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d25b7c6e30725feaac3d28b584653cf61d50c5ad",
      "title": "Evaluating Step-by-step Reasoning Traces: A Survey",
      "abstract": "Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.",
      "publication_date": "2025-02-17",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 9,
      "authors": "Jinu Lee, J. Hockenmaier",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "publication_date": "2024-04-22",
      "venue": "Neural Information Processing Systems",
      "year": "2024",
      "citation_count": 274,
      "authors": "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr F. Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning",
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
      "publication_date": "2025-02-04",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 5,
      "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "61166a7545c7cadb1ec79c4744c348bac7644d41",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "publication_date": "2024-09-16",
      "venue": "arXiv.org",
      "year": "2024",
      "citation_count": 61,
      "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2f066326c0d5e5ecd7dc646224272f1e13948579",
      "title": "Efficient Inference for Large Reasoning Models: A Survey",
      "abstract": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance \\&efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs'inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
      "publication_date": "2025-03-29",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 31,
      "authors": "Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "210b0a3d76e93079cc51b03c4115fde545eea966",
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are\"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "publication_date": "2023-11-20",
      "venue": "arXiv.org",
      "year": "2023",
      "citation_count": 1096,
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .",
      "publication_date": "2024-06-16",
      "venue": "International Conference on Machine Learning",
      "year": "2024",
      "citation_count": 153,
      "authors": "Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
      "title": "Qwen3 Technical Report",
      "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
      "publication_date": "2025-05-14",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 808,
      "authors": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jingren Zhou, Junyan Lin, Kai Dang, Keqin Bao, Ke\u2010Pei Yang, Le Yu, Li-Chun Deng, Mei Li, Min Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shi-Qiang Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "publication_date": "2022-01-28",
      "venue": "Neural Information Processing Systems",
      "year": "2022",
      "citation_count": 11679,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, Denny Zhou",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fdc53c2c10742464087c0525f77e32604827a21d",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "publication_date": "2023-09-29",
      "venue": "International Conference on Learning Representations",
      "year": "2023",
      "citation_count": 965,
      "authors": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8f3d959238e67bf6b9bc9818025d0d2e403e478f",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "publication_date": "2024-10-14",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 105,
      "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a336bf38ebfdadd2992f9f976c963f7280c99d5f",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
      "publication_date": "2024-10-07",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 5,
      "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "23c2e21b6a5789724715f2986fdc586a517ffe57",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "publication_date": "2025-02-16",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": "2025",
      "citation_count": 113,
      "authors": "Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, C. Ruan, Ming Zhang, W. Liang, Wangding Zeng",
      "novel": null,
      "cited_paper": true
    }
  ],
  "query_papers": [
    {
      "paper_id": "882588a4fe6134bb5d474f7e1ca788f3eb7db2e2",
      "title": "Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs",
      "abstract": "Existing Large Vision-Language Models (LVLMs) primarily align image features of vision encoder with Large Language Models (LLMs) to leverage their superior text generation capabilities. However, the scale disparity between vision encoder and language model may led to LLMs assuming a predominant role in multi-modal comprehension. This imbalance in LVLMs may result in the instances of hallucinatory. Concretely, LVLMs may generate consistent descriptions with or without visual input, indicating that certain outputs are influenced solely by context text. We refer to this phenomenon as\"text inertia.\"To counteract this issue, we introduce a training-free algorithm to find an equilibrium point between image comprehension and language inference. Specifically, we adaptively involve adjusting and amplifying the attention weights assigned to image tokens, thereby granting greater prominence to visual elements. Meanwhile, we subtract the logits of multi-modal inputs from ones of pure text input, which can help LVLMs be not biased towards LLMs. By enhancing images tokens and reducing the stubborn output of LLM, we can let LVLM pay more attention to images, towards alleviating text inertia and reducing the hallucination in LVLMs. Our extensive experiments shows that this method substantially reduces the frequency of hallucinatory outputs in various LVLMs in terms of different metrics. Project page is available at https://lalbj.github.io/projects/PAI/.",
      "publication_date": "2024-07-31",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e10ba28cd6dab4a292acdab0a0733a89de1ba640",
      "title": "FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention",
      "abstract": "Video diffusion models have made substantial progress in various video generation applications. However, training models for long video generation tasks require significant computational and data resources, posing a challenge to developing long video diffusion models. This paper investigates a straightforward and training-free approach to extend an existing short video diffusion model (e.g. pre-trained on 16-frame videos) for consistent long video generation (e.g. 128 frames). Our preliminary observation has found that directly applying the short video diffusion model to generate long videos can lead to severe video quality degradation. Further investigation reveals that this degradation is primarily due to the distortion of high-frequency components in long videos, characterized by a decrease in spatial high-frequency components and an increase in temporal high-frequency components. Motivated by this, we propose a novel solution named FreeLong to balance the frequency distribution of long video features during the denoising process. FreeLong blends the low-frequency components of global video features, which encapsulate the entire video sequence, with the high-frequency components of local video features that focus on shorter subsequences of frames. This approach maintains global consistency while incorporating diverse and high-quality spatiotemporal details from local videos, enhancing both the consistency and fidelity of long video generation. We evaluated FreeLong on multiple base video diffusion models and observed significant improvements. Additionally, our method supports coherent multi-prompt generation, ensuring both visual coherence and seamless transitions between scenes.",
      "publication_date": "2024-07-29",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5c4f42e084e84a88bc8d964b613d4721618711a2",
      "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
      "abstract": "Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pretraining or access to additional pretrained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pretrained networks, or extensive hyperparameter tuning, making it highly practicalfor real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
      "publication_date": "2024-04-12",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09223db5260bc3c7486497ac3adc59b2d312a08",
      "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
      "abstract": "Diffusion models (DMs) have exhibited superior performance in generating high-quality and diverse images. How-ever, this exceptional performance comes at the cost of expensive generation process, particularly due to the heavily used attention module in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53\u00d7 speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.",
      "publication_date": "2024-05-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a195df2f7611aa75d5734b2efb32a408d2f8348",
      "title": "Training-Free Layout Control with Cross-Attention Guidance",
      "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.",
      "publication_date": "2023-04-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
      "publication_date": "2020-12-23",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2d9e1f3472f9e0e8f93a710b5141611395cd1e66",
      "title": "Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts",
      "abstract": "Existing perception models achieve great success by learning from large amounts of labeled data, but they still struggle with open-world scenarios. To alleviate this issue, researchers introduce open-set perception tasks to detect or segment unseen objects in the training set. However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios. Recently, researchers pose a new and more practical problem, \\textit{i.e.}, open-ended object detection, which discovers unseen objects without any object categories as inputs. In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (\\textit{i.e.,} Vision-Language Model) with the generalized object localization model (\\textit{i.e.,} Segment-Anything Model), to address the open-ended object detection and segmentation task. Without additional training, we connect these two generalized models with attention maps as the prompts. Specifically, we design an attention map generation module by employing head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in VLM, yielding high-quality attention maps. Then, we iteratively sample positive and negative points from the attention maps with a prompt generation module and send the sampled points to SAM to segment corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) show that our method surpasses the previous open-ended method on the object detection task and can provide additional instance segmentation masks. Besides, VL-SAM achieves favorable performance on the corner case object detection dataset (CODA), demonstrating the effectiveness of VL-SAM in real-world applications. Moreover, VL-SAM exhibits good model generalization that can incorporate various VLMs and SAMs.",
      "publication_date": "2024-10-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d",
      "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
      "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \\url{https://github.com/thunlp/InfLLM}.",
      "publication_date": "2024-02-07",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39ba6d541d94132b816938e7e16b1e8fd49c2fd9",
      "title": "Training-Free Consistent Text-to-Image Generation",
      "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
      "publication_date": "2024-02-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "25dcaff1caf5c80aa7c67a1a1a50fc9e1d5ad60c",
      "title": "HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads",
      "abstract": "Diffusion Transformers (DiTs) have exhibited robust capabilities in image generation tasks. However, accurate text-guided image editing for multimodal DiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-based structures that could utilize self/cross-attention maps for semantic editing, MM-DiTs inherently lack support for explicit and consistent incorporated text guidance, resulting in semantic misalignment between the edited results and texts. In this study, we disclose the sensitivity of different attention heads to different image semantics within MM-DiTs and introduce HeadRouter, a training-free image editing framework that edits the source image by adaptively routing the text guidance to different attention heads in MM-DiTs. Furthermore, we present a dual-token refinement module to refine text/image token representations for precise semantic guidance and accurate region expression. Experimental results on multiple benchmarks demonstrate HeadRouter's performance in terms of editing fidelity and image quality.",
      "publication_date": "2024-11-22",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e872ad446a3c6986feea7bd5cbd5b5754c068ab6",
      "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation",
      "abstract": "Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.",
      "publication_date": "2024-06-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1c5d98937eec4f4c1d8c2ffb35a317c8da674447",
      "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
      "publication_date": "2024-02-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ff7391dff405fc6f5aebd5d5f14257af19a0c26c",
      "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with Finite Attention Scope",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9f8a928167dbac891c01d3250409959c61ace23b",
      "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
      "abstract": "The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but the maximum supported context length in length extrapolation remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose ReAttention, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at https://github.com/OpenMOSS/ReAttention.",
      "publication_date": "2024-07-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee257e16d242b03eb75f44e99456f35027da893a",
      "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
      "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "925008328870faab8f371012da7277c658b166d4",
      "title": "Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models",
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) often use large image tokens to compensate the visual shortcoming of MLLMs, which not only exhibits obvious redundancy but also greatly exacerbates the already high computation. Token pruning is an effective solution for speeding up MLLMs, but when and how to drop tokens still remains a challenge. In this paper, we propose a novel and training-free approach for the effective visual token pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune considers token pruning as a statistical problem of MLLM and its objective is to find out an optimal pruning scheme that can minimize the divergence of the attention distributions before and after pruning. In practice, FitPrune can be quickly accomplished based on the attention statistics from a small batch of inference data, avoiding the expensive trials of MLLMs. According to the pruning recipe, an MLLM can directly remove the redundant visual tokens of different examples during inference. To validate FitPrune, we apply it to a set of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct extensive experiments on a set of benchmarks. The experimental results show that our FitPrune can not only reduce the computational complexity to a large extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in about 5 minutes.",
      "publication_date": "2024-09-16",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2e65e751dc422a5b7a634d1f4b10a62fce2bf1e",
      "title": "vid-TLDR: Training Free Token merging for Light-Weight Video Transformer",
      "abstract": "Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d07382cae7c767c9ed91477b18200968860ca2bb",
      "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
      "abstract": "The demand for stereo images increases as manufacturers launch more extended reality (XR) devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is training-free and straightforward to use with seamless integration into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight method for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layer Modifications to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.",
      "publication_date": "2024-03-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d9e13a19d97eb269d189a763adbfcd32e6333fb8",
      "title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation",
      "abstract": "Recent progress in interactive point prompt based Image Segmentation allows to significantly reduce the manual effort to obtain high quality semantic labels. State-of-the-art unsupervised methods use self-supervised pre-trained models to obtain pseudo-labels which are used in training a prompt-based segmentation model. In this paper, we propose a novel unsupervised and training-free approach based solely on the self-attention of Stable Diffusion. We interpret the self-attention tensor as a Markov transition operator, which enables us to iteratively construct a Markov chain. Pixel-wise counting of the required number of iterations along the Markov chain to reach a relative probability threshold yields a Markov-iteration-map, which we simply call a Markov-map. Compared to the raw attention maps, we show that our proposed Markov-map has less noise, sharper semantic boundaries and more uniform values within semantically similar regions. We integrate the Markov-map in a simple yet effective truncated nearest neighbor framework to obtain interactive point prompt based segmentation. Despite being training-free, we experimentally show that our approach yields excellent results in terms of Number of Clicks (NoC), even outperforming state-of-the-art training based unsupervised methods in most of the datasets. Code is available at https://github.com/mkarmann/m2n2.",
      "publication_date": "2024-11-15",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3d131fde9b571e6cda0183f286c7aaeb5d65c8bf",
      "title": "Global-locality preserving projection for word embedding",
      "abstract": null,
      "publication_date": "2022-06-16",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7cd3dea1be5fbfb58bf1949a9cd887f3ee8080b7",
      "title": "Temporal-Spatial Global Locality Projections for Multimode Process Monitoring",
      "abstract": "Multimode is an important feature of modern processes, since various manufacturing strategies are needed to satisfy different demands of markets. Direct application of traditional multivariate statistical process monitoring methods cannot obtain satisfactory results, as the data set collected from multimode processes always follows multimodal distribution. To construct a single model which can monitor multimode processes directly, this paper proposes an original algorithm named temporal\u2013spatial global locality projections. First, given that both temporal and spatial neighbors can express the similarity, the determination of the neighborhood is conducted in both the temporal and spatial scale. Second, an optimization objective function which preserves not only the local structure but also the global structure is defined. Third, the monitoring statistic is established via the local outlier factor. To certify the effectiveness, a numerical example, the multimode Tennessee Eastman process, and the CE117 process which is proposed by TecQuipment for process control are studied.",
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "30618f05160ebf08327ab88f0be3c524dce678c9",
      "title": "Supervised global-locality preserving projection for plant leaf recognition",
      "abstract": null,
      "publication_date": "2019-03-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "150c1fb3362486a1fdd79cc7963f7c8ea254f9bb",
      "title": "Global Locality in Biomedical Relation and Event Extraction",
      "abstract": "Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.",
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "867b2a9f7c1b5e2e411dd9211173bee42069db69",
      "title": "Global Locality in Event Extraction",
      "abstract": null,
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4347574a817aadd02a565ebff563f708c874e81c",
      "title": "Locality-Guided Global-Preserving Optimization for Robust Feature Matching",
      "abstract": "Feature matching is a fundamental problem in many computer vision tasks. This paper proposes a novel effective framework for mismatch removal, named LOcality-guided Global-preserving Optimization (LOGO). To identify inliers from a putative matching set generated by feature descriptor similarity, we introduce a fixed-point progressive approach to optimize a graph-based objective, which represents a two-class assignment problem regarding an affinity matrix containing global structures. We introduce a strategy that a small initial set with a high inlier ratio exploits the topology of the affinity matrix to elicit other inliers based on their reliable geometry, which enhances the robustness to outliers. Geometrically, we provide a locality-guided matching strategy, i.e., using local topology consensus as a criterion to determine the initial set, thus expanding to yield the final feature matching set. In addition, we apply local affine transformations based on reference points to determine the local consensus and similarity scores of nodes and edges, ensuring the validity and generality for various scenarios including complex nonrigid transformations. Extensive experiments demonstrate the effectiveness and robustness of the proposed LOGO, which is competitive with the current state-of-the-art methods. It also exhibits favorable potential for high-level vision tasks, such as essential and fundamental matrix estimation, image registration and loop closure detection.",
      "publication_date": "2022-07-27",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0f4e05bc5d7ba061587bfdbf888db7be3eb57834",
      "title": "Between the \"Global\" and the \"Local\": On Global Locality and Local Globality",
      "abstract": null,
      "publication_date": "2009-07-01",
      "venue": "",
      "year": 2009,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d767df4d8ba3f8709a4800b70ca60d4d3d6c9c87",
      "title": "A matrix-based approach to the global locality optimization problem",
      "abstract": null,
      "publication_date": "1998-10-12",
      "venue": "",
      "year": 1998,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ce5352e2a34197ae14b18619c9517e88421f7972",
      "title": "A Matrix-Based Approach to Global Locality Optimization",
      "abstract": null,
      "publication_date": "1999-08-01",
      "venue": "",
      "year": 1999,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "817a49d290ed85351b2fca3d20bcafa52016055c",
      "title": "Agent Selection And P2P Overlay Construction Using Global Locality Knowledge",
      "abstract": null,
      "publication_date": "2007-04-15",
      "venue": "",
      "year": 2007,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4dd10f293492a4f81b5ec8a29138df26159ecb99",
      "title": "Manufacturing the global locality, customizing the school and designing young workers",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2001,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "37324374e194bb4d07e1d0a5d81e778020afa776",
      "title": "LocalViT: Analyzing Locality in Vision Transformers",
      "abstract": "The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",
      "publication_date": "2021-04-12",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a5fdc7d8c9f5b0f3d1c687b905ef1948613a4bdb",
      "title": "Abandon Locality: Frame-Wise Embedding Aided Transformer for Automatic Modulation Recognition",
      "abstract": "Automatic modulation recognition (AMR) has been considered as an efficient technique for non-cooperative communication and intelligent communication. In this work, we propose a modified transformer-based method for AMR, called frame-wise embedding aided transformer (FEA-T), aiming to extract the global correlation feature of the signal to obtain higher classification accuracy as well as lower time cost. To enhance the global modeling capability of the transformer, we design a frame-wise embedding module (FEM) to aggregate more samples into a token in the embedding stage to generate a more efficient token sequence. We also present the optimal frame length by analyzing the representation ability of each transformer layer for a better trade-off between the speed and the performance. Moreover, we design a novel dual-branch gate linear unit (DB-GLU) scheme for the feed-forward network of the transformer to reduce the model size and enhance the performance. Experimental results on RadioML2018.01A datasets demonstrate that the proposed method outperforms state-of-the-art works in terms of recognition accuracy and running speed.",
      "publication_date": "2023-01-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b673df73f768db5738e3e34411504d0a10e078ac",
      "title": "The impact of cost function globality and locality in hybrid quantum neural networks on NISQ devices",
      "abstract": "Quantum neural networks (QNNs) are often challenged with the problem of flat cost function landscapes during training, known as barren plateaus (BP). A solution to potentially overcome the problem of the BP has recently been proposed by Cerezo et al In this solution, it is shown that, for an arbitrary deep quantum layer(s) in QNNs, a global cost function (all qubits measured in an n-qubit system) will always experience BP, whereas a local cost function (single qubit measured in an n-qubit system) can help to alleviate the problem of BP to a certain depth ( )). In this paper, we empirically analyze the locality and globality of the cost function in hybrid quantum neural networks. We consider two application scenarios namely, binary and multi-class classification, and show that for multiclass classification, the local cost function setting does not follow the claims of Cerezo et al; that is, the local cost function does not result in an extended quantum layer\u2019s depth. We also show that for multiclass classification, the overall performance in terms of accuracy for the global cost function setting is significantly higher than the local cost function setting. On the other hand, for binary classification, our results show that the local cost function setting follows the claims of Cerezo et al, and results in an extended depth of quantum layers. However, the global cost function setting still performs slightly better than the local cost function.",
      "publication_date": "2023-01-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9927002509ce571d321bdc93bd2a8e28a36363d3",
      "title": "On the notions of normality, locality, and operational stability in ADRC",
      "abstract": null,
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2a406ca9b535397fa9985ed00f1b127c29698967",
      "title": "Operational locality in global theories",
      "abstract": "Within a global physical theory, a notion of locality allows us to find and justify information-processing primitives, like non-signalling between distant agents. Here, we propose exploring the opposite direction: to take agents as the basic building blocks through which we test a physical theory, and recover operational notions of locality from signalling conditions. First, we introduce an operational model for the effective state spaces of individual agents, as well as the range of their actions. We then formulate natural secrecy conditions between agents and identify the aspects of locality relevant for signalling. We discuss the possibility of taking commutation of transformations as a primitive of physical theories, as well as applications to quantum theory and generalized probability frameworks. This \u2018it from bit\u2019 approach establishes an operational connection between local actions and local observations, and gives a global interpretation to concepts like discarding a subsystem or composing local functions. This article is part of a discussion meeting issue \u2018Foundations of quantum mechanics and their impact on contemporary society\u2019.",
      "publication_date": "2017-01-12",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d28fed119d9293af31776205150b3c34f3adc82b",
      "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
      "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "publication_date": "2022-05-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "33a7fd0b542f033031fdd8b49a787dd8c3755de0",
      "title": "Digital humor and the articulation of locality in an age of global flows",
      "abstract": null,
      "publication_date": "2016-02-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "07ef68619a089fee52efef382584bc6e30badf4a",
      "title": "Locality Guidance for Improving Vision Transformers on Tiny Datasets",
      "abstract": "While the Vision Transformer (VT) architecture is becoming trendy in computer vision, pure VT models perform poorly on tiny datasets. To address this issue, this paper proposes the locality guidance for improving the performance of VTs on tiny datasets. We first analyze that the local information, which is of great importance for understanding images, is hard to be learned with limited data due to the high flexibility and intrinsic globality of the self-attention mechanism in VTs. To facilitate local information, we realize the locality guidance for VTs by imitating the features of an already trained convolutional neural network (CNN), inspired by the built-in local-to-global hierarchy of CNN. Under our dual-task learning paradigm, the locality guidance provided by a lightweight CNN trained on low-resolution images is adequate to accelerate the convergence and improve the performance of VTs to a large extent. Therefore, our locality guidance approach is very simple and efficient, and can serve as a basic performance enhancement method for VTs on tiny datasets. Extensive experiments demonstrate that our method can significantly improve VTs when training from scratch on tiny datasets and is compatible with different kinds of VTs and datasets. For example, our proposed method can boost the performance of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85% for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing the potential of VTs on tiny datasets. The code is available at https://github.com/lkhl/tiny-transformers.",
      "publication_date": "2022-07-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
      "title": "Leveraging Locality in Abstractive Text Summarization",
      "abstract": "Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.",
      "publication_date": "2022-05-25",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fe56f4fc2bc24cb2b2eaf439454cc7253bbef6f3",
      "title": "Compressed Chain of Thought: Efficient Reasoning Through Dense Representations",
      "abstract": "Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the-shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.",
      "publication_date": "2024-12-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "19e00674c3b767f5388a7a9e080b2b97485099a4",
      "title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
      "abstract": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information of batch queries from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the overall cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.",
      "publication_date": "2024-08-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cccb9065f0a59e845d5b4d360ce31cf25036be",
      "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
      "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the\"answer consistency\"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.",
      "publication_date": "2023-10-04",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b4a6c010724f0459c9791018e34a982cf96987cf",
      "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
      "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info",
      "publication_date": "2023-05-19",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c993152d0858584dee2859c21e324ee811ec3991",
      "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
      "abstract": "Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose\"Synergy of Thoughts\"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.",
      "publication_date": "2024-02-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fc3c717987218662f49243e2be6bacc093dd47d8",
      "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
      "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
      "publication_date": "2024-02-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d5444633c7826af0dd149b4c9d367c191b4b4192",
      "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
      "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f52c5f1ec94e8a2bf27247bcde7893572c7d53d1",
      "title": "RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9ec9d316ba8d8dd731dd810f2a000d47d06924bf",
      "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
      "abstract": "A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models. Our project web page: https://sites.google.com/view/robomamba-web",
      "publication_date": "2024-06-06",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8fb92f51434543c4a8cd4980f84cf04552c712cc",
      "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
      "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
      "publication_date": "2024-01-30",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "65332c006c114a9eb8f1a848f4aad7214bba6514",
      "title": "Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "75062b58398b6e9409e5fec855f6912534331eaf",
      "title": "Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling",
      "abstract": "Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.",
      "publication_date": "2024-08-30",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "90027ca7802645671a69b00b65e1fa94e6b63544",
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
      "publication_date": "2023-05-23",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5cfa5e54d34ab13e596240003a90dded6802b1c4",
      "title": "Efficient Reasoning in Regular Boardgames",
      "abstract": "We present the technical side of reasoning in Regular Boardgames (RBG) language \u2013 a universal General Game Playing (GGP) formalism for the class of finite deterministic games with perfect information, encoding rules in the form of regular expressions. RBG serves as a research tool that aims to aid in the development of generalized algorithms for knowledge inference, analysis, generation, learning, and playing games. In all these tasks, both generality and efficiency are important.In the first part, this paper describes optimizations used by the RBG compiler. The impact of these optimizations ranges from 1.7 to even 33-fold efficiency improvement when measuring the number of possible game playouts per second. Then, we perform an in-depth efficiency comparison with three other modern GGP systems (GDL, Ludii, Ai Ai). We also include our own highly optimized game-specific reasoners to provide a point of reference of the maximum speed. Our experiments show that RBG is currently the fastest among the abstract general game playing languages, and its efficiency can be competitive to common interface-based systems that rely on handcrafted game-specific implementations. Finally, we discuss some issues and methodology of computing benchmarks like this.",
      "publication_date": "2020-06-15",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ae1f34f5a5bd2324bec25a2881d8710da6fc40d8",
      "title": "Efficient Reasoning for Inconsistent Horn Formulae",
      "abstract": null,
      "publication_date": "2016-11-09",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3552c1799a4afe5ee436b40ac04535804c059f9a",
      "title": "Efficient Reasoning with Constrained Goal Models",
      "abstract": null,
      "publication_date": "2017-04-19",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "42d21108b026d814d8243242459ea8d283c4d70d",
      "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events",
      "abstract": "Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles. In this paper, we create a novel dataset, SUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. Moreover, we propose Eclipse, a novel Efficient glimpse network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
      "publication_date": "2021-03-29",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27a256991068a946ae6f73d057a37e15c0fe7331",
      "title": "Efficient Reasoning With Consistent Proper Epistemic Knowledge Bases",
      "abstract": null,
      "publication_date": "2015-05-04",
      "venue": "",
      "year": 2015,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cb10356ec8c93518b16ce4ae0e0fb6b25c1298",
      "title": "The Incredible ELK - From Polynomial Procedures to Efficient Reasoning with \u2130\u2112 Ontologies",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2014,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "501f8a0200a12a6f3906c1e4f3f40715e0e7d23a",
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.",
      "publication_date": "2024-06-26",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9803d83bbb28d02fb01f00e0e05aa3c192a87255",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
      "publication_date": "2024-07-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "952fb6413499bc377faa51bf71e4d558ac6f6387",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}.",
      "publication_date": "2024-06-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4c07dc79976a4e3a558bb6fcd0d615673e8ecef",
      "title": "Loki: Low-Rank Keys for Efficient Sparse Attention",
      "abstract": "Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.",
      "publication_date": "2024-06-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ca0bf57f025fbcc993f282b7f130ddc2fd985e7d",
      "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
      "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity hinders efficiency and scalability, especially for long-context processing. A promising approach is to leverage sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics at the attention head level, struggling to adapt dynamically to different contexts efficiently. We propose SeerAttention, a simple yet effective attention mechanism that directly learns the block-level attention sparsity from the LLM itself. Inspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention augments the conventional attention with a learnable gate that selectively activates important blocks within the attention map. Specifically, the gate first pools the query (Q) and key (K) tensors along the sequence dimension and processes them through learnable linear layers. The resulting matrices are then multiplied together to produce the gating scores, which are used to predict block-level attention sparsity. Combined with our block-sparse FlashAttention kernel, SeerAttention can achieve significant speedup on GPUs. When applied to pre-trained LLMs, SeerAttention only requires training the gate parameters in a lightweight self-distillation manner, allowing rapid convergence. Our evaluation results demonstrate that SeerAttention achieves better model accuracy and lower latency for long-context pre-filling compared to prior methods. Code is available at: https://github.com/microsoft/SeerAttention",
      "publication_date": "2024-10-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee2b3f7703b553b487428862b83995ea3e8c0c3a",
      "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
      "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
      "publication_date": "2024-06-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "76eef89e0a05c4b67f19697e233916f4b3a77ff4",
      "title": "ASADI: Accelerating Sparse Attention Using Diagonal-based In-Situ Computing",
      "abstract": "The self-attention mechanism is the performance bottleneck of Transformer-based language models, particularly for long sequences. Researchers have proposed using sparse attention to speed up the Transformer. However, sparse attention introduces significant random access overhead, limiting computational efficiency. To mitigate this issue, researchers attempt to improve data reuse by utilizing row/column locality. Unfortunately, we find that sparse attention does not naturally exhibit strong row/column locality, but instead has excellent diagonal locality. Thus, it is worthwhile to use diagonal compression (DIA) format. However, existing sparse matrix computation paradigms struggle to efficiently support DIA format in attention computation. To address this problem, we propose ASADI, a novel software-hardware co-designed sparse attention accelerator. In the soft-ware side, we propose a new sparse matrix computation paradigm that directly supports the DIA format in self-attention computation. In the hardware side, we present a novel sparse attention accelerator that efficiently implements our computation paradigm using highly parallel in-situ computing. We thoroughly evaluate ASADI across various models and datasets. Our experimental results demonstrate an average performance improvement of 18.6 \u00d7 and energy savings of 2.9\u00d7 compared to a PIM-based baseline.",
      "publication_date": "2024-03-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7ddda143b08430ce3f07ecaf836c755e35fd3fbd",
      "title": "Post-Training Sparse Attention with Double Sparsity",
      "abstract": "The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces\"Double Sparsity,\"a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in attention operations and a 1.9$\\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.",
      "publication_date": "2024-08-11",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f57eba50dc96612ac1a1f574745619ed4c08dd18",
      "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
      "abstract": null,
      "publication_date": "2024-06-16",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "eca86a20639150786f37a67f69195f0875cba998",
      "title": "Adaptive Sparse Attention Wavelet Network for the Robust Open-Circuit Fault Diagnosis in PMSM Drives",
      "abstract": "Inverter open-circuit faults can cause significant changes in the output current of permanent magnet synchronous motor (PMSM), so many fault diagnosis methods based on convolutional neural networks (CNNs) take the current as monitor signals. However, there are some shortcomings in the application of CNNs for fault diagnosis. On the one hand, most of the traditional pooling methods confuse the different frequency components of the feature map due to ignoring the classical sampling theorem, which decreases the accuracy of fault diagnosis. On the other hand, the generalization performance of the diagnostic model is not good enough when the data distribution changes due to variations in the operating conditions of the drive system. To address these issues, this article proposes an open-circuit fault diagnosis method using an adaptive sparse attention wavelet network (ASAWN), where a novel pooling layer, termed as adaptive sparse attention wavelet pooling (ASAW-pooling) layer, is designed using discrete wavelet transform (DWT) and adaptive sparse attention (ASA) mechanism. The pooling layer improves the robustness of the diagnostic model by filtering out the high-frequency components of the current data and the redundant information of the attention mechanism. Experimental results show that the proposed method can effectively diagnose 22 types of open-circuit faults. Besides, a comparison with other models shows that the proposed model not only has high accuracy but also performs strong robustness.",
      "publication_date": null,
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b2c04e082a56c0eb70ed62bc36148919f665e1c",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.",
      "publication_date": "2024-06-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2439fef7e7e36a6fda4a2f39c1f85f0511d76f6a",
      "title": "HSR-Enhanced Sparse Attention Acceleration",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, but their performance on long-context tasks is often limited by the computational complexity of attention mechanisms. We introduce a novel approach to accelerate attention computation in LLMs, particularly for long-context scenarios. We leverage the inherent sparsity within attention mechanisms, both in conventional Softmax attention and ReLU attention (with $\\mathsf{ReLU}^\\alpha$ activation, $\\alpha \\in \\mathbb{N}_+$), to significantly reduce the running time complexity. Our method employs a Half-Space Reporting (HSR) data structure to identify non-zero or ``massively activated'' entries in the attention matrix. We present theoretical analyses for two key scenarios: generation decoding and prompt prefilling. Our approach achieves a running time of $O(mn^{4/5})$ significantly faster than the naive approach $O(mn)$ for generation decoding, where $n$ is the context length, $m$ is the query length, and $d$ is the hidden dimension. We can also reduce the running time for prompt prefilling from $O(mn)$ to $O(mn^{1 - 1 / \\lfloor d/2\\rfloor} + mn^{4/5})$. Our method introduces only provably negligible error for Softmax attention. This work represents a significant step towards enabling efficient long-context processing in LLMs.",
      "publication_date": "2024-10-14",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "657329c633709dd1ac34a30d57341b186b1a47c2",
      "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
      "publication_date": "2020-03-12",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3c358d9588674bab1649fc103515c1c7f7f29b16",
      "title": "ASA-Net: Adaptive Sparse Attention Network for Robust Electric Load Forecasting",
      "abstract": "Electric load forecasting (ELF) is always employed to perform power systems management. However, it is difficult to predict electric load due to the following issues: 1) electric load prediction is prone to external interference, e.g., temperature and weather; 2) the user behaviors are random, such as family gatherings and business rush orders; and 3) electric load consumption varies significantly in different time periods. To solve such problems, an adaptive sparse attention network (ASA-Net) is proposed for ELF, where the adaptive sparse spatial attention (ASSA) module is first designed to increase the anti-interference ability by capturing the detail change caused by external interference; next, the adaptive sparse channel attention (ASCA) module is developed to enhance the tolerance to local outliers by learning their feature information; and finally, the adaptive sparse batch attention (ASBA) module is devised to model the dependencies of the timestamp to reduce the time impact on ELF. Experiments conducted on the benchmarks show the excellent performance of ASA-Net for ELF, and it can further provide valuable assistance for the smart grid.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a37e55b6bb39b50a31ac47100fb2f7ce10cc725b",
      "title": "Supplementary File: Image Super-Resolution with Non-Local Sparse Attention",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "095becc96744fd1ebcb2fe7941b439e681ac4f1f",
      "title": "Hardware\u2013Software Co-Design Enabling Static and Dynamic Sparse Attention Mechanisms",
      "abstract": null,
      "publication_date": "2024-09-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48e6f754c53f7a1c253576f8e18e33bbed0d546f",
      "title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency",
      "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM regime, as current large models will fail on simple tasks. Using a parity-learning setup, we demonstrate that CoT can substantially improve sample efficiency even when the representation power is sufficient. Specifically, with CoT, a transformer can learn the function within polynomial samples, whereas without CoT, the required sample size is exponential. Additionally, we show that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, and leads to a sparse and interpretable attention. We validate our theoretical analysis with both synthetic and real-world experiments, confirming that sparsity in attention layers is a key factor of the improvement induced by CoT.",
      "publication_date": "2024-10-07",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3e87524f02be2ff5f5a853ff5823616d59af5aac",
      "title": "Predicting and Understanding Student Learning Performance Using Multi-Source Sparse Attention Convolutional Neural Networks",
      "abstract": "Predicting and understanding student learning performance has been a long-standing task in learning science, which can benefit personalized teaching and learning. This study shows that the progress towards this task can be accelerated by using learning record data to feed a deep learning model that considers the intrinsic course association and the structured features. We proposed a multi-source sparse attention convolutional neural network (MsaCNN) to predict the course grades in a general formulation. MsaCNN adopts multi-scale convolution kernels on student grade records to capture structured features, a global attention strategy to discover the relationship between courses, and multiple input-heads to integrate multi-source features. All achieved features are then poured into a softmax classifier towards an end-to-end supervised deep learning model. Conducting insights into higher education on real-world university datasets, the results show that MsaCNN achieves better performance than traditional methods and delivers an interpretation of student performance by virtue of the resulted course relationships. Inspired by this interpretation, we created an association map for all mentioned courses, followed by evaluating the map with a questionnaire survey. This study provides computer-aided system tools and discovers the course-space map from the educational data, potentially facilitating the personalized learning progress.",
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "44c266ad28cdc779680044c7e87aacd5d342c702",
      "title": "MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications",
      "abstract": "Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.",
      "publication_date": "2023-06-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "21a1db2503dc844d71228309d2c3fd75e74d1e8f",
      "title": "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning",
      "abstract": "Redundancy is a persistent challenge in Capsule Networks (CapsNet), leading to high computational costs and parameter counts. Although previous studies have introduced pruning after the initial capsule layer, dynamic routing's fully connected nature and non-orthogonal weight matrices reintroduce redundancy in deeper layers. Besides, dynamic routing requires iterating to converge, further increasing computational demands. In this paper, we propose an Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter counts. Firstly, an efficient pruned capsule layer is introduced to discard redundant capsules. Secondly, dynamic routing is replaced with orthogonal sparse attention routing, eliminating the need for iterations and fully connected structures. Lastly, weight matrices during routing are orthogonalized to sustain low capsule similarity, which is the first approach to use Householder orthogonal decomposition to enforce orthogonality in CapsNet. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. OrthCaps-Shallow outperforms other Capsule Network benchmarks on four datasets, utilizing only 110k parameters - a mere 1.25% of a standard Capsule Network's total. To the best of our knowledge, $it$ achieves the smallest parameter count among existing Capsule Networks. Similarly, OrthCaps-Deep demonstrates competitive performance across four datasets, utilizing only 1.2% of the parameters required by its counterparts.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "224d2385eace559649f293756f9934f1750b3055",
      "title": "Aquaculture Water Quality Classification with Sparse Attention Transformers: Leveraging Water and Environmental Parameters",
      "abstract": "For aquaculture operations to be successful, water quality is essential. Maintaining a healthy aquaculture environment depends on the correct and timely evaluation of water quality based on both water parameters and environmental variables. Using deep learning and a sparse attention transformer model, this work provides a unique method for categorizing water quality in aquaculture. Aquaculture has always assessed water quality using crude rule-based techniques. This study shows how sophisticated machine learning methods, particularly sparse attention transformers, may be used to capture intricate connections between water parameter values and environmental influences. Sparse attention transformers make it possible to model lengthy sequences well and consider how several environmental variables, including temperature, dissolved oxygen, pH, and nutrient concentrations, are interdependent. A dataset that includes measurements of the water quality and the accompanying ambient condition over time is used to train the suggested model. The model may successfully filter out less significant data points by concentrating on limited windows of relevant information using a sparse attention mechanism. This dynamic attention mechanism adjusts to the temporal and geographical features of aquaculture systems, resulting in more precise and context-aware categorization of water quality. Importantly, this work makes use of IoT-based real-time data to provide the model a constant supply of input. The integration of real-time data ensures that the model's predictions are not only accurate but also timely, enabling rapid responses to changes in water quality conditions. The proposed model gives 99.79% accuracy whereas the existing DNN-LSTM gives 96.86%. The results of this study demonstrate the effectiveness of the deep learning-based sparse attention transformer model for water quality classification in aquaculture. By accurately predicting water quality status, aquaculture practitioners can proactively manage their systems, optimizing conditions for fish and aquatic organisms.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "final_ranked_papers": [
    {
      "paper_id": "23c2e21b6a5789724715f2986fdc586a517ffe57",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "publication_date": "2025-02-16",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": "2025",
      "citation_count": 113,
      "authors": "Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, C. Ruan, Ming Zhang, W. Liang, Wangding Zeng",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a336bf38ebfdadd2992f9f976c963f7280c99d5f",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
      "publication_date": "2024-10-07",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 5,
      "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8f3d959238e67bf6b9bc9818025d0d2e403e478f",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "publication_date": "2024-10-14",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 105,
      "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5b2c04e082a56c0eb70ed62bc36148919f665e1c",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.",
      "publication_date": "2024-06-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "publication_date": "2024-04-22",
      "venue": "Neural Information Processing Systems",
      "year": "2024",
      "citation_count": 274,
      "authors": "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr F. Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning",
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
      "publication_date": "2025-02-04",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 5,
      "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "04b2f3d742f33c372df81d8af2ea34c8fec629fb",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "publication_date": "2025-06-10",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 4,
      "authors": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ee2b3f7703b553b487428862b83995ea3e8c0c3a",
      "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
      "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
      "publication_date": "2024-06-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "952fb6413499bc377faa51bf71e4d558ac6f6387",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}.",
      "publication_date": "2024-06-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
      "publication_date": "2024-02-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6027d9b7f875a42a36029e2d3b7308be6fa88bb1",
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.",
      "publication_date": "2024-10-21",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 40,
      "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, L\u00e9on Bottou, Zhihao Jia, Beidi Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .",
      "publication_date": "2024-06-16",
      "venue": "International Conference on Machine Learning",
      "year": "2024",
      "citation_count": 153,
      "authors": "Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fdc53c2c10742464087c0525f77e32604827a21d",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "publication_date": "2023-09-29",
      "venue": "International Conference on Learning Representations",
      "year": "2023",
      "citation_count": 965,
      "authors": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "501f8a0200a12a6f3906c1e4f3f40715e0e7d23a",
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.",
      "publication_date": "2024-06-26",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "61166a7545c7cadb1ec79c4744c348bac7644d41",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "publication_date": "2024-09-16",
      "venue": "arXiv.org",
      "year": "2024",
      "citation_count": 61,
      "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",
      "novel": null,
      "cited_paper": true
    }
  ],
  "all_retrieved_papers": [
    {
      "paper_id": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
      "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
      "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.",
      "publication_date": "2024-03-14",
      "venue": "Conference on Machine Learning and Systems",
      "year": "2024",
      "citation_count": 82,
      "authors": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "48d73af1a5820c5c4fa56a7dc310ee6de7421a3f",
      "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": null,
      "citation_count": 1,
      "authors": "Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6027d9b7f875a42a36029e2d3b7308be6fa88bb1",
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.",
      "publication_date": "2024-10-21",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 40,
      "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, L\u00e9on Bottou, Zhihao Jia, Beidi Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "publication_date": "2025-01-22",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 3517,
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, R. Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, A. Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, C. Deng, Chenyu Zhang, C. Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. Cai, J. Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, K. Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, T. Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, W. Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, X. Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu-mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Ren, Z. Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen-guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "04b2f3d742f33c372df81d8af2ea34c8fec629fb",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "publication_date": "2025-06-10",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 4,
      "authors": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bd6f57cfd1d597fc107995df36601ec969d329a2",
      "title": "OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs",
      "abstract": null,
      "publication_date": null,
      "venue": "International Conference on Learning Representations",
      "year": "2025",
      "citation_count": 10,
      "authors": "Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, Sheng Guo",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d25b7c6e30725feaac3d28b584653cf61d50c5ad",
      "title": "Evaluating Step-by-step Reasoning Traces: A Survey",
      "abstract": "Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.",
      "publication_date": "2025-02-17",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 9,
      "authors": "Jinu Lee, J. Hockenmaier",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "publication_date": "2024-04-22",
      "venue": "Neural Information Processing Systems",
      "year": "2024",
      "citation_count": 274,
      "authors": "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr F. Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning",
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
      "publication_date": "2025-02-04",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 5,
      "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "61166a7545c7cadb1ec79c4744c348bac7644d41",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "publication_date": "2024-09-16",
      "venue": "arXiv.org",
      "year": "2024",
      "citation_count": 61,
      "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2f066326c0d5e5ecd7dc646224272f1e13948579",
      "title": "Efficient Inference for Large Reasoning Models: A Survey",
      "abstract": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance \\&efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs'inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
      "publication_date": "2025-03-29",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 31,
      "authors": "Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "210b0a3d76e93079cc51b03c4115fde545eea966",
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are\"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "publication_date": "2023-11-20",
      "venue": "arXiv.org",
      "year": "2023",
      "citation_count": 1096,
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .",
      "publication_date": "2024-06-16",
      "venue": "International Conference on Machine Learning",
      "year": "2024",
      "citation_count": 153,
      "authors": "Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
      "title": "Qwen3 Technical Report",
      "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
      "publication_date": "2025-05-14",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 808,
      "authors": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jingren Zhou, Junyan Lin, Kai Dang, Keqin Bao, Ke\u2010Pei Yang, Le Yu, Li-Chun Deng, Mei Li, Min Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shi-Qiang Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "publication_date": "2022-01-28",
      "venue": "Neural Information Processing Systems",
      "year": "2022",
      "citation_count": 11679,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, Denny Zhou",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fdc53c2c10742464087c0525f77e32604827a21d",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "publication_date": "2023-09-29",
      "venue": "International Conference on Learning Representations",
      "year": "2023",
      "citation_count": 965,
      "authors": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8f3d959238e67bf6b9bc9818025d0d2e403e478f",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "publication_date": "2024-10-14",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 105,
      "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a336bf38ebfdadd2992f9f976c963f7280c99d5f",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
      "publication_date": "2024-10-07",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 5,
      "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "23c2e21b6a5789724715f2986fdc586a517ffe57",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "publication_date": "2025-02-16",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": "2025",
      "citation_count": 113,
      "authors": "Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, C. Ruan, Ming Zhang, W. Liang, Wangding Zeng",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5c4f42e084e84a88bc8d964b613d4721618711a2",
      "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
      "abstract": "Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pretraining or access to additional pretrained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pretrained networks, or extensive hyperparameter tuning, making it highly practicalfor real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
      "publication_date": "2024-04-12",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09223db5260bc3c7486497ac3adc59b2d312a08",
      "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
      "abstract": "Diffusion models (DMs) have exhibited superior performance in generating high-quality and diverse images. How-ever, this exceptional performance comes at the cost of expensive generation process, particularly due to the heavily used attention module in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53\u00d7 speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.",
      "publication_date": "2024-05-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a195df2f7611aa75d5734b2efb32a408d2f8348",
      "title": "Training-Free Layout Control with Cross-Attention Guidance",
      "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.",
      "publication_date": "2023-04-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
      "publication_date": "2020-12-23",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d",
      "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
      "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \\url{https://github.com/thunlp/InfLLM}.",
      "publication_date": "2024-02-07",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39ba6d541d94132b816938e7e16b1e8fd49c2fd9",
      "title": "Training-Free Consistent Text-to-Image Generation",
      "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
      "publication_date": "2024-02-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e872ad446a3c6986feea7bd5cbd5b5754c068ab6",
      "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation",
      "abstract": "Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.",
      "publication_date": "2024-06-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
      "publication_date": "2024-02-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee257e16d242b03eb75f44e99456f35027da893a",
      "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
      "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2e65e751dc422a5b7a634d1f4b10a62fce2bf1e",
      "title": "vid-TLDR: Training Free Token merging for Light-Weight Video Transformer",
      "abstract": "Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d07382cae7c767c9ed91477b18200968860ca2bb",
      "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
      "abstract": "The demand for stereo images increases as manufacturers launch more extended reality (XR) devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is training-free and straightforward to use with seamless integration into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight method for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layer Modifications to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.",
      "publication_date": "2024-03-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3d131fde9b571e6cda0183f286c7aaeb5d65c8bf",
      "title": "Global-locality preserving projection for word embedding",
      "abstract": null,
      "publication_date": "2022-06-16",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7cd3dea1be5fbfb58bf1949a9cd887f3ee8080b7",
      "title": "Temporal-Spatial Global Locality Projections for Multimode Process Monitoring",
      "abstract": "Multimode is an important feature of modern processes, since various manufacturing strategies are needed to satisfy different demands of markets. Direct application of traditional multivariate statistical process monitoring methods cannot obtain satisfactory results, as the data set collected from multimode processes always follows multimodal distribution. To construct a single model which can monitor multimode processes directly, this paper proposes an original algorithm named temporal\u2013spatial global locality projections. First, given that both temporal and spatial neighbors can express the similarity, the determination of the neighborhood is conducted in both the temporal and spatial scale. Second, an optimization objective function which preserves not only the local structure but also the global structure is defined. Third, the monitoring statistic is established via the local outlier factor. To certify the effectiveness, a numerical example, the multimode Tennessee Eastman process, and the CE117 process which is proposed by TecQuipment for process control are studied.",
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "30618f05160ebf08327ab88f0be3c524dce678c9",
      "title": "Supervised global-locality preserving projection for plant leaf recognition",
      "abstract": null,
      "publication_date": "2019-03-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "150c1fb3362486a1fdd79cc7963f7c8ea254f9bb",
      "title": "Global Locality in Biomedical Relation and Event Extraction",
      "abstract": "Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.",
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "867b2a9f7c1b5e2e411dd9211173bee42069db69",
      "title": "Global Locality in Event Extraction",
      "abstract": null,
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4347574a817aadd02a565ebff563f708c874e81c",
      "title": "Locality-Guided Global-Preserving Optimization for Robust Feature Matching",
      "abstract": "Feature matching is a fundamental problem in many computer vision tasks. This paper proposes a novel effective framework for mismatch removal, named LOcality-guided Global-preserving Optimization (LOGO). To identify inliers from a putative matching set generated by feature descriptor similarity, we introduce a fixed-point progressive approach to optimize a graph-based objective, which represents a two-class assignment problem regarding an affinity matrix containing global structures. We introduce a strategy that a small initial set with a high inlier ratio exploits the topology of the affinity matrix to elicit other inliers based on their reliable geometry, which enhances the robustness to outliers. Geometrically, we provide a locality-guided matching strategy, i.e., using local topology consensus as a criterion to determine the initial set, thus expanding to yield the final feature matching set. In addition, we apply local affine transformations based on reference points to determine the local consensus and similarity scores of nodes and edges, ensuring the validity and generality for various scenarios including complex nonrigid transformations. Extensive experiments demonstrate the effectiveness and robustness of the proposed LOGO, which is competitive with the current state-of-the-art methods. It also exhibits favorable potential for high-level vision tasks, such as essential and fundamental matrix estimation, image registration and loop closure detection.",
      "publication_date": "2022-07-27",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0f4e05bc5d7ba061587bfdbf888db7be3eb57834",
      "title": "Between the \"Global\" and the \"Local\": On Global Locality and Local Globality",
      "abstract": null,
      "publication_date": "2009-07-01",
      "venue": "",
      "year": 2009,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d767df4d8ba3f8709a4800b70ca60d4d3d6c9c87",
      "title": "A matrix-based approach to the global locality optimization problem",
      "abstract": null,
      "publication_date": "1998-10-12",
      "venue": "",
      "year": 1998,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ce5352e2a34197ae14b18619c9517e88421f7972",
      "title": "A Matrix-Based Approach to Global Locality Optimization",
      "abstract": null,
      "publication_date": "1999-08-01",
      "venue": "",
      "year": 1999,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "817a49d290ed85351b2fca3d20bcafa52016055c",
      "title": "Agent Selection And P2P Overlay Construction Using Global Locality Knowledge",
      "abstract": null,
      "publication_date": "2007-04-15",
      "venue": "",
      "year": 2007,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4dd10f293492a4f81b5ec8a29138df26159ecb99",
      "title": "Manufacturing the global locality, customizing the school and designing young workers",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2001,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "37324374e194bb4d07e1d0a5d81e778020afa776",
      "title": "LocalViT: Analyzing Locality in Vision Transformers",
      "abstract": "The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",
      "publication_date": "2021-04-12",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a5fdc7d8c9f5b0f3d1c687b905ef1948613a4bdb",
      "title": "Abandon Locality: Frame-Wise Embedding Aided Transformer for Automatic Modulation Recognition",
      "abstract": "Automatic modulation recognition (AMR) has been considered as an efficient technique for non-cooperative communication and intelligent communication. In this work, we propose a modified transformer-based method for AMR, called frame-wise embedding aided transformer (FEA-T), aiming to extract the global correlation feature of the signal to obtain higher classification accuracy as well as lower time cost. To enhance the global modeling capability of the transformer, we design a frame-wise embedding module (FEM) to aggregate more samples into a token in the embedding stage to generate a more efficient token sequence. We also present the optimal frame length by analyzing the representation ability of each transformer layer for a better trade-off between the speed and the performance. Moreover, we design a novel dual-branch gate linear unit (DB-GLU) scheme for the feed-forward network of the transformer to reduce the model size and enhance the performance. Experimental results on RadioML2018.01A datasets demonstrate that the proposed method outperforms state-of-the-art works in terms of recognition accuracy and running speed.",
      "publication_date": "2023-01-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b673df73f768db5738e3e34411504d0a10e078ac",
      "title": "The impact of cost function globality and locality in hybrid quantum neural networks on NISQ devices",
      "abstract": "Quantum neural networks (QNNs) are often challenged with the problem of flat cost function landscapes during training, known as barren plateaus (BP). A solution to potentially overcome the problem of the BP has recently been proposed by Cerezo et al In this solution, it is shown that, for an arbitrary deep quantum layer(s) in QNNs, a global cost function (all qubits measured in an n-qubit system) will always experience BP, whereas a local cost function (single qubit measured in an n-qubit system) can help to alleviate the problem of BP to a certain depth ( )). In this paper, we empirically analyze the locality and globality of the cost function in hybrid quantum neural networks. We consider two application scenarios namely, binary and multi-class classification, and show that for multiclass classification, the local cost function setting does not follow the claims of Cerezo et al; that is, the local cost function does not result in an extended quantum layer\u2019s depth. We also show that for multiclass classification, the overall performance in terms of accuracy for the global cost function setting is significantly higher than the local cost function setting. On the other hand, for binary classification, our results show that the local cost function setting follows the claims of Cerezo et al, and results in an extended depth of quantum layers. However, the global cost function setting still performs slightly better than the local cost function.",
      "publication_date": "2023-01-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9927002509ce571d321bdc93bd2a8e28a36363d3",
      "title": "On the notions of normality, locality, and operational stability in ADRC",
      "abstract": null,
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2a406ca9b535397fa9985ed00f1b127c29698967",
      "title": "Operational locality in global theories",
      "abstract": "Within a global physical theory, a notion of locality allows us to find and justify information-processing primitives, like non-signalling between distant agents. Here, we propose exploring the opposite direction: to take agents as the basic building blocks through which we test a physical theory, and recover operational notions of locality from signalling conditions. First, we introduce an operational model for the effective state spaces of individual agents, as well as the range of their actions. We then formulate natural secrecy conditions between agents and identify the aspects of locality relevant for signalling. We discuss the possibility of taking commutation of transformations as a primitive of physical theories, as well as applications to quantum theory and generalized probability frameworks. This \u2018it from bit\u2019 approach establishes an operational connection between local actions and local observations, and gives a global interpretation to concepts like discarding a subsystem or composing local functions. This article is part of a discussion meeting issue \u2018Foundations of quantum mechanics and their impact on contemporary society\u2019.",
      "publication_date": "2017-01-12",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d28fed119d9293af31776205150b3c34f3adc82b",
      "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
      "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "publication_date": "2022-05-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "33a7fd0b542f033031fdd8b49a787dd8c3755de0",
      "title": "Digital humor and the articulation of locality in an age of global flows",
      "abstract": null,
      "publication_date": "2016-02-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "07ef68619a089fee52efef382584bc6e30badf4a",
      "title": "Locality Guidance for Improving Vision Transformers on Tiny Datasets",
      "abstract": "While the Vision Transformer (VT) architecture is becoming trendy in computer vision, pure VT models perform poorly on tiny datasets. To address this issue, this paper proposes the locality guidance for improving the performance of VTs on tiny datasets. We first analyze that the local information, which is of great importance for understanding images, is hard to be learned with limited data due to the high flexibility and intrinsic globality of the self-attention mechanism in VTs. To facilitate local information, we realize the locality guidance for VTs by imitating the features of an already trained convolutional neural network (CNN), inspired by the built-in local-to-global hierarchy of CNN. Under our dual-task learning paradigm, the locality guidance provided by a lightweight CNN trained on low-resolution images is adequate to accelerate the convergence and improve the performance of VTs to a large extent. Therefore, our locality guidance approach is very simple and efficient, and can serve as a basic performance enhancement method for VTs on tiny datasets. Extensive experiments demonstrate that our method can significantly improve VTs when training from scratch on tiny datasets and is compatible with different kinds of VTs and datasets. For example, our proposed method can boost the performance of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85% for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing the potential of VTs on tiny datasets. The code is available at https://github.com/lkhl/tiny-transformers.",
      "publication_date": "2022-07-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
      "title": "Leveraging Locality in Abstractive Text Summarization",
      "abstract": "Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.",
      "publication_date": "2022-05-25",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cccb9065f0a59e845d5b4d360ce31cf25036be",
      "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
      "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the\"answer consistency\"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.",
      "publication_date": "2023-10-04",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b4a6c010724f0459c9791018e34a982cf96987cf",
      "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
      "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info",
      "publication_date": "2023-05-19",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c993152d0858584dee2859c21e324ee811ec3991",
      "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
      "abstract": "Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose\"Synergy of Thoughts\"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.",
      "publication_date": "2024-02-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fc3c717987218662f49243e2be6bacc093dd47d8",
      "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
      "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
      "publication_date": "2024-02-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d5444633c7826af0dd149b4c9d367c191b4b4192",
      "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
      "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9ec9d316ba8d8dd731dd810f2a000d47d06924bf",
      "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
      "abstract": "A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models. Our project web page: https://sites.google.com/view/robomamba-web",
      "publication_date": "2024-06-06",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8fb92f51434543c4a8cd4980f84cf04552c712cc",
      "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
      "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
      "publication_date": "2024-01-30",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "90027ca7802645671a69b00b65e1fa94e6b63544",
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
      "publication_date": "2023-05-23",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5cfa5e54d34ab13e596240003a90dded6802b1c4",
      "title": "Efficient Reasoning in Regular Boardgames",
      "abstract": "We present the technical side of reasoning in Regular Boardgames (RBG) language \u2013 a universal General Game Playing (GGP) formalism for the class of finite deterministic games with perfect information, encoding rules in the form of regular expressions. RBG serves as a research tool that aims to aid in the development of generalized algorithms for knowledge inference, analysis, generation, learning, and playing games. In all these tasks, both generality and efficiency are important.In the first part, this paper describes optimizations used by the RBG compiler. The impact of these optimizations ranges from 1.7 to even 33-fold efficiency improvement when measuring the number of possible game playouts per second. Then, we perform an in-depth efficiency comparison with three other modern GGP systems (GDL, Ludii, Ai Ai). We also include our own highly optimized game-specific reasoners to provide a point of reference of the maximum speed. Our experiments show that RBG is currently the fastest among the abstract general game playing languages, and its efficiency can be competitive to common interface-based systems that rely on handcrafted game-specific implementations. Finally, we discuss some issues and methodology of computing benchmarks like this.",
      "publication_date": "2020-06-15",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ae1f34f5a5bd2324bec25a2881d8710da6fc40d8",
      "title": "Efficient Reasoning for Inconsistent Horn Formulae",
      "abstract": null,
      "publication_date": "2016-11-09",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3552c1799a4afe5ee436b40ac04535804c059f9a",
      "title": "Efficient Reasoning with Constrained Goal Models",
      "abstract": null,
      "publication_date": "2017-04-19",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "42d21108b026d814d8243242459ea8d283c4d70d",
      "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events",
      "abstract": "Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles. In this paper, we create a novel dataset, SUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. Moreover, we propose Eclipse, a novel Efficient glimpse network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
      "publication_date": "2021-03-29",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27a256991068a946ae6f73d057a37e15c0fe7331",
      "title": "Efficient Reasoning With Consistent Proper Epistemic Knowledge Bases",
      "abstract": null,
      "publication_date": "2015-05-04",
      "venue": "",
      "year": 2015,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cb10356ec8c93518b16ce4ae0e0fb6b25c1298",
      "title": "The Incredible ELK - From Polynomial Procedures to Efficient Reasoning with \u2130\u2112 Ontologies",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2014,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "501f8a0200a12a6f3906c1e4f3f40715e0e7d23a",
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.",
      "publication_date": "2024-06-26",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9803d83bbb28d02fb01f00e0e05aa3c192a87255",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
      "publication_date": "2024-07-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "952fb6413499bc377faa51bf71e4d558ac6f6387",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}.",
      "publication_date": "2024-06-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4c07dc79976a4e3a558bb6fcd0d615673e8ecef",
      "title": "Loki: Low-Rank Keys for Efficient Sparse Attention",
      "abstract": "Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.",
      "publication_date": "2024-06-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee2b3f7703b553b487428862b83995ea3e8c0c3a",
      "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
      "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
      "publication_date": "2024-06-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "76eef89e0a05c4b67f19697e233916f4b3a77ff4",
      "title": "ASADI: Accelerating Sparse Attention Using Diagonal-based In-Situ Computing",
      "abstract": "The self-attention mechanism is the performance bottleneck of Transformer-based language models, particularly for long sequences. Researchers have proposed using sparse attention to speed up the Transformer. However, sparse attention introduces significant random access overhead, limiting computational efficiency. To mitigate this issue, researchers attempt to improve data reuse by utilizing row/column locality. Unfortunately, we find that sparse attention does not naturally exhibit strong row/column locality, but instead has excellent diagonal locality. Thus, it is worthwhile to use diagonal compression (DIA) format. However, existing sparse matrix computation paradigms struggle to efficiently support DIA format in attention computation. To address this problem, we propose ASADI, a novel software-hardware co-designed sparse attention accelerator. In the soft-ware side, we propose a new sparse matrix computation paradigm that directly supports the DIA format in self-attention computation. In the hardware side, we present a novel sparse attention accelerator that efficiently implements our computation paradigm using highly parallel in-situ computing. We thoroughly evaluate ASADI across various models and datasets. Our experimental results demonstrate an average performance improvement of 18.6 \u00d7 and energy savings of 2.9\u00d7 compared to a PIM-based baseline.",
      "publication_date": "2024-03-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f57eba50dc96612ac1a1f574745619ed4c08dd18",
      "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
      "abstract": null,
      "publication_date": "2024-06-16",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b2c04e082a56c0eb70ed62bc36148919f665e1c",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.",
      "publication_date": "2024-06-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "657329c633709dd1ac34a30d57341b186b1a47c2",
      "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
      "publication_date": "2020-03-12",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3c358d9588674bab1649fc103515c1c7f7f29b16",
      "title": "ASA-Net: Adaptive Sparse Attention Network for Robust Electric Load Forecasting",
      "abstract": "Electric load forecasting (ELF) is always employed to perform power systems management. However, it is difficult to predict electric load due to the following issues: 1) electric load prediction is prone to external interference, e.g., temperature and weather; 2) the user behaviors are random, such as family gatherings and business rush orders; and 3) electric load consumption varies significantly in different time periods. To solve such problems, an adaptive sparse attention network (ASA-Net) is proposed for ELF, where the adaptive sparse spatial attention (ASSA) module is first designed to increase the anti-interference ability by capturing the detail change caused by external interference; next, the adaptive sparse channel attention (ASCA) module is developed to enhance the tolerance to local outliers by learning their feature information; and finally, the adaptive sparse batch attention (ASBA) module is devised to model the dependencies of the timestamp to reduce the time impact on ELF. Experiments conducted on the benchmarks show the excellent performance of ASA-Net for ELF, and it can further provide valuable assistance for the smart grid.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a37e55b6bb39b50a31ac47100fb2f7ce10cc725b",
      "title": "Supplementary File: Image Super-Resolution with Non-Local Sparse Attention",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3e87524f02be2ff5f5a853ff5823616d59af5aac",
      "title": "Predicting and Understanding Student Learning Performance Using Multi-Source Sparse Attention Convolutional Neural Networks",
      "abstract": "Predicting and understanding student learning performance has been a long-standing task in learning science, which can benefit personalized teaching and learning. This study shows that the progress towards this task can be accelerated by using learning record data to feed a deep learning model that considers the intrinsic course association and the structured features. We proposed a multi-source sparse attention convolutional neural network (MsaCNN) to predict the course grades in a general formulation. MsaCNN adopts multi-scale convolution kernels on student grade records to capture structured features, a global attention strategy to discover the relationship between courses, and multiple input-heads to integrate multi-source features. All achieved features are then poured into a softmax classifier towards an end-to-end supervised deep learning model. Conducting insights into higher education on real-world university datasets, the results show that MsaCNN achieves better performance than traditional methods and delivers an interpretation of student performance by virtue of the resulted course relationships. Inspired by this interpretation, we created an association map for all mentioned courses, followed by evaluating the map with a questionnaire survey. This study provides computer-aided system tools and discovers the course-space map from the educational data, potentially facilitating the personalized learning progress.",
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "44c266ad28cdc779680044c7e87aacd5d342c702",
      "title": "MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications",
      "abstract": "Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.",
      "publication_date": "2023-06-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "21a1db2503dc844d71228309d2c3fd75e74d1e8f",
      "title": "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning",
      "abstract": "Redundancy is a persistent challenge in Capsule Networks (CapsNet), leading to high computational costs and parameter counts. Although previous studies have introduced pruning after the initial capsule layer, dynamic routing's fully connected nature and non-orthogonal weight matrices reintroduce redundancy in deeper layers. Besides, dynamic routing requires iterating to converge, further increasing computational demands. In this paper, we propose an Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter counts. Firstly, an efficient pruned capsule layer is introduced to discard redundant capsules. Secondly, dynamic routing is replaced with orthogonal sparse attention routing, eliminating the need for iterations and fully connected structures. Lastly, weight matrices during routing are orthogonalized to sustain low capsule similarity, which is the first approach to use Householder orthogonal decomposition to enforce orthogonality in CapsNet. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. OrthCaps-Shallow outperforms other Capsule Network benchmarks on four datasets, utilizing only 110k parameters - a mere 1.25% of a standard Capsule Network's total. To the best of our knowledge, $it$ achieves the smallest parameter count among existing Capsule Networks. Similarly, OrthCaps-Deep demonstrates competitive performance across four datasets, utilizing only 1.2% of the parameters required by its counterparts.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "224d2385eace559649f293756f9934f1750b3055",
      "title": "Aquaculture Water Quality Classification with Sparse Attention Transformers: Leveraging Water and Environmental Parameters",
      "abstract": "For aquaculture operations to be successful, water quality is essential. Maintaining a healthy aquaculture environment depends on the correct and timely evaluation of water quality based on both water parameters and environmental variables. Using deep learning and a sparse attention transformer model, this work provides a unique method for categorizing water quality in aquaculture. Aquaculture has always assessed water quality using crude rule-based techniques. This study shows how sophisticated machine learning methods, particularly sparse attention transformers, may be used to capture intricate connections between water parameter values and environmental influences. Sparse attention transformers make it possible to model lengthy sequences well and consider how several environmental variables, including temperature, dissolved oxygen, pH, and nutrient concentrations, are interdependent. A dataset that includes measurements of the water quality and the accompanying ambient condition over time is used to train the suggested model. The model may successfully filter out less significant data points by concentrating on limited windows of relevant information using a sparse attention mechanism. This dynamic attention mechanism adjusts to the temporal and geographical features of aquaculture systems, resulting in more precise and context-aware categorization of water quality. Importantly, this work makes use of IoT-based real-time data to provide the model a constant supply of input. The integration of real-time data ensures that the model's predictions are not only accurate but also timely, enabling rapid responses to changes in water quality conditions. The proposed model gives 99.79% accuracy whereas the existing DNN-LSTM gives 96.86%. The results of this study demonstrate the effectiveness of the deep learning-based sparse attention transformer model for water quality classification in aquaculture. By accurately predicting water quality status, aquaculture practitioners can proactively manage their systems, optimizing conditions for fish and aquatic organisms.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "queries_used": [
    "training-free attention",
    "global locality",
    "efficient reasoning",
    "sparse attention"
  ],
  "total_cost": 0.00134,
  "general_ranking": [
    {
      "paper_id": "23c2e21b6a5789724715f2986fdc586a517ffe57",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "publication_date": "2025-02-16",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": "2025",
      "citation_count": 113,
      "authors": "Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, C. Ruan, Ming Zhang, W. Liang, Wangding Zeng",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a336bf38ebfdadd2992f9f976c963f7280c99d5f",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
      "publication_date": "2024-10-07",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 5,
      "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8f3d959238e67bf6b9bc9818025d0d2e403e478f",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "publication_date": "2024-10-14",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 105,
      "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5b2c04e082a56c0eb70ed62bc36148919f665e1c",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.",
      "publication_date": "2024-06-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "publication_date": "2024-04-22",
      "venue": "Neural Information Processing Systems",
      "year": "2024",
      "citation_count": 274,
      "authors": "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr F. Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning",
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
      "publication_date": "2025-02-04",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 5,
      "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "04b2f3d742f33c372df81d8af2ea34c8fec629fb",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "publication_date": "2025-06-10",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 4,
      "authors": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ee2b3f7703b553b487428862b83995ea3e8c0c3a",
      "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
      "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
      "publication_date": "2024-06-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "952fb6413499bc377faa51bf71e4d558ac6f6387",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}.",
      "publication_date": "2024-06-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
      "publication_date": "2024-02-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6027d9b7f875a42a36029e2d3b7308be6fa88bb1",
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.",
      "publication_date": "2024-10-21",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 40,
      "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, L\u00e9on Bottou, Zhihao Jia, Beidi Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .",
      "publication_date": "2024-06-16",
      "venue": "International Conference on Machine Learning",
      "year": "2024",
      "citation_count": 153,
      "authors": "Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fdc53c2c10742464087c0525f77e32604827a21d",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "publication_date": "2023-09-29",
      "venue": "International Conference on Learning Representations",
      "year": "2023",
      "citation_count": 965,
      "authors": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "501f8a0200a12a6f3906c1e4f3f40715e0e7d23a",
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.",
      "publication_date": "2024-06-26",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "61166a7545c7cadb1ec79c4744c348bac7644d41",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "publication_date": "2024-09-16",
      "venue": "arXiv.org",
      "year": "2024",
      "citation_count": 61,
      "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "9803d83bbb28d02fb01f00e0e05aa3c192a87255",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
      "publication_date": "2024-07-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
      "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
      "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.",
      "publication_date": "2024-03-14",
      "venue": "Conference on Machine Learning and Systems",
      "year": "2024",
      "citation_count": 82,
      "authors": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d",
      "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
      "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \\url{https://github.com/thunlp/InfLLM}.",
      "publication_date": "2024-02-07",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09223db5260bc3c7486497ac3adc59b2d312a08",
      "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
      "abstract": "Diffusion models (DMs) have exhibited superior performance in generating high-quality and diverse images. How-ever, this exceptional performance comes at the cost of expensive generation process, particularly due to the heavily used attention module in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53\u00d7 speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.",
      "publication_date": "2024-05-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "657329c633709dd1ac34a30d57341b186b1a47c2",
      "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
      "publication_date": "2020-03-12",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "90027ca7802645671a69b00b65e1fa94e6b63544",
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
      "publication_date": "2023-05-23",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4c07dc79976a4e3a558bb6fcd0d615673e8ecef",
      "title": "Loki: Low-Rank Keys for Efficient Sparse Attention",
      "abstract": "Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.",
      "publication_date": "2024-06-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c993152d0858584dee2859c21e324ee811ec3991",
      "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
      "abstract": "Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose\"Synergy of Thoughts\"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.",
      "publication_date": "2024-02-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "76eef89e0a05c4b67f19697e233916f4b3a77ff4",
      "title": "ASADI: Accelerating Sparse Attention Using Diagonal-based In-Situ Computing",
      "abstract": "The self-attention mechanism is the performance bottleneck of Transformer-based language models, particularly for long sequences. Researchers have proposed using sparse attention to speed up the Transformer. However, sparse attention introduces significant random access overhead, limiting computational efficiency. To mitigate this issue, researchers attempt to improve data reuse by utilizing row/column locality. Unfortunately, we find that sparse attention does not naturally exhibit strong row/column locality, but instead has excellent diagonal locality. Thus, it is worthwhile to use diagonal compression (DIA) format. However, existing sparse matrix computation paradigms struggle to efficiently support DIA format in attention computation. To address this problem, we propose ASADI, a novel software-hardware co-designed sparse attention accelerator. In the soft-ware side, we propose a new sparse matrix computation paradigm that directly supports the DIA format in self-attention computation. In the hardware side, we present a novel sparse attention accelerator that efficiently implements our computation paradigm using highly parallel in-situ computing. We thoroughly evaluate ASADI across various models and datasets. Our experimental results demonstrate an average performance improvement of 18.6 \u00d7 and energy savings of 2.9\u00d7 compared to a PIM-based baseline.",
      "publication_date": "2024-03-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2f066326c0d5e5ecd7dc646224272f1e13948579",
      "title": "Efficient Inference for Large Reasoning Models: A Survey",
      "abstract": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance \\&efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs'inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
      "publication_date": "2025-03-29",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 31,
      "authors": "Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "37324374e194bb4d07e1d0a5d81e778020afa776",
      "title": "LocalViT: Analyzing Locality in Vision Transformers",
      "abstract": "The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",
      "publication_date": "2021-04-12",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cccb9065f0a59e845d5b4d360ce31cf25036be",
      "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
      "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the\"answer consistency\"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.",
      "publication_date": "2023-10-04",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2e65e751dc422a5b7a634d1f4b10a62fce2bf1e",
      "title": "vid-TLDR: Training Free Token merging for Light-Weight Video Transformer",
      "abstract": "Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
      "title": "Leveraging Locality in Abstractive Text Summarization",
      "abstract": "Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.",
      "publication_date": "2022-05-25",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8fb92f51434543c4a8cd4980f84cf04552c712cc",
      "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
      "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
      "publication_date": "2024-01-30",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
      "title": "Qwen3 Technical Report",
      "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
      "publication_date": "2025-05-14",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 808,
      "authors": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jingren Zhou, Junyan Lin, Kai Dang, Keqin Bao, Ke\u2010Pei Yang, Le Yu, Li-Chun Deng, Mei Li, Min Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shi-Qiang Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
      "publication_date": "2020-12-23",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fc3c717987218662f49243e2be6bacc093dd47d8",
      "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
      "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
      "publication_date": "2024-02-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d5444633c7826af0dd149b4c9d367c191b4b4192",
      "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
      "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "21a1db2503dc844d71228309d2c3fd75e74d1e8f",
      "title": "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning",
      "abstract": "Redundancy is a persistent challenge in Capsule Networks (CapsNet), leading to high computational costs and parameter counts. Although previous studies have introduced pruning after the initial capsule layer, dynamic routing's fully connected nature and non-orthogonal weight matrices reintroduce redundancy in deeper layers. Besides, dynamic routing requires iterating to converge, further increasing computational demands. In this paper, we propose an Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter counts. Firstly, an efficient pruned capsule layer is introduced to discard redundant capsules. Secondly, dynamic routing is replaced with orthogonal sparse attention routing, eliminating the need for iterations and fully connected structures. Lastly, weight matrices during routing are orthogonalized to sustain low capsule similarity, which is the first approach to use Householder orthogonal decomposition to enforce orthogonality in CapsNet. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. OrthCaps-Shallow outperforms other Capsule Network benchmarks on four datasets, utilizing only 110k parameters - a mere 1.25% of a standard Capsule Network's total. To the best of our knowledge, $it$ achieves the smallest parameter count among existing Capsule Networks. Similarly, OrthCaps-Deep demonstrates competitive performance across four datasets, utilizing only 1.2% of the parameters required by its counterparts.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b4a6c010724f0459c9791018e34a982cf96987cf",
      "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
      "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info",
      "publication_date": "2023-05-19",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9ec9d316ba8d8dd731dd810f2a000d47d06924bf",
      "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
      "abstract": "A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models. Our project web page: https://sites.google.com/view/robomamba-web",
      "publication_date": "2024-06-06",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "07ef68619a089fee52efef382584bc6e30badf4a",
      "title": "Locality Guidance for Improving Vision Transformers on Tiny Datasets",
      "abstract": "While the Vision Transformer (VT) architecture is becoming trendy in computer vision, pure VT models perform poorly on tiny datasets. To address this issue, this paper proposes the locality guidance for improving the performance of VTs on tiny datasets. We first analyze that the local information, which is of great importance for understanding images, is hard to be learned with limited data due to the high flexibility and intrinsic globality of the self-attention mechanism in VTs. To facilitate local information, we realize the locality guidance for VTs by imitating the features of an already trained convolutional neural network (CNN), inspired by the built-in local-to-global hierarchy of CNN. Under our dual-task learning paradigm, the locality guidance provided by a lightweight CNN trained on low-resolution images is adequate to accelerate the convergence and improve the performance of VTs to a large extent. Therefore, our locality guidance approach is very simple and efficient, and can serve as a basic performance enhancement method for VTs on tiny datasets. Extensive experiments demonstrate that our method can significantly improve VTs when training from scratch on tiny datasets and is compatible with different kinds of VTs and datasets. For example, our proposed method can boost the performance of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85% for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing the potential of VTs on tiny datasets. The code is available at https://github.com/lkhl/tiny-transformers.",
      "publication_date": "2022-07-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "publication_date": "2022-01-28",
      "venue": "Neural Information Processing Systems",
      "year": "2022",
      "citation_count": 11679,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, Denny Zhou",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d28fed119d9293af31776205150b3c34f3adc82b",
      "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
      "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "publication_date": "2022-05-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "publication_date": "2025-01-22",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 3517,
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, R. Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, A. Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, C. Deng, Chenyu Zhang, C. Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. Cai, J. Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, K. Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, T. Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, W. Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, X. Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu-mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Ren, Z. Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen-guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5c4f42e084e84a88bc8d964b613d4721618711a2",
      "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
      "abstract": "Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pretraining or access to additional pretrained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pretrained networks, or extensive hyperparameter tuning, making it highly practicalfor real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
      "publication_date": "2024-04-12",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "44c266ad28cdc779680044c7e87aacd5d342c702",
      "title": "MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications",
      "abstract": "Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.",
      "publication_date": "2023-06-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d25b7c6e30725feaac3d28b584653cf61d50c5ad",
      "title": "Evaluating Step-by-step Reasoning Traces: A Survey",
      "abstract": "Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.",
      "publication_date": "2025-02-17",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 9,
      "authors": "Jinu Lee, J. Hockenmaier",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "39ba6d541d94132b816938e7e16b1e8fd49c2fd9",
      "title": "Training-Free Consistent Text-to-Image Generation",
      "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
      "publication_date": "2024-02-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a195df2f7611aa75d5734b2efb32a408d2f8348",
      "title": "Training-Free Layout Control with Cross-Attention Guidance",
      "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.",
      "publication_date": "2023-04-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "150c1fb3362486a1fdd79cc7963f7c8ea254f9bb",
      "title": "Global Locality in Biomedical Relation and Event Extraction",
      "abstract": "Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.",
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "210b0a3d76e93079cc51b03c4115fde545eea966",
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are\"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "publication_date": "2023-11-20",
      "venue": "arXiv.org",
      "year": "2023",
      "citation_count": 1096,
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3e87524f02be2ff5f5a853ff5823616d59af5aac",
      "title": "Predicting and Understanding Student Learning Performance Using Multi-Source Sparse Attention Convolutional Neural Networks",
      "abstract": "Predicting and understanding student learning performance has been a long-standing task in learning science, which can benefit personalized teaching and learning. This study shows that the progress towards this task can be accelerated by using learning record data to feed a deep learning model that considers the intrinsic course association and the structured features. We proposed a multi-source sparse attention convolutional neural network (MsaCNN) to predict the course grades in a general formulation. MsaCNN adopts multi-scale convolution kernels on student grade records to capture structured features, a global attention strategy to discover the relationship between courses, and multiple input-heads to integrate multi-source features. All achieved features are then poured into a softmax classifier towards an end-to-end supervised deep learning model. Conducting insights into higher education on real-world university datasets, the results show that MsaCNN achieves better performance than traditional methods and delivers an interpretation of student performance by virtue of the resulted course relationships. Inspired by this interpretation, we created an association map for all mentioned courses, followed by evaluating the map with a questionnaire survey. This study provides computer-aided system tools and discovers the course-space map from the educational data, potentially facilitating the personalized learning progress.",
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee257e16d242b03eb75f44e99456f35027da893a",
      "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
      "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5cfa5e54d34ab13e596240003a90dded6802b1c4",
      "title": "Efficient Reasoning in Regular Boardgames",
      "abstract": "We present the technical side of reasoning in Regular Boardgames (RBG) language \u2013 a universal General Game Playing (GGP) formalism for the class of finite deterministic games with perfect information, encoding rules in the form of regular expressions. RBG serves as a research tool that aims to aid in the development of generalized algorithms for knowledge inference, analysis, generation, learning, and playing games. In all these tasks, both generality and efficiency are important.In the first part, this paper describes optimizations used by the RBG compiler. The impact of these optimizations ranges from 1.7 to even 33-fold efficiency improvement when measuring the number of possible game playouts per second. Then, we perform an in-depth efficiency comparison with three other modern GGP systems (GDL, Ludii, Ai Ai). We also include our own highly optimized game-specific reasoners to provide a point of reference of the maximum speed. Our experiments show that RBG is currently the fastest among the abstract general game playing languages, and its efficiency can be competitive to common interface-based systems that rely on handcrafted game-specific implementations. Finally, we discuss some issues and methodology of computing benchmarks like this.",
      "publication_date": "2020-06-15",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3c358d9588674bab1649fc103515c1c7f7f29b16",
      "title": "ASA-Net: Adaptive Sparse Attention Network for Robust Electric Load Forecasting",
      "abstract": "Electric load forecasting (ELF) is always employed to perform power systems management. However, it is difficult to predict electric load due to the following issues: 1) electric load prediction is prone to external interference, e.g., temperature and weather; 2) the user behaviors are random, such as family gatherings and business rush orders; and 3) electric load consumption varies significantly in different time periods. To solve such problems, an adaptive sparse attention network (ASA-Net) is proposed for ELF, where the adaptive sparse spatial attention (ASSA) module is first designed to increase the anti-interference ability by capturing the detail change caused by external interference; next, the adaptive sparse channel attention (ASCA) module is developed to enhance the tolerance to local outliers by learning their feature information; and finally, the adaptive sparse batch attention (ASBA) module is devised to model the dependencies of the timestamp to reduce the time impact on ELF. Experiments conducted on the benchmarks show the excellent performance of ASA-Net for ELF, and it can further provide valuable assistance for the smart grid.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "42d21108b026d814d8243242459ea8d283c4d70d",
      "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events",
      "abstract": "Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles. In this paper, we create a novel dataset, SUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. Moreover, we propose Eclipse, a novel Efficient glimpse network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
      "publication_date": "2021-03-29",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a5fdc7d8c9f5b0f3d1c687b905ef1948613a4bdb",
      "title": "Abandon Locality: Frame-Wise Embedding Aided Transformer for Automatic Modulation Recognition",
      "abstract": "Automatic modulation recognition (AMR) has been considered as an efficient technique for non-cooperative communication and intelligent communication. In this work, we propose a modified transformer-based method for AMR, called frame-wise embedding aided transformer (FEA-T), aiming to extract the global correlation feature of the signal to obtain higher classification accuracy as well as lower time cost. To enhance the global modeling capability of the transformer, we design a frame-wise embedding module (FEM) to aggregate more samples into a token in the embedding stage to generate a more efficient token sequence. We also present the optimal frame length by analyzing the representation ability of each transformer layer for a better trade-off between the speed and the performance. Moreover, we design a novel dual-branch gate linear unit (DB-GLU) scheme for the feed-forward network of the transformer to reduce the model size and enhance the performance. Experimental results on RadioML2018.01A datasets demonstrate that the proposed method outperforms state-of-the-art works in terms of recognition accuracy and running speed.",
      "publication_date": "2023-01-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "224d2385eace559649f293756f9934f1750b3055",
      "title": "Aquaculture Water Quality Classification with Sparse Attention Transformers: Leveraging Water and Environmental Parameters",
      "abstract": "For aquaculture operations to be successful, water quality is essential. Maintaining a healthy aquaculture environment depends on the correct and timely evaluation of water quality based on both water parameters and environmental variables. Using deep learning and a sparse attention transformer model, this work provides a unique method for categorizing water quality in aquaculture. Aquaculture has always assessed water quality using crude rule-based techniques. This study shows how sophisticated machine learning methods, particularly sparse attention transformers, may be used to capture intricate connections between water parameter values and environmental influences. Sparse attention transformers make it possible to model lengthy sequences well and consider how several environmental variables, including temperature, dissolved oxygen, pH, and nutrient concentrations, are interdependent. A dataset that includes measurements of the water quality and the accompanying ambient condition over time is used to train the suggested model. The model may successfully filter out less significant data points by concentrating on limited windows of relevant information using a sparse attention mechanism. This dynamic attention mechanism adjusts to the temporal and geographical features of aquaculture systems, resulting in more precise and context-aware categorization of water quality. Importantly, this work makes use of IoT-based real-time data to provide the model a constant supply of input. The integration of real-time data ensures that the model's predictions are not only accurate but also timely, enabling rapid responses to changes in water quality conditions. The proposed model gives 99.79% accuracy whereas the existing DNN-LSTM gives 96.86%. The results of this study demonstrate the effectiveness of the deep learning-based sparse attention transformer model for water quality classification in aquaculture. By accurately predicting water quality status, aquaculture practitioners can proactively manage their systems, optimizing conditions for fish and aquatic organisms.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b673df73f768db5738e3e34411504d0a10e078ac",
      "title": "The impact of cost function globality and locality in hybrid quantum neural networks on NISQ devices",
      "abstract": "Quantum neural networks (QNNs) are often challenged with the problem of flat cost function landscapes during training, known as barren plateaus (BP). A solution to potentially overcome the problem of the BP has recently been proposed by Cerezo et al In this solution, it is shown that, for an arbitrary deep quantum layer(s) in QNNs, a global cost function (all qubits measured in an n-qubit system) will always experience BP, whereas a local cost function (single qubit measured in an n-qubit system) can help to alleviate the problem of BP to a certain depth ( )). In this paper, we empirically analyze the locality and globality of the cost function in hybrid quantum neural networks. We consider two application scenarios namely, binary and multi-class classification, and show that for multiclass classification, the local cost function setting does not follow the claims of Cerezo et al; that is, the local cost function does not result in an extended quantum layer\u2019s depth. We also show that for multiclass classification, the overall performance in terms of accuracy for the global cost function setting is significantly higher than the local cost function setting. On the other hand, for binary classification, our results show that the local cost function setting follows the claims of Cerezo et al, and results in an extended depth of quantum layers. However, the global cost function setting still performs slightly better than the local cost function.",
      "publication_date": "2023-01-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e872ad446a3c6986feea7bd5cbd5b5754c068ab6",
      "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation",
      "abstract": "Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.",
      "publication_date": "2024-06-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d07382cae7c767c9ed91477b18200968860ca2bb",
      "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
      "abstract": "The demand for stereo images increases as manufacturers launch more extended reality (XR) devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is training-free and straightforward to use with seamless integration into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight method for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layer Modifications to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.",
      "publication_date": "2024-03-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4347574a817aadd02a565ebff563f708c874e81c",
      "title": "Locality-Guided Global-Preserving Optimization for Robust Feature Matching",
      "abstract": "Feature matching is a fundamental problem in many computer vision tasks. This paper proposes a novel effective framework for mismatch removal, named LOcality-guided Global-preserving Optimization (LOGO). To identify inliers from a putative matching set generated by feature descriptor similarity, we introduce a fixed-point progressive approach to optimize a graph-based objective, which represents a two-class assignment problem regarding an affinity matrix containing global structures. We introduce a strategy that a small initial set with a high inlier ratio exploits the topology of the affinity matrix to elicit other inliers based on their reliable geometry, which enhances the robustness to outliers. Geometrically, we provide a locality-guided matching strategy, i.e., using local topology consensus as a criterion to determine the initial set, thus expanding to yield the final feature matching set. In addition, we apply local affine transformations based on reference points to determine the local consensus and similarity scores of nodes and edges, ensuring the validity and generality for various scenarios including complex nonrigid transformations. Extensive experiments demonstrate the effectiveness and robustness of the proposed LOGO, which is competitive with the current state-of-the-art methods. It also exhibits favorable potential for high-level vision tasks, such as essential and fundamental matrix estimation, image registration and loop closure detection.",
      "publication_date": "2022-07-27",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2a406ca9b535397fa9985ed00f1b127c29698967",
      "title": "Operational locality in global theories",
      "abstract": "Within a global physical theory, a notion of locality allows us to find and justify information-processing primitives, like non-signalling between distant agents. Here, we propose exploring the opposite direction: to take agents as the basic building blocks through which we test a physical theory, and recover operational notions of locality from signalling conditions. First, we introduce an operational model for the effective state spaces of individual agents, as well as the range of their actions. We then formulate natural secrecy conditions between agents and identify the aspects of locality relevant for signalling. We discuss the possibility of taking commutation of transformations as a primitive of physical theories, as well as applications to quantum theory and generalized probability frameworks. This \u2018it from bit\u2019 approach establishes an operational connection between local actions and local observations, and gives a global interpretation to concepts like discarding a subsystem or composing local functions. This article is part of a discussion meeting issue \u2018Foundations of quantum mechanics and their impact on contemporary society\u2019.",
      "publication_date": "2017-01-12",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7cd3dea1be5fbfb58bf1949a9cd887f3ee8080b7",
      "title": "Temporal-Spatial Global Locality Projections for Multimode Process Monitoring",
      "abstract": "Multimode is an important feature of modern processes, since various manufacturing strategies are needed to satisfy different demands of markets. Direct application of traditional multivariate statistical process monitoring methods cannot obtain satisfactory results, as the data set collected from multimode processes always follows multimodal distribution. To construct a single model which can monitor multimode processes directly, this paper proposes an original algorithm named temporal\u2013spatial global locality projections. First, given that both temporal and spatial neighbors can express the similarity, the determination of the neighborhood is conducted in both the temporal and spatial scale. Second, an optimization objective function which preserves not only the local structure but also the global structure is defined. Third, the monitoring statistic is established via the local outlier factor. To certify the effectiveness, a numerical example, the multimode Tennessee Eastman process, and the CE117 process which is proposed by TecQuipment for process control are studied.",
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48d73af1a5820c5c4fa56a7dc310ee6de7421a3f",
      "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": null,
      "citation_count": 1,
      "authors": "Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bd6f57cfd1d597fc107995df36601ec969d329a2",
      "title": "OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs",
      "abstract": null,
      "publication_date": null,
      "venue": "International Conference on Learning Representations",
      "year": "2025",
      "citation_count": 10,
      "authors": "Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, Sheng Guo",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3d131fde9b571e6cda0183f286c7aaeb5d65c8bf",
      "title": "Global-locality preserving projection for word embedding",
      "abstract": null,
      "publication_date": "2022-06-16",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "30618f05160ebf08327ab88f0be3c524dce678c9",
      "title": "Supervised global-locality preserving projection for plant leaf recognition",
      "abstract": null,
      "publication_date": "2019-03-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "867b2a9f7c1b5e2e411dd9211173bee42069db69",
      "title": "Global Locality in Event Extraction",
      "abstract": null,
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0f4e05bc5d7ba061587bfdbf888db7be3eb57834",
      "title": "Between the \"Global\" and the \"Local\": On Global Locality and Local Globality",
      "abstract": null,
      "publication_date": "2009-07-01",
      "venue": "",
      "year": 2009,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d767df4d8ba3f8709a4800b70ca60d4d3d6c9c87",
      "title": "A matrix-based approach to the global locality optimization problem",
      "abstract": null,
      "publication_date": "1998-10-12",
      "venue": "",
      "year": 1998,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ce5352e2a34197ae14b18619c9517e88421f7972",
      "title": "A Matrix-Based Approach to Global Locality Optimization",
      "abstract": null,
      "publication_date": "1999-08-01",
      "venue": "",
      "year": 1999,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "817a49d290ed85351b2fca3d20bcafa52016055c",
      "title": "Agent Selection And P2P Overlay Construction Using Global Locality Knowledge",
      "abstract": null,
      "publication_date": "2007-04-15",
      "venue": "",
      "year": 2007,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4dd10f293492a4f81b5ec8a29138df26159ecb99",
      "title": "Manufacturing the global locality, customizing the school and designing young workers",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2001,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9927002509ce571d321bdc93bd2a8e28a36363d3",
      "title": "On the notions of normality, locality, and operational stability in ADRC",
      "abstract": null,
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "33a7fd0b542f033031fdd8b49a787dd8c3755de0",
      "title": "Digital humor and the articulation of locality in an age of global flows",
      "abstract": null,
      "publication_date": "2016-02-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ae1f34f5a5bd2324bec25a2881d8710da6fc40d8",
      "title": "Efficient Reasoning for Inconsistent Horn Formulae",
      "abstract": null,
      "publication_date": "2016-11-09",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3552c1799a4afe5ee436b40ac04535804c059f9a",
      "title": "Efficient Reasoning with Constrained Goal Models",
      "abstract": null,
      "publication_date": "2017-04-19",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27a256991068a946ae6f73d057a37e15c0fe7331",
      "title": "Efficient Reasoning With Consistent Proper Epistemic Knowledge Bases",
      "abstract": null,
      "publication_date": "2015-05-04",
      "venue": "",
      "year": 2015,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cb10356ec8c93518b16ce4ae0e0fb6b25c1298",
      "title": "The Incredible ELK - From Polynomial Procedures to Efficient Reasoning with \u2130\u2112 Ontologies",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2014,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f57eba50dc96612ac1a1f574745619ed4c08dd18",
      "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
      "abstract": null,
      "publication_date": "2024-06-16",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a37e55b6bb39b50a31ac47100fb2f7ce10cc725b",
      "title": "Supplementary File: Image Super-Resolution with Non-Local Sparse Attention",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "purpose_ranking": [
    {
      "paper_id": "23c2e21b6a5789724715f2986fdc586a517ffe57",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "publication_date": "2025-02-16",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "year": "2025",
      "citation_count": 113,
      "authors": "Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, C. Ruan, Ming Zhang, W. Liang, Wangding Zeng",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a336bf38ebfdadd2992f9f976c963f7280c99d5f",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
      "publication_date": "2024-10-07",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 5,
      "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8f3d959238e67bf6b9bc9818025d0d2e403e478f",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
      "publication_date": "2024-10-14",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 105,
      "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5b2c04e082a56c0eb70ed62bc36148919f665e1c",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.",
      "publication_date": "2024-06-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1784c987e681d60c634765fe64c8d9c26f73d5ff",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abstract": "Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
      "publication_date": "2024-04-22",
      "venue": "Neural Information Processing Systems",
      "year": "2024",
      "citation_count": 274,
      "authors": "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr F. Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning",
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
      "publication_date": "2025-02-04",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 5,
      "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "04b2f3d742f33c372df81d8af2ea34c8fec629fb",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "publication_date": "2025-06-10",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 4,
      "authors": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ee2b3f7703b553b487428862b83995ea3e8c0c3a",
      "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
      "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
      "publication_date": "2024-06-24",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "952fb6413499bc377faa51bf71e4d558ac6f6387",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by $6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with minimal impact on performance. Our code is available at \\url{https://github.com/thu-nics/MoA}.",
      "publication_date": "2024-06-21",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cf7ab5df804575bad88a9fcf0fbf7707bf500944",
      "title": "Training-Free Long-Context Scaling of Large Language Models",
      "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
      "publication_date": "2024-02-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6027d9b7f875a42a36029e2d3b7308be6fa88bb1",
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.",
      "publication_date": "2024-10-21",
      "venue": "International Conference on Learning Representations",
      "year": "2024",
      "citation_count": 40,
      "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, L\u00e9on Bottou, Zhihao Jia, Beidi Chen",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .",
      "publication_date": "2024-06-16",
      "venue": "International Conference on Machine Learning",
      "year": "2024",
      "citation_count": 153,
      "authors": "Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fdc53c2c10742464087c0525f77e32604827a21d",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "publication_date": "2023-09-29",
      "venue": "International Conference on Learning Representations",
      "year": "2023",
      "citation_count": 965,
      "authors": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "501f8a0200a12a6f3906c1e4f3f40715e0e7d23a",
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.",
      "publication_date": "2024-06-26",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "61166a7545c7cadb1ec79c4744c348bac7644d41",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.",
      "publication_date": "2024-09-16",
      "venue": "arXiv.org",
      "year": "2024",
      "citation_count": 61,
      "authors": "Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "9803d83bbb28d02fb01f00e0e05aa3c192a87255",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
      "publication_date": "2024-07-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
      "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
      "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.",
      "publication_date": "2024-03-14",
      "venue": "Conference on Machine Learning and Systems",
      "year": "2024",
      "citation_count": 82,
      "authors": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d",
      "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
      "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \\url{https://github.com/thunlp/InfLLM}.",
      "publication_date": "2024-02-07",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09223db5260bc3c7486497ac3adc59b2d312a08",
      "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
      "abstract": "Diffusion models (DMs) have exhibited superior performance in generating high-quality and diverse images. How-ever, this exceptional performance comes at the cost of expensive generation process, particularly due to the heavily used attention module in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53\u00d7 speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.",
      "publication_date": "2024-05-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "657329c633709dd1ac34a30d57341b186b1a47c2",
      "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
      "publication_date": "2020-03-12",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "90027ca7802645671a69b00b65e1fa94e6b63544",
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
      "publication_date": "2023-05-23",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4c07dc79976a4e3a558bb6fcd0d615673e8ecef",
      "title": "Loki: Low-Rank Keys for Efficient Sparse Attention",
      "abstract": "Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.",
      "publication_date": "2024-06-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c993152d0858584dee2859c21e324ee811ec3991",
      "title": "Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models",
      "abstract": "Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose\"Synergy of Thoughts\"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.",
      "publication_date": "2024-02-04",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "76eef89e0a05c4b67f19697e233916f4b3a77ff4",
      "title": "ASADI: Accelerating Sparse Attention Using Diagonal-based In-Situ Computing",
      "abstract": "The self-attention mechanism is the performance bottleneck of Transformer-based language models, particularly for long sequences. Researchers have proposed using sparse attention to speed up the Transformer. However, sparse attention introduces significant random access overhead, limiting computational efficiency. To mitigate this issue, researchers attempt to improve data reuse by utilizing row/column locality. Unfortunately, we find that sparse attention does not naturally exhibit strong row/column locality, but instead has excellent diagonal locality. Thus, it is worthwhile to use diagonal compression (DIA) format. However, existing sparse matrix computation paradigms struggle to efficiently support DIA format in attention computation. To address this problem, we propose ASADI, a novel software-hardware co-designed sparse attention accelerator. In the soft-ware side, we propose a new sparse matrix computation paradigm that directly supports the DIA format in self-attention computation. In the hardware side, we present a novel sparse attention accelerator that efficiently implements our computation paradigm using highly parallel in-situ computing. We thoroughly evaluate ASADI across various models and datasets. Our experimental results demonstrate an average performance improvement of 18.6 \u00d7 and energy savings of 2.9\u00d7 compared to a PIM-based baseline.",
      "publication_date": "2024-03-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2f066326c0d5e5ecd7dc646224272f1e13948579",
      "title": "Efficient Inference for Large Reasoning Models: A Survey",
      "abstract": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance \\&efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs'inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
      "publication_date": "2025-03-29",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 31,
      "authors": "Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "37324374e194bb4d07e1d0a5d81e778020afa776",
      "title": "LocalViT: Analyzing Locality in Vision Transformers",
      "abstract": "The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",
      "publication_date": "2021-04-12",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cccb9065f0a59e845d5b4d360ce31cf25036be",
      "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
      "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the\"answer consistency\"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.",
      "publication_date": "2023-10-04",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2e65e751dc422a5b7a634d1f4b10a62fce2bf1e",
      "title": "vid-TLDR: Training Free Token merging for Light-Weight Video Transformer",
      "abstract": "Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strategy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
      "title": "Leveraging Locality in Abstractive Text Summarization",
      "abstract": "Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.",
      "publication_date": "2022-05-25",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8fb92f51434543c4a8cd4980f84cf04552c712cc",
      "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
      "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
      "publication_date": "2024-01-30",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
      "title": "Qwen3 Technical Report",
      "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
      "publication_date": "2025-05-14",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 808,
      "authors": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jingren Zhou, Junyan Lin, Kai Dang, Keqin Bao, Ke\u2010Pei Yang, Le Yu, Li-Chun Deng, Mei Li, Min Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shi-Qiang Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
      "publication_date": "2020-12-23",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fc3c717987218662f49243e2be6bacc093dd47d8",
      "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
      "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
      "publication_date": "2024-02-17",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d5444633c7826af0dd149b4c9d367c191b4b4192",
      "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
      "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "21a1db2503dc844d71228309d2c3fd75e74d1e8f",
      "title": "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning",
      "abstract": "Redundancy is a persistent challenge in Capsule Networks (CapsNet), leading to high computational costs and parameter counts. Although previous studies have introduced pruning after the initial capsule layer, dynamic routing's fully connected nature and non-orthogonal weight matrices reintroduce redundancy in deeper layers. Besides, dynamic routing requires iterating to converge, further increasing computational demands. In this paper, we propose an Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter counts. Firstly, an efficient pruned capsule layer is introduced to discard redundant capsules. Secondly, dynamic routing is replaced with orthogonal sparse attention routing, eliminating the need for iterations and fully connected structures. Lastly, weight matrices during routing are orthogonalized to sustain low capsule similarity, which is the first approach to use Householder orthogonal decomposition to enforce orthogonality in CapsNet. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. OrthCaps-Shallow outperforms other Capsule Network benchmarks on four datasets, utilizing only 110k parameters - a mere 1.25% of a standard Capsule Network's total. To the best of our knowledge, $it$ achieves the smallest parameter count among existing Capsule Networks. Similarly, OrthCaps-Deep demonstrates competitive performance across four datasets, utilizing only 1.2% of the parameters required by its counterparts.",
      "publication_date": "2024-03-20",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b4a6c010724f0459c9791018e34a982cf96987cf",
      "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
      "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info",
      "publication_date": "2023-05-19",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9ec9d316ba8d8dd731dd810f2a000d47d06924bf",
      "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
      "abstract": "A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models. Our project web page: https://sites.google.com/view/robomamba-web",
      "publication_date": "2024-06-06",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "07ef68619a089fee52efef382584bc6e30badf4a",
      "title": "Locality Guidance for Improving Vision Transformers on Tiny Datasets",
      "abstract": "While the Vision Transformer (VT) architecture is becoming trendy in computer vision, pure VT models perform poorly on tiny datasets. To address this issue, this paper proposes the locality guidance for improving the performance of VTs on tiny datasets. We first analyze that the local information, which is of great importance for understanding images, is hard to be learned with limited data due to the high flexibility and intrinsic globality of the self-attention mechanism in VTs. To facilitate local information, we realize the locality guidance for VTs by imitating the features of an already trained convolutional neural network (CNN), inspired by the built-in local-to-global hierarchy of CNN. Under our dual-task learning paradigm, the locality guidance provided by a lightweight CNN trained on low-resolution images is adequate to accelerate the convergence and improve the performance of VTs to a large extent. Therefore, our locality guidance approach is very simple and efficient, and can serve as a basic performance enhancement method for VTs on tiny datasets. Extensive experiments demonstrate that our method can significantly improve VTs when training from scratch on tiny datasets and is compatible with different kinds of VTs and datasets. For example, our proposed method can boost the performance of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85% for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing the potential of VTs on tiny datasets. The code is available at https://github.com/lkhl/tiny-transformers.",
      "publication_date": "2022-07-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "publication_date": "2022-01-28",
      "venue": "Neural Information Processing Systems",
      "year": "2022",
      "citation_count": 11679,
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, Denny Zhou",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d28fed119d9293af31776205150b3c34f3adc82b",
      "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
      "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "publication_date": "2022-05-20",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "publication_date": "2025-01-22",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 3517,
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, R. Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, A. Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, C. Deng, Chenyu Zhang, C. Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. Cai, J. Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, K. Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, T. Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, W. Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, X. Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu-mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Ren, Z. Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen-guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5c4f42e084e84a88bc8d964b613d4721618711a2",
      "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
      "abstract": "Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pretraining or access to additional pretrained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pretrained networks, or extensive hyperparameter tuning, making it highly practicalfor real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
      "publication_date": "2024-04-12",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "44c266ad28cdc779680044c7e87aacd5d342c702",
      "title": "MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications",
      "abstract": "Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.",
      "publication_date": "2023-06-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d25b7c6e30725feaac3d28b584653cf61d50c5ad",
      "title": "Evaluating Step-by-step Reasoning Traces: A Survey",
      "abstract": "Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.",
      "publication_date": "2025-02-17",
      "venue": "arXiv.org",
      "year": "2025",
      "citation_count": 9,
      "authors": "Jinu Lee, J. Hockenmaier",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "39ba6d541d94132b816938e7e16b1e8fd49c2fd9",
      "title": "Training-Free Consistent Text-to-Image Generation",
      "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
      "publication_date": "2024-02-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a195df2f7611aa75d5734b2efb32a408d2f8348",
      "title": "Training-Free Layout Control with Cross-Attention Guidance",
      "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.",
      "publication_date": "2023-04-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "150c1fb3362486a1fdd79cc7963f7c8ea254f9bb",
      "title": "Global Locality in Biomedical Relation and Event Extraction",
      "abstract": "Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.",
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "210b0a3d76e93079cc51b03c4115fde545eea966",
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
      "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are\"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",
      "publication_date": "2023-11-20",
      "venue": "arXiv.org",
      "year": "2023",
      "citation_count": 1096,
      "authors": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3e87524f02be2ff5f5a853ff5823616d59af5aac",
      "title": "Predicting and Understanding Student Learning Performance Using Multi-Source Sparse Attention Convolutional Neural Networks",
      "abstract": "Predicting and understanding student learning performance has been a long-standing task in learning science, which can benefit personalized teaching and learning. This study shows that the progress towards this task can be accelerated by using learning record data to feed a deep learning model that considers the intrinsic course association and the structured features. We proposed a multi-source sparse attention convolutional neural network (MsaCNN) to predict the course grades in a general formulation. MsaCNN adopts multi-scale convolution kernels on student grade records to capture structured features, a global attention strategy to discover the relationship between courses, and multiple input-heads to integrate multi-source features. All achieved features are then poured into a softmax classifier towards an end-to-end supervised deep learning model. Conducting insights into higher education on real-world university datasets, the results show that MsaCNN achieves better performance than traditional methods and delivers an interpretation of student performance by virtue of the resulted course relationships. Inspired by this interpretation, we created an association map for all mentioned courses, followed by evaluating the map with a questionnaire survey. This study provides computer-aided system tools and discovers the course-space map from the educational data, potentially facilitating the personalized learning progress.",
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee257e16d242b03eb75f44e99456f35027da893a",
      "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
      "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5cfa5e54d34ab13e596240003a90dded6802b1c4",
      "title": "Efficient Reasoning in Regular Boardgames",
      "abstract": "We present the technical side of reasoning in Regular Boardgames (RBG) language \u2013 a universal General Game Playing (GGP) formalism for the class of finite deterministic games with perfect information, encoding rules in the form of regular expressions. RBG serves as a research tool that aims to aid in the development of generalized algorithms for knowledge inference, analysis, generation, learning, and playing games. In all these tasks, both generality and efficiency are important.In the first part, this paper describes optimizations used by the RBG compiler. The impact of these optimizations ranges from 1.7 to even 33-fold efficiency improvement when measuring the number of possible game playouts per second. Then, we perform an in-depth efficiency comparison with three other modern GGP systems (GDL, Ludii, Ai Ai). We also include our own highly optimized game-specific reasoners to provide a point of reference of the maximum speed. Our experiments show that RBG is currently the fastest among the abstract general game playing languages, and its efficiency can be competitive to common interface-based systems that rely on handcrafted game-specific implementations. Finally, we discuss some issues and methodology of computing benchmarks like this.",
      "publication_date": "2020-06-15",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3c358d9588674bab1649fc103515c1c7f7f29b16",
      "title": "ASA-Net: Adaptive Sparse Attention Network for Robust Electric Load Forecasting",
      "abstract": "Electric load forecasting (ELF) is always employed to perform power systems management. However, it is difficult to predict electric load due to the following issues: 1) electric load prediction is prone to external interference, e.g., temperature and weather; 2) the user behaviors are random, such as family gatherings and business rush orders; and 3) electric load consumption varies significantly in different time periods. To solve such problems, an adaptive sparse attention network (ASA-Net) is proposed for ELF, where the adaptive sparse spatial attention (ASSA) module is first designed to increase the anti-interference ability by capturing the detail change caused by external interference; next, the adaptive sparse channel attention (ASCA) module is developed to enhance the tolerance to local outliers by learning their feature information; and finally, the adaptive sparse batch attention (ASBA) module is devised to model the dependencies of the timestamp to reduce the time impact on ELF. Experiments conducted on the benchmarks show the excellent performance of ASA-Net for ELF, and it can further provide valuable assistance for the smart grid.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "42d21108b026d814d8243242459ea8d283c4d70d",
      "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events",
      "abstract": "Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles. In this paper, we create a novel dataset, SUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. Moreover, we propose Eclipse, a novel Efficient glimpse network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
      "publication_date": "2021-03-29",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a5fdc7d8c9f5b0f3d1c687b905ef1948613a4bdb",
      "title": "Abandon Locality: Frame-Wise Embedding Aided Transformer for Automatic Modulation Recognition",
      "abstract": "Automatic modulation recognition (AMR) has been considered as an efficient technique for non-cooperative communication and intelligent communication. In this work, we propose a modified transformer-based method for AMR, called frame-wise embedding aided transformer (FEA-T), aiming to extract the global correlation feature of the signal to obtain higher classification accuracy as well as lower time cost. To enhance the global modeling capability of the transformer, we design a frame-wise embedding module (FEM) to aggregate more samples into a token in the embedding stage to generate a more efficient token sequence. We also present the optimal frame length by analyzing the representation ability of each transformer layer for a better trade-off between the speed and the performance. Moreover, we design a novel dual-branch gate linear unit (DB-GLU) scheme for the feed-forward network of the transformer to reduce the model size and enhance the performance. Experimental results on RadioML2018.01A datasets demonstrate that the proposed method outperforms state-of-the-art works in terms of recognition accuracy and running speed.",
      "publication_date": "2023-01-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "224d2385eace559649f293756f9934f1750b3055",
      "title": "Aquaculture Water Quality Classification with Sparse Attention Transformers: Leveraging Water and Environmental Parameters",
      "abstract": "For aquaculture operations to be successful, water quality is essential. Maintaining a healthy aquaculture environment depends on the correct and timely evaluation of water quality based on both water parameters and environmental variables. Using deep learning and a sparse attention transformer model, this work provides a unique method for categorizing water quality in aquaculture. Aquaculture has always assessed water quality using crude rule-based techniques. This study shows how sophisticated machine learning methods, particularly sparse attention transformers, may be used to capture intricate connections between water parameter values and environmental influences. Sparse attention transformers make it possible to model lengthy sequences well and consider how several environmental variables, including temperature, dissolved oxygen, pH, and nutrient concentrations, are interdependent. A dataset that includes measurements of the water quality and the accompanying ambient condition over time is used to train the suggested model. The model may successfully filter out less significant data points by concentrating on limited windows of relevant information using a sparse attention mechanism. This dynamic attention mechanism adjusts to the temporal and geographical features of aquaculture systems, resulting in more precise and context-aware categorization of water quality. Importantly, this work makes use of IoT-based real-time data to provide the model a constant supply of input. The integration of real-time data ensures that the model's predictions are not only accurate but also timely, enabling rapid responses to changes in water quality conditions. The proposed model gives 99.79% accuracy whereas the existing DNN-LSTM gives 96.86%. The results of this study demonstrate the effectiveness of the deep learning-based sparse attention transformer model for water quality classification in aquaculture. By accurately predicting water quality status, aquaculture practitioners can proactively manage their systems, optimizing conditions for fish and aquatic organisms.",
      "publication_date": "2024-02-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b673df73f768db5738e3e34411504d0a10e078ac",
      "title": "The impact of cost function globality and locality in hybrid quantum neural networks on NISQ devices",
      "abstract": "Quantum neural networks (QNNs) are often challenged with the problem of flat cost function landscapes during training, known as barren plateaus (BP). A solution to potentially overcome the problem of the BP has recently been proposed by Cerezo et al In this solution, it is shown that, for an arbitrary deep quantum layer(s) in QNNs, a global cost function (all qubits measured in an n-qubit system) will always experience BP, whereas a local cost function (single qubit measured in an n-qubit system) can help to alleviate the problem of BP to a certain depth ( )). In this paper, we empirically analyze the locality and globality of the cost function in hybrid quantum neural networks. We consider two application scenarios namely, binary and multi-class classification, and show that for multiclass classification, the local cost function setting does not follow the claims of Cerezo et al; that is, the local cost function does not result in an extended quantum layer\u2019s depth. We also show that for multiclass classification, the overall performance in terms of accuracy for the global cost function setting is significantly higher than the local cost function setting. On the other hand, for binary classification, our results show that the local cost function setting follows the claims of Cerezo et al, and results in an extended depth of quantum layers. However, the global cost function setting still performs slightly better than the local cost function.",
      "publication_date": "2023-01-06",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e872ad446a3c6986feea7bd5cbd5b5754c068ab6",
      "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation",
      "abstract": "Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.",
      "publication_date": "2024-06-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d07382cae7c767c9ed91477b18200968860ca2bb",
      "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
      "abstract": "The demand for stereo images increases as manufacturers launch more extended reality (XR) devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is training-free and straightforward to use with seamless integration into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight method for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layer Modifications to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.",
      "publication_date": "2024-03-08",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4347574a817aadd02a565ebff563f708c874e81c",
      "title": "Locality-Guided Global-Preserving Optimization for Robust Feature Matching",
      "abstract": "Feature matching is a fundamental problem in many computer vision tasks. This paper proposes a novel effective framework for mismatch removal, named LOcality-guided Global-preserving Optimization (LOGO). To identify inliers from a putative matching set generated by feature descriptor similarity, we introduce a fixed-point progressive approach to optimize a graph-based objective, which represents a two-class assignment problem regarding an affinity matrix containing global structures. We introduce a strategy that a small initial set with a high inlier ratio exploits the topology of the affinity matrix to elicit other inliers based on their reliable geometry, which enhances the robustness to outliers. Geometrically, we provide a locality-guided matching strategy, i.e., using local topology consensus as a criterion to determine the initial set, thus expanding to yield the final feature matching set. In addition, we apply local affine transformations based on reference points to determine the local consensus and similarity scores of nodes and edges, ensuring the validity and generality for various scenarios including complex nonrigid transformations. Extensive experiments demonstrate the effectiveness and robustness of the proposed LOGO, which is competitive with the current state-of-the-art methods. It also exhibits favorable potential for high-level vision tasks, such as essential and fundamental matrix estimation, image registration and loop closure detection.",
      "publication_date": "2022-07-27",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2a406ca9b535397fa9985ed00f1b127c29698967",
      "title": "Operational locality in global theories",
      "abstract": "Within a global physical theory, a notion of locality allows us to find and justify information-processing primitives, like non-signalling between distant agents. Here, we propose exploring the opposite direction: to take agents as the basic building blocks through which we test a physical theory, and recover operational notions of locality from signalling conditions. First, we introduce an operational model for the effective state spaces of individual agents, as well as the range of their actions. We then formulate natural secrecy conditions between agents and identify the aspects of locality relevant for signalling. We discuss the possibility of taking commutation of transformations as a primitive of physical theories, as well as applications to quantum theory and generalized probability frameworks. This \u2018it from bit\u2019 approach establishes an operational connection between local actions and local observations, and gives a global interpretation to concepts like discarding a subsystem or composing local functions. This article is part of a discussion meeting issue \u2018Foundations of quantum mechanics and their impact on contemporary society\u2019.",
      "publication_date": "2017-01-12",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7cd3dea1be5fbfb58bf1949a9cd887f3ee8080b7",
      "title": "Temporal-Spatial Global Locality Projections for Multimode Process Monitoring",
      "abstract": "Multimode is an important feature of modern processes, since various manufacturing strategies are needed to satisfy different demands of markets. Direct application of traditional multivariate statistical process monitoring methods cannot obtain satisfactory results, as the data set collected from multimode processes always follows multimodal distribution. To construct a single model which can monitor multimode processes directly, this paper proposes an original algorithm named temporal\u2013spatial global locality projections. First, given that both temporal and spatial neighbors can express the similarity, the determination of the neighborhood is conducted in both the temporal and spatial scale. Second, an optimization objective function which preserves not only the local structure but also the global structure is defined. Third, the monitoring statistic is established via the local outlier factor. To certify the effectiveness, a numerical example, the multimode Tennessee Eastman process, and the CE117 process which is proposed by TecQuipment for process control are studied.",
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48d73af1a5820c5c4fa56a7dc310ee6de7421a3f",
      "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": null,
      "citation_count": 1,
      "authors": "Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bd6f57cfd1d597fc107995df36601ec969d329a2",
      "title": "OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs",
      "abstract": null,
      "publication_date": null,
      "venue": "International Conference on Learning Representations",
      "year": "2025",
      "citation_count": 10,
      "authors": "Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, Sheng Guo",
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3d131fde9b571e6cda0183f286c7aaeb5d65c8bf",
      "title": "Global-locality preserving projection for word embedding",
      "abstract": null,
      "publication_date": "2022-06-16",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "30618f05160ebf08327ab88f0be3c524dce678c9",
      "title": "Supervised global-locality preserving projection for plant leaf recognition",
      "abstract": null,
      "publication_date": "2019-03-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "867b2a9f7c1b5e2e411dd9211173bee42069db69",
      "title": "Global Locality in Event Extraction",
      "abstract": null,
      "publication_date": "2019-09-11",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0f4e05bc5d7ba061587bfdbf888db7be3eb57834",
      "title": "Between the \"Global\" and the \"Local\": On Global Locality and Local Globality",
      "abstract": null,
      "publication_date": "2009-07-01",
      "venue": "",
      "year": 2009,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d767df4d8ba3f8709a4800b70ca60d4d3d6c9c87",
      "title": "A matrix-based approach to the global locality optimization problem",
      "abstract": null,
      "publication_date": "1998-10-12",
      "venue": "",
      "year": 1998,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ce5352e2a34197ae14b18619c9517e88421f7972",
      "title": "A Matrix-Based Approach to Global Locality Optimization",
      "abstract": null,
      "publication_date": "1999-08-01",
      "venue": "",
      "year": 1999,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "817a49d290ed85351b2fca3d20bcafa52016055c",
      "title": "Agent Selection And P2P Overlay Construction Using Global Locality Knowledge",
      "abstract": null,
      "publication_date": "2007-04-15",
      "venue": "",
      "year": 2007,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4dd10f293492a4f81b5ec8a29138df26159ecb99",
      "title": "Manufacturing the global locality, customizing the school and designing young workers",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2001,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9927002509ce571d321bdc93bd2a8e28a36363d3",
      "title": "On the notions of normality, locality, and operational stability in ADRC",
      "abstract": null,
      "publication_date": "2023-02-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "33a7fd0b542f033031fdd8b49a787dd8c3755de0",
      "title": "Digital humor and the articulation of locality in an age of global flows",
      "abstract": null,
      "publication_date": "2016-02-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ae1f34f5a5bd2324bec25a2881d8710da6fc40d8",
      "title": "Efficient Reasoning for Inconsistent Horn Formulae",
      "abstract": null,
      "publication_date": "2016-11-09",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3552c1799a4afe5ee436b40ac04535804c059f9a",
      "title": "Efficient Reasoning with Constrained Goal Models",
      "abstract": null,
      "publication_date": "2017-04-19",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27a256991068a946ae6f73d057a37e15c0fe7331",
      "title": "Efficient Reasoning With Consistent Proper Epistemic Knowledge Bases",
      "abstract": null,
      "publication_date": "2015-05-04",
      "venue": "",
      "year": 2015,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "00cb10356ec8c93518b16ce4ae0e0fb6b25c1298",
      "title": "The Incredible ELK - From Polynomial Procedures to Efficient Reasoning with \u2130\u2112 Ontologies",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2014,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f57eba50dc96612ac1a1f574745619ed4c08dd18",
      "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
      "abstract": null,
      "publication_date": "2024-06-16",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a37e55b6bb39b50a31ac47100fb2f7ce10cc725b",
      "title": "Supplementary File: Image Super-Resolution with Non-Local Sparse Attention",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ]
}