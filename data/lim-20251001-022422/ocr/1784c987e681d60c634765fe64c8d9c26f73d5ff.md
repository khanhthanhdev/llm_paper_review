# SnapKV: LLM Knows What You are Looking for Before Generation

Yuhong $\mathbf { L i } ^ { 1 * }$ Yingbing Huang1∗ Bowen Yang2 Bharat Venkitesh2 Acyr Locatelli2 Hanchen $\mathbf { Y e } ^ { 1 }$ Tianle Cai3 Patrick Lewis2 Deming Chen1

1 University of Illinois Urbana-Champaign 2 Cohere 3 Princeton University

1{leeyh, yh21, hanchen8, dchen}@illinois.edu 2{bowen, bharat, acyr, patrick}@cohere.com 3tianle.cai@princeton.edu

# Abstract

Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.

We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an ‘observation’ window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an $8 . 2 \mathbf { x }$ enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV’s potential for practical applications.

# 1 Introduction

Many leading LLMs have started to handle longer contexts, overcoming the difficulties in context maintenance and attention mechanism scalability, such as GPT-4 [1] and Command-R [2] with context length 128K, Claude-3 [3] with 200K, and Gemini-Pro-1.5 with 1M [4]. Despite their impressive capabilities, LLMs still face significant challenges when dealing with long context prompts. Specifically, the KV cache in attention calculation becomes less efficient when processing long context. During inference time, as prompt length increases, the decoding latency per step grows linearly due to the attention calculation across past KVs. Moreover, the large KV cache requires significant memory capacity, increasing hardware demands and limiting model scalability.

![](images/9740a302347b93cdd243b7f7cc57cb3e6e1a58380a93ab7f8f91522809cd1e17.jpg)  
Figure 1: The graph shows the simplified workflow of SnapKV, where the orange area represents the cluster of features per head selected by SnapKV. These features are then used to form new Key-Value pairs concatenated with the features in the observation window. Together, the selected prefix and observation windows constitute the new KV cache utilized for the generation.

There are many approaches to mitigate these problems, such as KV cache eviction during generation stage [5–8]. However, most of these methods lack a detailed evaluation in long-context settings. Moreover, they mainly focus on compressing the KV cache appended during decoding steps, while overlooking the realistic problem of compressing KV cache for prompts, which is typically the bottleneck in memory efficiency. In practical applications like chatbots and agents, where prompts range from multi-turn conversations to extensive articles or codebases [1, 9, 10], prompts are often much larger than generated responses such as summaries and code pieces, thus creating significant inference latency and memory utilization overhead. Additional challenge lies in compressing KV cache for such vast prompts without losing crucial information for accurate generation, especially in scenarios with various noisy contexts.

In our paper, we find an important attention allocation phenomenon: only a portion of prompt tokens convey essential information for response generation, and these tokens remain unchanged during generation. To validate the robustness of this finding, we design a thorough set of experiments across diverse prompts in terms of length, format, and content. From our observations, we derive an innovative and intuitive method, SnapKV, which can smartly identify the attention allocation pattern and compress the KV cache for long sequence prompts without compromising the model’s accuracy. With its comprehensive design, SnapKV demonstrates its effectiveness on various datasets and can be easily integrated into popular deep-learning frameworks with just a few code adjustments. Our contributions are as follows:

• We design experiments to explore the attention allocation pattern during generation, focusing on two key questions: 1. Is there a consistent attention allocation pattern for input sequence tokens? 2. Is it feasible to identify this pattern prior to the generation stage? Our finding suggests that for LLMs, the attention allocation of most input sequence tokens stay consistent during generation. Thus, LLMs knows what you are looking for before generation.   
• Inspired by our observations above, we develop an efficient and fine-tuning-free algorithm, SnapKV, which efficiently identifies critical attention features and compresses KV cache correspondingly with minimal model modification (See Fig. 1).   
• We evaluate SnapKV across diverse LLMs and long-sequence datasets. SnapKV shows comparable accuracy with full KV caching method while achieving improved decoding speed and memory efficiency. Meanwhile, we conduct the pressure test with Needle-in-a-Haystack to further demonstrate its memory efficiency and information retrieval ability.

# 2 Related Works

Many previous works compress the KV cache by selectively dropping KVs using different algorithms. In StreamLLM [5], only the most recent tokens and attention sinks (first few tokens) are retained to reduce the KV cache size, making it lose the important information carried by the discarded middle tokens 2. Heavy-Hitter Oracle (H2O) [6] introduces a policy that greedily drops KVs during generation based on a scoring function derived from cumulative attention. While this approach effectively compresses the KVs appended to the cache during generation, it overlooks compression of prompt KVs, which is crucial for reducing memory and computational overhead. Building on a similar concept, Adaptive KV Compression (FastGen) [8] implements a dual-phase algorithm that encompasses four KV cache compression policies. Initially, it identifies optimal policies through profiling results obtained from prompt encoding. Subsequently, it dynamically evicts caches during the generation phase based on these policies. Nonetheless, it faces the similar problem with H2O. ScissorHands [7] focuses on identifying and retaining pivotal tokens that exhibit a consistent attention weight pattern with previous token windows during generation steps. However, this method concentrates solely on the window of previous pivotal tokens in generation and neglects the extensive prompt that contains essential information for generating accurate responses. This oversight could lead to an inability to extract detailed information from prompts.

![](images/51e350203e8af84a19dc409d9fb0631fe8ee3ed1d891dfbd2f6c2283456ac3e2.jpg)  
Figure 2: The overlap rates between attention features of the input sequence, selected by various windows along the input and during generation, with each line representing a model layer.

![](images/ffe9b79f747d752e8b536873df5227b2887583d56b378f4ca965ab44593d2960.jpg)  
Figure 3: The layer-wise overlap rates between input sequence attention features selected by the last window of input sequence and those selected by 4 windows along generation.

In summary, existing methods have not effectively addressed the challenges encountered in realworld applications, where prompts are exceptionally long yet require accurate information retrieval. Although these techniques may reduce the KV cache size during generation, they do not address the primary challenges of understanding complex prompt contexts, leaving critical issues unresolved.

# 3 Observations

In this section, we present our observations regarding the attention allocation patterns in the QueryKey matrix during token generation. Our analysis utilizes samples from Ultrachat [11], a multi-turns, high-quality instruction dataset consisting of 1.4 million dialogues. We further filter the sequences with response length greater than 512 and prompt length greater than 3k. Our findings are concluded into two key observations as follows:

• Pattern can be identified before generation. In this experiment, we split the attention features of input sequence of each layer into multiple windows, each with 128 tokens, and calculate the averaged attention weights of the last 20 windows separately. To understand the attention allocation patterns along input sequences, we calculate the overlap rates between important attention features of input sequence (those with high average attention weights) identified by each window and the actual ones used by generation. The experimental results are shown in Fig. 2.

We observe that the last window of input sequence recognizes highly similar attention allocation pattern with the actual generation.

• Pattern is consistent during generation. We study if the positions of features identified as crucial in the last window of input sequence maintain their significance in the subsequent token generation. In the experiment, we split the generated tokens into 4 windows for every layer, each spanning 128 tokens, to compute the averaged overlap rates of these windows versus the last window of input sequence. As shown in Fig. 3, active attention features of input sequence obtained from the last window exhibit remarkable consistency throughout the generation process, as evidenced by high overlap rates.

# 4 SnapKV

In the attention mechanism, the growth in prompts will significantly increase time complexity for generation due to the Query-Key matrix multiplication. SnapKV addresses this issue by maintaining a constant amount of prompt KVs during generation, significantly reducing serving times for longcontext LLMs. To structure our method coherently, we propose the following terminologies:

• Prompt Length $( L _ { \mathrm { p r o m p t } } )$ : The total length of the user-provided input.

• Observation Window $( L _ { \mathrm { o b s } } )$ : The last segment of the prompt. This window is crucial for analyzing the influence of different contexts on attention allocation patterns.

• Prefix Length $\left( L _ { \mathrm { p r e f i x } } \right)$ : The length of the input preceding the observation window. It is part of the prompt and does not include the observation window. Overall, we have:

$$
L _ { \mathrm { p r o m p t } } = L _ { \mathrm { p r e f i x } } + L _ { \mathrm { o b s } }
$$

• Voting: The process of calculating attention weights for each query within the observation window across all heads, aggregating these weights to highlight the prefix positions that are considered most significant. For a single batch of sequence, formally:

$$
\mathbf { C } = \sum _ { i = 0 } ^ { L _ { \mathrm { o b s } } } \mathbf { W } _ { \mathrm { o b s } } [ : , i , : ]
$$

$$
I = \mathrm { T o p } _ { k } ( { \bf C } , k )
$$

where $\mathrm { T o p } _ { k } ( \mathbf { C } , k )$ selects the indices $I$ of the top $k$ values in tensor $\mathbf { C }$ per head. $k$ is defined as $\lvert p \times L _ { \mathrm { p r e f i x } } \rvert$ , where $p$ stands for the compression rate. The tensor $\bar { \bf W } _ { \mathrm { o b s } } \in \mathbb { R } ^ { N \times L _ { \mathrm { o b s } } \times L _ { \mathrm { p r e f i x } } }$ represents the subset of the prompt softmax-normalized attention features over $N$ heads.

• Hit Rate: We define attention features above a predefined threshold $\theta$ during generation as important features. The hit rate, $H$ , is the number of important features successfully selected by the previous voting process over the total number of important features. $H$ quantifies the effectiveness of the voting mechanism and is calculated as follows:

$$
\begin{array} { r l } & { \quad \mathbf { M } _ { \mathrm { v o t e \_ o b s } } = \mathrm { z e r o s \_ l i k e } ( \mathbf { A } _ { \mathrm { c u r } } ) } \\ & { \mathbf { M } _ { \mathrm { v o t e \_ o b s } } [ I ] = 1 } \\ & { \mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } } = \mathbf { 1 } ( \mathbf { A } _ { \mathrm { c u r } } > \theta ) } \\ & { \quad \quad \quad \quad \mathbf { O } = \mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } } \wedge \mathbf { M } _ { \mathrm { v o t e \_ o b s } } } \\ & { \quad \quad \quad H = \frac { \displaystyle \sum _ { \mathbf { W } = \mathbf { h } \mathrm { o l d \_ c u r } } } { \displaystyle \sum _ { \mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } } } } } \end{array}
$$

$\mathbf { A } _ { \mathrm { c u r } } \in \mathbb { R } ^ { N \times L _ { \mathrm { p r e f i x } } }$ represents the attention features between the current generated query and prefix keys. M selects attention features by indices. The threshold operation filters $\mathbf { A } _ { \mathrm { c u r } }$ to retain only features with values over $\theta$ , indicating important attention activations. The $\mathbf { O }$ measures the overlap between attention features selected by $\mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } }$ and $\mathbf { M } _ { \mathrm { v o t e \_ o b s } }$ , quantifying the alignment of the current attention with previously identified important features. The hit rate $H$ is then computed as the ratio of the sum of overlap $\mathbf { O }$ to the sum of important features $\mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } }$ providing a metric for the efficacy of the attention mechanism in recognizing and emphasizing important attention features within the context. We use $\mathcal { H } ( \mathbf { M } _ { \mathrm { t h r e s h o l d \_ c u r } } , \mathbf { M } _ { \mathrm { v o t e \_ o b s } } )$ to denote combination of Eq. 7 and Eq. 8.

# 4.1 Observation Window-based Algorithm

The core approach of SnapKV involves identifying and selecting the most crucial attention features per head to create the compressed KV cache. Listing 1 shows the PyTorch-style pseudo code of SnapKV. Overall, SnapKV operates through two stages as follows:

• Vote for important previous features. By the voting process defined above (Eq. 2), we select the important attention features based on the observation window. Sec. 3 highlights the consistency of the attention allocation pattern within observation windows throughout the generation, suggesting that these selected attention features are also vital for subsequent generation. Furthermore, we implement clustering to retain the features surrounding the selected attention features (Sec. 4.3). Line 8-17 shows the pseudo code of the voting process. • Update and store compressed keys and values. We concatenate the selected attention features with all features within the observation window, which encompasses all features containing the necessary prompt information. Line 18- 24 shows the compressing process. The concatenated KVs are stored for later use in generation, thereby saving memory usage.

![](images/314e4db78a201c08292f8ba3726732a724e09a5fe836cc9f17d8b8e5294be128.jpg)

# 4.2 Robustness Analysis of Hit Rate

To understand the robustness of the observation window-based algorithm, we analyze its hit rate on multiple long documents QA datasets including QMSum [12], a query-based multi-domain meeting summarization; Openreview [13], a collection of papers from openreview.net; SPACE [14], an extractive opinion summarization in quantized transformer spaces. The model we probe is Mistral-7B-Instruct-v0.2. Overall, we want to answer the following two questions:

1. Does the nature of instructions in the prompt affect the hit rate?   
2. Does the context and instruction positioning affect the hit rate?

# 4.2.1 Contextual Dependency of Patterns

We analyze whether instructions will affect the selection of important features even if the provided context is the same. Our experiment utilizes different instructions on the same document and selects the important features based on the observation window that consists of both the instructions and their corresponding responses. Then we calculate the hit rates between important features selected by different instruction-response pairs within the same document by using $\mathcal { H } ( \mathbf { M } _ { \mathrm { v o t e \_ A } } , \mathbf { M } _ { \mathrm { v o t e \_ B } } )$ . By varying the instructions, we observe that different instructions prioritize different prefix attention features, as indicated by the descending trend in hit rates shown in Fig. 4. Our findings reveal an interesting aspect of KV cache in LLMs: the important attention features change with different instructions. This variability challenges the effectiveness of static compression methods that depend on constant weighted importance or fixed policies [7, 6, 8]. Thus, the complex relationship between context and related KV cache emphasizes the need for context-aware compression strategies and highlights the capability of SnapKV that recognizes this dynamic.

![](images/b4a7cc3e4d9b9a71944a7ee8275b9a95b1fcc284db1229225a23152f9edd4575.jpg)  
Figure 4: The layer-wise overlap of important positions utilized by different question-answer pairs in the same dataset.

![](images/b7946266d9e2aa6c74d9432ea2b4731b7869380cf4a0d1780936b6c6ab1b7b78.jpg)  
Figure 5: The layer-wise average hit rate of important positions used by prompts with questions at the beginning and the end.

# 4.2.2 Invariance to Instruction Positions

Our analysis also extends to the significance of instruction positioning on the interpretability of LLMs and their selection of important features. We calculate the average hit rate for the responses using the same observation window size as in the previous experiment. Our results shown in Fig. 5 indicate that across all three datasets, the hit rates are consistently high regardless of whether instructions are positioned before or after extensive supplementary contexts. This consistency suggests that SnapKV is able to identify attention allocation patterns regardless of the question’s positions.

# 4.3 Efficient Clustering via Pooling

In LLMs, information retrieval and generation rely on features with high attention weight and are supplemented by copying the rest of features in context using induction heads [15]. Hence, naively selecting the top features results in retaining only portions of details and then losing the completeness of the information. For example, such compression might cause the LLMs to retrieve only the country code of a phone number and hallucinate the rest. Our experiment also revealed that only selecting the features with the highest weights is insufficient (Sec. 5.2). Such sparse selection risks compromising the contextual integrity encapsulated in between features, thereby reducing accuracy. Based on the insights, we propose a fine-grained clustering algorithm utilizing a pooling layer shown in Line 13.

# 5 Experiments

In our experimental setup, we explore the performance of SnapKV across models that can handle extended prompt sequence contexts. First, we deliver a pressure test and benchmark the speed of LWM-Text-Chat-1M [16], which is state-of-the-art regarding its context length. We then conduct an ablation study on Mistral-7B-Instruct-v0.2 to understand the influence of pooling on the model’s information retrieval performance. We assess model performances using the LongBench [17] dataset. Further, we dive into a comprehensive examination of the Command- $\scriptstyle \cdot \mathrm { \mathbb { R } }$ [2] model, another leading open-source model in the field. Lastly, we show that SnapKV can be utilized with other acceleration strategies such as parallel decoding.

![](images/ca08736fc5a7abb2130990618872f3ea6b82b2333291ac6b56c41cb75ffe1bef.jpg)  
Figure 6: Needle-in-a-Haystack test performance comparison on single A100-80GB GPU, native HuggingFace implementation with only a few lines of code changed. The $\mathbf { X }$ -axis denotes the length of the document (the “haystack”) from 1K to 380K tokens; the y-axis indicates the position that the “needle” (a short sentence) is located within the document. For example, $50 \%$ indicates that the needle is placed in the middle of the document. Here LWMChat with SnapKV is able to retrieve the needle correctly before $1 4 0 \mathrm { k }$ and with only a little accuracy drop after. Meanwhile, the original implementation encounters OOM error with $3 3 \mathrm { k }$ input tokens (white dashed line).

# 5.1 Benchmarks on LWM-Text-Chat-1M

LWM-Text-Chat-1M [16] is a 7B instruction-fine-tuned model with up to one million context length.   
In this section, we conduct a pressure test on this model and examine its algorithmic efficiencies.

# 5.1.1 Needle-in-a-Haystack

The Needle-in-a-Haystack test [18] challenges the model to accurately retrieve information from a specific sentence ("needle") concealed within an extensive document (the "haystack"), with the sentence placed at a random location. Typically, sentences that are inserted in the middle of prompts are harder to retrieve. To rigorously evaluate SnapKV’s capabilities, we extended the document length to $3 8 0 \mathrm { k }$ tokens which is the longest content that can be processed by a single A100-80GB GPU. We configured the prompt KV cache size to 1024, enabling SnapKV to select the most crucial 1024 attention features from the prompt for answer generation, with a maximum pooling kernel size of 5 and an observation window size of 16, both of which are hyperparameters that can be customized. The compelling outcomes in Fig. 6 from the Needle-in-a-Haystack test underscore SnapKV’s potential to precisely manage small details on extremely long input contexts with a $3 8 0 \mathrm { x }$ compression ratio.

# 5.1.2 Decoding Speed and Memory Bound

We further benchmark the speed of LWM-Text-Chat-1M under different batch-size settings using SnapKV. We set the maximum KV cache size as 2048 for SnapKV, and fix the generation length at 512 to ensure a fair comparison. There are two main takeaways from our experiment on decoding speed and prompt sequence length on various batch sizes, as shown in Fig. 7. First, as the input sequence length increases, the decoding latency of the baseline implementation escalates linearly. Conversely, the SnapKV-optimized model maintains a constant decoding speed since the compressed KV cache size of prompt stays the same regardless of input sequence length and there is no extra update during the inference. For instance, at a sequence length of $1 6 \mathrm { k }$ and a batch size of 2, the decoding time for the baseline model surpasses $1 0 0 \mathrm { m s }$ , whereas for SnapKV-optimized model, the decoding time consistently remains below $4 0 \mathrm { m s }$ , achieving approximately a $3 . 6 \mathrm { x }$ speedup. Second, with the same batch size, the model integrated with SnapKV can decode significantly longer sequences. For example, at a batch size of 2, the baseline model encounters an OOM error beyond $1 6 \mathrm { k }$ input tokens, whereas the SnapKV-enhanced model extends this limit to $1 3 1 \mathrm { k }$ input tokens, indicating an approximately $8 . 2 \mathbf { x }$ improvement. This demonstrates SnapKV’s effectiveness in minimizing memory consumption.

![](images/f38938f25bd53bf6683c84312a3c4cc01dd275d58634e8b1b995e147925ceaac.jpg)  
Figure 7: Decoding latency comparison of baseline implementation and SnapKV optimized solutions on various batch sizes. The $\mathbf { X }$ -axis denotes the input sequence length; the y-axis indicates decoding latency (ms/token). All experiments are conducted on an A100 80GB GPU. The red dotted line denotes the common context length of state-of-the-art long sequence models.

![](images/44fb187a39e85a6a31dcf1d4896b2a83f3440638ba64c2e1e6242e061ed379f0.jpg)

![](images/63d9df06e7fc8fa549f39b90bfdef2e1e08c54bbd35be6575c60b44788d29b1a.jpg)  
Figure 8: Ablation study of pooling on LongEval-Lines. The evaluation includes inputs, each comprised of lines formatted as "line makeshift-penguin: REGISTER_CONTENT is $< 1 0 5 3 6 > "$ , where the key is an adjective-noun pair and the value is a random 5-digit number. The model needs to retrieve the value based on a given key. The $\mathbf { X }$ -axis denotes the length of the input; the y-axis indicates the position of the groundtruth, from 5K to 30K tokens. With the pooling, the model can retrieve correct values before 16k and performs significantly better than the one without pooling.

# 5.2 Ablation Study of Effectiveness of Pooling

We perform an ablation study on Mistral-7B-Instruct-v0.2 to assess the impact of our pooling technique, a straightforward but efficient method for consolidating information through clustering. Our evaluation utilizes the modified LongEval-Lines benchmark [19], incorporating randomly generated pairs and averaged scores. LongEval-Lines presents a greater challenge compared to Needle-in-a-Haystack because it involves identifying key-value pairs in noisy contexts of the same format, while in Needle-in-a-Haystack, the relevant information is more distinctly separated from other contexts. We apply max pooling with a kernel size of 5 and use the observation window with a size of 16, which are hyperparameters and could be customized according to different models. As illustrated in our results (Fig. 8), we find that pooling significantly enhances retrieval accuracy compared to methods not utilizing pooling. We hypothesize that this is because the initial portions of critical token clusters are weighted higher by attention mechanisms. Typically, large language models tend to copy the tokens surrounding the initial portions to keep the contextual integrity. However, naively compressed KV cache breaks this mechanism and could lead to partially correct results (Fig. 8). Note that throughout our experiments, the choice between max pooling and average pooling did not yield significant differences in performance.

Table 1: Performance comparison of SnapKV and H2O across various LLMs on LongBench.   

<table><tr><td rowspan="2"></td><td rowspan="2">LLMs *</td><td colspan="3">Single-Document QA</td><td colspan="3">Multi-Document QA 2WikiMQA</td><td colspan="3">Summarization</td><td colspan="3">Few-shot Learning</td><td colspan="2">Synthetic</td><td colspan="2">Code</td></tr><tr><td>NrtvQA</td><td>Qasper</td><td>MF-en</td><td>HotpotQA</td><td></td><td>Musique</td><td>GovReport</td><td>QMSum</td><td>MultiNews</td><td>TREC</td><td>TriviaQA</td><td></td><td>SAMSum</td><td>PCount</td><td>PRe Lcc</td><td>RB-P</td></tr><tr><td></td><td>AII KV</td><td>18.18</td><td>25.56</td><td>40.94</td><td>24.57</td><td>19.39</td><td>10.49</td><td>27.97</td><td>24.9</td><td>24.81</td><td>71.0</td><td>60.9</td><td>39.73</td><td>3.17</td><td>3.5</td><td>44.4  43.82</td><td></td></tr><tr><td></td><td>SnapKV: 1024</td><td>18.02</td><td>23.73</td><td>40.25</td><td>24.61</td><td>19.84</td><td>10.77</td><td>19.79</td><td>24.44</td><td>23.53</td><td>70.0</td><td>61.42</td><td>39.64</td><td></td><td>1.67 3.0</td><td></td><td>43.34  44.0</td></tr><tr><td></td><td>Snapkv: 2048</td><td>17.92</td><td>25.03</td><td>41.38</td><td>24.49</td><td>19.38</td><td>11.34</td><td>21.6</td><td>24.22</td><td>24.36</td><td>70.0</td><td>61.11</td><td>39.91</td><td>2.17</td><td>4.0</td><td></td><td>44.4644.92</td></tr><tr><td>WMT</td><td>Snapkv: 4096 H2O:4096</td><td>17.92 13.17</td><td>25.47</td><td>40.76</td><td>24.92</td><td>19.53</td><td>11.27</td><td>25.34</td><td>25.42</td><td>24.58</td><td>70.5</td><td>61.08</td><td>39.62</td><td>3.17</td><td>4.0</td><td></td><td>44.4944.08</td></tr><tr><td></td><td></td><td></td><td>24.82</td><td>20.01</td><td>16.86</td><td>9.74</td><td>7.2</td><td>25.77</td><td>23.26</td><td>23.83</td><td>71.0</td><td>61.06</td><td>40.33</td><td>0.0</td><td>0.0</td><td> 41.52 40.97</td><td></td></tr><tr><td></td><td>AIl KV</td><td>20.88</td><td>29.36</td><td>43.2</td><td>33.05</td><td>24.58</td><td>14.66</td><td>30.89</td><td>22.76</td><td>26.61</td><td>66.5</td><td>83.99</td><td>40.83</td><td>0.0</td><td></td><td>30.5 54.89 59.05</td><td></td></tr><tr><td>uoT</td><td>SnapKV: 1024</td><td>19.32</td><td>26.6</td><td>37.93</td><td>34.15</td><td>23.34</td><td>12.71</td><td>23.45</td><td>21.81</td><td>24.93</td><td>65.0</td><td>80.88</td><td>38.19</td><td>0.0</td><td></td><td></td><td>31.053.63 57.62</td></tr><tr><td></td><td>Snapkv: 2048</td><td>19.28</td><td>28.81</td><td>40.26</td><td>35.31</td><td>23.75</td><td>13.44</td><td>26.3</td><td>22.29</td><td>25.73</td><td>66.0</td><td>79.93</td><td>39.59</td><td></td><td>0.0 31.0</td><td></td><td>56.0558.61</td></tr><tr><td></td><td>SnapkV: 4096</td><td>20.68</td><td>29.34</td><td>42.21</td><td>33.95</td><td>24.88</td><td>14.15</td><td>28.55</td><td>23.11</td><td>26.45</td><td>66.0</td><td>81.25</td><td>40.52</td><td></td><td>0.0</td><td></td><td>29.554.79 58.81</td></tr><tr><td></td><td>H2O:4096</td><td>19.31</td><td>28.3</td><td>37.75</td><td>30.51</td><td>23.06</td><td>11.76</td><td>27.55</td><td>21.37</td><td>26.49</td><td>66.0</td><td>75.8</td><td>39.92</td><td></td><td>0.0</td><td></td><td>25.553.56 55.53</td></tr><tr><td></td><td>All KV</td><td>26.82</td><td>33.06</td><td>49.28</td><td>42.77</td><td>27.33</td><td>19.27</td><td>32.85</td><td>24.25</td><td>27.06</td><td>71.0</td><td>86.23</td><td>42.98</td><td>2.75</td><td></td><td>86.98 55.51 52.88</td><td></td></tr><tr><td>SI</td><td>SnapKV: 1024</td><td>25.54</td><td>29.51</td><td>49.25</td><td>40.94</td><td>25.7</td><td>19.42</td><td>25.89</td><td>23.82</td><td>26.11</td><td>69.5</td><td>86.48</td><td>42.06</td><td></td><td>2.98</td><td></td><td>88.56 55.65 51.87</td></tr><tr><td></td><td>Snapkv: 2048</td><td>25.89</td><td>32.47</td><td>48.6</td><td>41.71</td><td>27.31</td><td>18.69</td><td>28.81</td><td>24.5</td><td>26.6</td><td>70.0</td><td>86.27</td><td></td><td>42.47</td><td>3.09</td><td></td><td>87.43 55.93 52.01</td></tr><tr><td></td><td>SnapkV: 4096</td><td>26.41</td><td>33.36</td><td>49.81</td><td>42.32</td><td>27.93</td><td>18.76</td><td>30.74</td><td>24.19</td><td>27.08</td><td>71.0</td><td>86.25</td><td></td><td>43.01</td><td>2.73</td><td></td><td>86.18 55.62 52.65</td></tr><tr><td></td><td>H2O: 4096</td><td>22.61</td><td>29.06</td><td>47.22</td><td>36.54</td><td>20.6</td><td>16.25</td><td>30.0</td><td>23.8</td><td>26.75</td><td>70.5</td><td>86.16</td><td></td><td>42.97</td><td>3.46</td><td></td><td>86.38 53.7251.1</td></tr><tr><td></td><td>AIl KV</td><td>26.81</td><td>37.06</td><td>51.55</td><td>47.77</td><td>32.46</td><td>26.59</td><td>34.25</td><td>26.05</td><td>27.91</td><td>76.0</td><td>90.57</td><td></td><td>46.98</td><td>5.5</td><td></td><td>100.0 69.07 69.65</td></tr><tr><td>nI</td><td>SnapKv: 1024</td><td>26.01</td><td>34.65</td><td>51.58</td><td>48.23</td><td>32.67</td><td>25.92</td><td>27.77</td><td>25.0</td><td>27.25</td><td>74.5</td><td>90.42</td><td>46.48</td><td></td><td>5.5</td><td></td><td>99.5 69.02 68.98</td></tr><tr><td></td><td>Snapkv: 2048</td><td>27.12</td><td>36.9</td><td>51.91</td><td>47.46</td><td>33.23</td><td>26.27</td><td>30.19</td><td>25.84</td><td>27.8</td><td>76.0</td><td>90.24</td><td></td><td>46.31</td><td>5.5</td><td></td><td>100.068.72 70.01</td></tr><tr><td></td><td>SnapkV: 4096</td><td>26.46</td><td>37.03</td><td>52.62</td><td>47.71</td><td>33.35</td><td>26.45</td><td>32.64</td><td>25.87</td><td>27.94</td><td>75.5</td><td>90.71</td><td></td><td>47.14</td><td>5.5</td><td></td><td>100.0 68.81 69.56</td></tr><tr><td></td><td>H2O: 4096</td><td>20.45</td><td>32.09</td><td>48.02</td><td>34.76</td><td>25.69</td><td>16.5</td><td>29.76</td><td>23.53</td><td>26.84</td><td>74.5</td><td>90.24</td><td>47.1</td><td></td><td>7.06</td><td></td><td>99.42 64.91 63.52</td></tr></table>

\* Credit to Jin et al. [20] for the template used in the table.

# 5.3 Experiments on LongBench

We evaluate SnapKV on these four models using LongBench [17], a multi-task benchmark designed to rigorously evaluate long context understanding capabilities across various datasets, spanning single and multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. We choose LWM-Text-Chat-1M with 1 million context length, LongChat-7b-v1.5-32k, Mistral-7B-Instruct-v0.2, Mixtral-8x7B-Instruct-v0.1 with $3 2 \mathrm { k }$ context length as our baselines. For each model, we test SnapKV with various settings: compressing KV caches in the prompt to 1024, 2048, and 4096 tokens. We use max pooling with kernel size 7 and observation window size 32. Table 1 illustrates a negligible performance drop from models with SnapKV compared with original implementations for 16 different datasets, even with prompt-KV with 1024 tokens. Some models even outperform the baseline. Our results substantiate that SnapKV can grasp the key information in the long context and give comprehensive summaries with details. Moreover, our results also indicate the effectiveness of SnapKV in compressing the prompt KV cache. For these 4 models, the average input token length is around $1 3 \mathrm { k }$ . Thus, using 1024, SnapKV achieves an average compression rate of $92 \%$ , and using 4096, it reaches $68 \%$ , all with negligible drops in accuracy. We compare SnapKV and H2O on the LongBench dataset to further demonstrate the performance of SnapKV. To fairly evaluate the accuracy, we set the prompt capacity for H2O to 4096. As Table 1 shows, SnapKV delivers significantly better performance than H2O. Even with 1024 prompt KV caches, SnapKV on Mistral-7B-Instruct-v0.2 achieves better performance than H2O with 4096 caches on 11 out of 16 benchmarks.

# 5.4 Experiments on Command-R

To further assess the performance of SnapKV, we conduct experiments using Cohere’s Command-R model [2], an open-source model with 35B parameters and capable of handling sequences of up to $1 2 8 \mathrm { k }$ token length. Command- $\scriptstyle \cdot \mathrm { \mathtt { R } }$ is designed for complex tasks requiring long context, such as retrievalaugmented generation (RAG). We extensively test Command- $\scriptstyle \cdot \mathrm { \mathbb { R } }$ on NarrativeQA and a modified version of the Needle-in-a-Haystack where it achieves promising results. To evaluate SnapKV’s impact on RAG, we ran tests on bioasq [21], multi-hop question answering with HotpotQA [22], and an internal benchmark on tool use, which further demonstrated its effectiveness. Throughout all experiments, we limit the KV cache to a maximum of 4096 tokens, while the pooling kernel size and window size are set to 13 and 64, respectively. For our evaluations, these hyper-parameters give a KV cache compression ratio between $2 \mathbf { x }$ to $3 2 \mathrm { x }$ depending on the sequence length.

# 5.4.1 Needle-in-a-Haystack

In previous experiments [23], it was noted that Needle-in-a-Haystack [18] evaluation was heavily influenced by the specific context used. To address this issue, we modify the evaluation by permuting context compositions for each length and depth combination. This approach, which we ran eight times, yielded more robust results. We observe a slight decrease in scores across all models tested under this setting compared to the original setup with no context shuffling. For simplicity, we aggregated the scores across all depths and lengths for the baseline model and the one with SnapKV. As seen in Table 2, applying SnapKV to Command- $\scriptstyle \cdot \mathrm { \mathbb { R } }$ shows no degradation in performance, even with a $1 2 8 \mathrm { k }$ sequence length resulting in $3 2 \mathrm { x }$ compression of KV cache.

Table 2: Needles-in-a-Haystack Test Results   

<table><tr><td>Model</td><td>Command-R </td><td>Command-R + SnapKV</td><td>% Difference</td></tr><tr><td>Score</td><td>9.866</td><td>9.819</td><td>-0.5%</td></tr></table>

# 5.4.2 Retrieval Augmented Generation (RAG)

We assess SnapKV’s effectiveness in RAG tasks, which are more intricate than synthetic long-context tasks like Needle-in-a-Haystack and closer to real use cases compared to tasks like NarrativeQA. RAG tasks require selecting pertinent documents from an indexed corpus based on the given prompt. An expanded context window enables the retrieval of additional documents, which can lead to improved model performance. However, this also increases memory requirements and latency, highlighting the delicate balance between retrieval scope and system resources. SnapKV proves beneficial in these tasks by reducing memory usage while enhancing the performance. We evaluated SnapKV’s impact on RAG tasks with sequence lengths up to approximately 40,000 tokens.

RAG Citation We begin by assessing SnapKV’s impact on the model’s ability to select relevant documents, a crucial aspect of effective RAG. We evaluate on an internal benchmarks from Cohere. The setup of the benchmark is as follow: for each prompt, we gathered a set of topic-related documents that included ground truth answers along with a sample of negative documents ensuring a total of 100 documents per prompt. We measured the model’s performance by calculating the F1-score when the model successfully retrieved the ground truth documents. The dataset employed in this experiment spanned context lengths from 20,000 to 40,000 tokens. Given our KV cache size of 4096, we achieve a compression of 5-10x. As observed in Table 3, SnapKV demonstrates a remarkable ability to retain nearly $9 8 . 8 \%$ of Command-R’s performance.

Table 3: RAG Test Results   

<table><tr><td>Evaluation Task</td><td>Metric</td><td>% Difference</td></tr><tr><td>RAG Citation</td><td>F1 score</td><td>-1.2%</td></tr><tr><td>RAG End-to-end</td><td>F1 score</td><td>-2.1%</td></tr></table>

Generation As the quality of generation is important to a model’s RAG capability, we evaluate Command-R on lost-in-the-middle and generation quality. Lost-in-the-middle is aimed to analyze whether the performance of the model varies when altering the position of ground-truth information in the context [24]. The latter is a relatively simple metric where we define the accuracy of the model to be the proportion of the ground-truth answer phrase appearing in model’s response. We conducted 3 experiments with 30, 100 and 200 sampled documents for each ground-truth. We repeat each experiment 3 times and insert the relevant documents at beginning, middle and end of the context to test SnapKV’s robustness.We report the relative difference to the baseline model. The dataset used in this phase is based on the bioasq dataset [21] with RAG-style formulation from Cohere [25].

Table 4: RAG Generation Test Results on bioasq   

<table><tr><td>Number of Documents</td><td>Approximate Context Length</td><td>Ground Truth Position</td><td>% Difference</td></tr><tr><td rowspan="4">30</td><td rowspan="4">8k</td><td>0</td><td>-1.8%</td></tr><tr><td>14</td><td>0%</td></tr><tr><td>30</td><td>-3.4%</td></tr><tr><td>Avg</td><td>-1.7%</td></tr><tr><td rowspan="4">100</td><td rowspan="4">14k</td><td>0</td><td>-1.2%</td></tr><tr><td>14</td><td>+0.9%</td></tr><tr><td>30</td><td>-0.9%</td></tr><tr><td>Avg</td><td>-0.6%</td></tr><tr><td rowspan="4">200</td><td rowspan="4">24k</td><td>0</td><td>+4.9%</td></tr><tr><td>14</td><td>+4.9%</td></tr><tr><td>30</td><td>+6.4%</td></tr><tr><td>Avg</td><td>+5.4%</td></tr></table>

Note: For each number of sampled documents, we report the approximate context length and the difference from the baseline at each ground-truth position.

As Table 4 shows, SnapKV is robust in terms of generation quality and does not suffer from the well-known lost-in-the-middle pathology. Moreover, SnapKV improves performance over the baseline model when the context contains close to 200 documents. One potential explanation to this is that by adequately compressing the KV cache, we can effectively reduce the noise from negative documents and push the model to construct attention scores more focused on the relevant information.

End-to-End RAG To assess SnapKV’s robustness in a comprehensive manner, we integrated it into a complete RAG pipeline. This evaluation starts by retrieving 200 documents using Cohere’s embedding service [26] in response to a given query. These documents were then re-ranked using Cohere’s re-ranking model [27], which filtered out half of the candidates, resulting in a list of 100 documents. We prompt Command- $\scriptstyle \cdot \mathrm { \mathbb { R } }$ using this list and calculate the accuracy metric as described in Section 5.4.2. We employed a modified version of the HotpotQA dataset [22] and leveraged Wikipedia as the document source. This setup introduces a more challenging set of documents as all documents, relevant or not, are semantically similar.

Table 3 showcases SnapKV’s robust performance in a production-like RAG setting. With an average dataset length of around 16,000 tokens, the KV cache benefits from a compression ratio of approximately $4 \mathbf { x }$ .

# 5.5 Case Study: Compatibility with Parallel Decoding

In this section, we provide a novel perspective on employing KV cache compression synergistically with parallel decoding [28–32]. Parallel decoding leverages a lightweight model or an adaptor to draft initial tokens, which are subsequently verified by larger LLMs. This strategy effectively reduces memory overhead, a critical concern given the autoregressive nature of LLMs that renders them more memory-intensive than computationally demanding. Specifically, in LLMs, each decoding step involves generating a single token, with the transfer of weights between High Bandwidth Memory (HBM) and cache contributing to significant overhead [33, 34].

Our investigation incorporates SnapKV with Medusa $[ 3 5 ] ^ { 3 }$ , a cutting-edge parallel decoding framework that utilizes multiple classifiers and tree attention mechanisms for drafting tokens, subsequently verified by LLMs. One of the challenges identified is the issue of speculative decoding in processing long sequences since generating multiple tokens per decoding step introduces computational bottlenecks during long sequence processing, such as query-key matrix multiplication tiling [36]. By maintaining a constant size for the KV cache associated with prompts during generation, SnapKV enhances generation efficiency.

![](images/7696c05130d14b3591fb97e61e9bdfe2e85f94521579ad1d3c7c02042a49f77a.jpg)  
Figure 9: Comparison of generation speed (ms/token). The baseline is the Huggingface implementation of naive decoding.

Empirical results shown in Figure 9 highlight the performance across various prompt lengths, with Mistral-7B-Instruct- $\cdot \mathtt { v } 0 . 2 ^ { 4 }$ undergoing a maximum of 128 generation steps unless preemptively halted. The experiments utilized a subset of the QASPER [37], with a fixed prompt instructing the LLM to summarize the paper. The truncation strategy adopted aligns with LongBench [17] standards, by removing the context in the middle to achieve the desired sequence length for benchmarking.

The findings indicate a slowdown in Medusa’s performance as sequence lengths extend, a challenge effectively mitigated by SnapKV’s intervention, which achieved a $1 . 3 \mathrm { x }$ speedup for sequences with $1 0 \mathrm { k }$ length compared to Medusa and a $2 . 2 \mathbf { x }$ speedup compared to the native decoding. This improvement underscores the potential of combining KV cache compression with parallel decoding frameworks to enhance LLM efficiency, particularly in long-context scenarios.

# 6 Discussions

SnapKV is an effective yet straightforward solution that compresses the KV cache to mitigate the computational and memory burdens of processing extensive prompts. Observing that specific tokens within prompts gain consistent attention from each head during generation, our methodology not only retrieve crucial information but also enhances processing efficiency. Despite its strengths, SnapKV’s scope is primarily confined to the generative aspect of models, specifically targeting the KV cache during the generation. This limitation implies that SnapKV cannot extend a model’s long context capability if the model inherently struggles with long contexts or exhibits poor performance. Additionally, SnapKV’s design does not cover the processing of the prompt inference, which limits its effectiveness in scenarios where the system cannot handle prompts of extensive length. Nonetheless, our contributions offer significant insights and tools for the community, paving the way for more refined approaches on managing the challenges of large-scale language modeling. The appendix provides more experiments with parallel decoding and the discussion about generation speedup.

References   
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
[2] Cohere. Command r: Retrieval-augmented generation at production scale, March 2024. URL https://txt.cohere.com/command-r.   
[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku, March 2024. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf.   
[4] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   
[5] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.   
[6] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.   
[7] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024.   
[8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023.   
[9] Bing Liu and Sahisnu Mazumder. Lifelong and continual learning dialogue systems: learning during conversation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 15058–15063, 2021.   
[10] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, et al. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499, 2023.   
[11] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.   
[12] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization. arXiv preprint arXiv:2104.05938, 2021.   
[13] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023.   
[14] Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277–293, 2021.

[15] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

[16] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024.

[17] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

[18] G Kamradt. Needle in a haystack–pressure testing llms, 2023.

[19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.

[20] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024.

[21] Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima López, Eulália Farré-Maduell, Luis Gasco, Martin Krallinger, and Georgios Paliouras. Overview of bioasq 2023: The eleventh bioasq challenge on large-scale biomedical semantic indexing and question answering, 2023.

[22] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

[23] Anthropic. Long context prompting for claude 2.1, December 2023. URL https://www. anthropic.com/news/claude-2-1-prompting.

[24] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157–173, 2024.

[25] Cohere. Retrieval augmented generation (rag), 2023. URL https://docs.cohere.com/ docs/retrieval-augmented-generation-rag.

[26] Cohere. Cohere embed, 2023. URL https://docs.cohere.com/reference/embed.

[27] Cohere. Cohere rerank, 2023. URL https://docs.cohere.com/docs/rerank-guide.

[28] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.

[29] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.

[30] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[31] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.

[32] Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, and Yunfei Cheng. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919, 2024.   
[33] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.   
[34] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.   
[35] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.   
[36] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference, 2023.   
[37] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021.

# A Discussion of Generation Time Speedup

To better assess SnapKV’s effectiveness across different stages, we documented a detailed time breakdown for Mistral-7B-Instruct-v0.2 during both the prompting and generation stages. We configured the model to consistently generate 512 tokens, facilitating a direct comparison with the prompting stage. As illustrated in Figure 10, generation time dominates the whole processing time for LLMs over input sequences, introducing significant overhead. While the generation time for the original model increases with input length, SnapKV maintains a consistent decoding speed regardless of input length, significantly reducing generation time. Especially, SnapKV is able to achieve balanced prompting time and generation time with input length smaller than 100k.

![](images/01c3815cee685f549d277d3a4ebc776fdb6bbcec6e2c6e82b0b054acfa262f50.jpg)  
Prompting Latency V.S. Generation Time   
Figure 10: The prompting time and generation time comparison between Mistral model with and without SnapKV.

# B Visulization of the Generated Context

![](images/c634582b855a84ff8efe3c218926b440c092b02f6dbfed95d53a286dcece22f8.jpg)  
Figure 11: Visualization of generation examples from Samsum, Qasper, HotpotQA datasets with mistral-7B-instruct-v0.2. Results are compared between ground truth, SnapKV with 1024 prompt tokens, with 2048, with 4096, the baseline model with full KV cache.