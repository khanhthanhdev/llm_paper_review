# Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers

Chao Lou1 Zixia Jia2 Zilong Zheng2 ,∗ Kewei Tu1,∗ 1 ShanghaiTech University 2 National Key Laboratory of General Artificial Intelligence, BIGAI {louchao,tukw}@shanghaitech.edu.cn {jiazixia,zlzheng}@bigai.ai

# Abstract

Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top- $\mathbf { \nabla } \cdot \mathbf { k }$ mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available.

# 1 Introduction

Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1, 20, 19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.

Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods [39, 61] allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually cost $O ( n \log n )$ to maintain clusters. Ainslie et al. [1] incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.

![](images/f7cedf213663e0f72b017d84873fae8e23ec692cf87d241e87ec16274d4e043d.jpg)  
Figure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query $( \tau ( \mathbf { u } ) )$ such that the sum of normalized scores is $k$ , which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency.

To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows.

# Incremental KV Selection.

The SPARSEK operator (§ 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results.

Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65, 32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (§ 3.2).

Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency.

We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments on language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention.

# 2 Related Work

Long-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence [17, 35, 36, 12] or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15, 27, 42].

Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns [34, 13], or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27]. However, these static methods often prove suboptimal in various scenarios [66, 2]. Alternatively, sparse patterns can be learned in a data-driven manner. For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens. Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs. Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference. A recent approach by Anagnostidis et al. [2] introduces a learnable, irreversible key-value pair pruning for inference-time memory efficiency with the concept of relaxing pruning actions to accumulated gating. However, this method still suffers from quadratic complexity during training, hindering its ability to expedite the training process. In this paper, we present a novel, efficient sparse attention mechanism with learnable patterns, addressing all the aforementioned challenges.

# 3 SparseK Attention

# 3.1 Background

Self-Attention Given a sequence of vectors $\ b { X } \in \mathbb { R } ^ { n \times d }$ where $n$ is the sequence length and $d$ is the hidden dimension, an attention head first projects $\boldsymbol { X }$ into query, key and value vectors with $\ b { W _ { Q } } , \ b { W _ { K } } , \ b { W _ { V } } \in \mathbb { R } ^ { d \times p }$ where $\textstyle p = { \frac { d } { h } }$ and $h$ is the number of attention heads:

$$
Q = X W _ { Q }
$$

$$
K = X W _ { K } \quad \quad \quad \quad V = X W _ { V } ,
$$

In the decoder-only architecture [72], a causal attention mask $M$ guarantees each query $q _ { i }$ only attends to positions $\leq i$ . Consequently, the output $o$ of single-head dot-product attention is defined as

$$
S = Q K ^ { \top } \qquad P = \operatorname { S O F T M A X } ( S + M ) \qquad O = P V
$$

The multi-head self-attention concatenates the outputs of multiple heads (indexed by subscripts) and applies a linear projection with WO ∈ Rd×d:

$$
\mathbf { M H A } ( { \boldsymbol { X } } ) = \mathbf { C o n c a t e n a t e } ( O _ { 1 } , O _ { 2 } , \ldots , O _ { h } ) W _ { O }
$$

The quadratic complexity of self-attention is contributed by the quadratically sized attention weight $_ { s }$ . Inspired by Ainslie et al. [1], we propose to select a constant number of key-value pairs for each query in an irreversible way (defined formally in the following subsections 3.2 and 3.3), leading to linear training complexity and a constant inference-time memory cost. For simplicity, here we omit the RoPE position embedding [64] and focus on single-head attention to illustrate our methodology. The multi-head case is briefly discussed in Appendix C.7.

SparseMax operator There are many popular technical choices that relax ARGMAX operation, such as SOFTMAX and SPARSEMAX [46]. Especially, SPARSEMAX uses the Euclidean projection onto the probabilistic simplex and tends to yield sparse solutions:

$$
\begin{array} { r } { \mathrm { S P A R S E M A X } ( z ) : = \underset { p \in \triangle ^ { m - 1 } } { \arg \operatorname* { m i n } } | | p - z | | ^ { 2 } , } \end{array}
$$

where $\triangle ^ { m - 1 } = \{ p \in \mathbb { R } ^ { m } | \mathbf { 1 } ^ { \top } p = 1 , p \geq 0 \}$ . Building on this, we introduce SPARSEK, an extension of SPARSEMAX for the case where $k = \mathbf { 1 } ^ { \top } p \geq 1$ .

# 3.2 Learnable Key-Value Pair Selection

Key-value pair selection We use $\Delta \in \{ 0 , 1 \} ^ { k \times m }$ to represent the selection of $k$ key-value pairs out of $m$ entries, where $\Delta ( i , j ) = 1$ indicates that the $j$ -th key-value pair is the $i$ -th selected entry ( i.e., the $j$ -th key-value pair is positioned in the $i$ -th slot after sorting), and $\Delta ( i , j ) = 0$ otherwise. It is noteworthy that each column vector and row vector exhibit one-hot characteristics. We use subscripts to distinguish selection corresponding to different queries, i.e., $\Delta _ { t }$ for $\pmb q _ { t }$ . The causality of Transformer decoders puts a natural constraint: $\Delta _ { t } ( i , j ) = 0$ if $j > t$ . Then, the self-attention with query $\pmb q _ { i }$ and its selected contexts are defined as

$$
\begin{array} { c } { \pmb { q } _ { i } = \pmb { W } _ { Q } \pmb { x } _ { i } \qquad \hat { \pmb { K } } _ { i } = \Delta _ { i } \pmb { K } \qquad \hat { \pmb { V } } _ { i } = \Delta _ { i } \pmb { V } } \\ { \hat { \pmb { s } } _ { i } = \pmb { q } _ { i } ^ { \top } \hat { \pmb { K } } _ { i } \quad \pmb { o } _ { i } = \hat { \pmb { p } } _ { i } ^ { \top } \hat { \pmb { V } } _ { i } \quad \hat { \pmb { p } } _ { i } = \mathrm { S O F T M A X } \big ( \hat { \pmb { s } } _ { i } \big ) , } \end{array}
$$

Irreversibile selection In a series of decoder steps, the selection of key-value pairs for queries $\left\{ q _ { 1 } , q _ { 2 } , \ldots , q _ { n } \right\}$ is performed over incremental candidate sets (e.g., ${ \pmb k } _ { 1 } \bar  \} , \{ { \pmb k } _ { 1 } , { \pmb k } _ { 2 } \} , \dots , \{ { \bar { \pmb k _ { i } } } \} _ { i = 1 } ^ { n } )$ . Irreversible selection is a strategy that, at step $t$ , $k _ { i }$ $( i < t )$ can be selected only if it has been selected in all preceding steps from $i$ to $t - 1$ , $\forall t ^ { \prime } \in \{ i , i + 1 , \dots , t - 1 \} , \exists j : \Delta _ { t ^ { \prime } } ( j , i ) = 1$ . Irreversible selection is memory-efficient because it guarantees the feasibility of pruning a key-value pair once it is not selected at a step. Many previous studies, such as clustering-based methods [61, 39], put no constraint on interactions between queries and key-value pairs and thus are not irreversible, resulting in a memory cost equivalent to that of pure self-attention.

We employ a simple selection strategy in this paper: we score each key-value pair and choose the key-value pairs with the top- $k$ scores. This strategy is irreversible because of the monotonic increase of the $k$ -th largest value in incremental sets. Note that we do not use attention scores as the selection criteria [32] to avoid the quadratic cost of computing $Q K ^ { \top }$ .

Learnable selection Formally, we use a scoring network, a linear projection with parameter $\pmb { w } _ { s c o r e } \in \mathbb { R } ^ { d }$ augmented with a position slope $\bar { d } ( \bar { \boldsymbol { u _ { ) } } } \in \mathbb { R } ^ { n }$ , to get the importance scores $\textbf { \em u }$ of the key-value pairs computed from the input vectors $\boldsymbol { X }$ :

$$
\pmb { u } = \pmb { X } \pmb { w } _ { s c o r e } + d ( \pmb { u } ) , \qquad \mathrm { w h e r e } \quad d ( \pmb { u } ) _ { i } = i \epsilon .
$$

for some small positive scalar $\epsilon$ . The introduction of distance slope is to avoid numerical explosion of $X w _ { s c o r e }$ : if without it, the scoring network would be compelled to predict increasingly larger scores to retain new tokens and discard old ones. This hurts training stabilitiy and generalization ability to larger context length.

Then, the selection matrix $\Delta _ { t }$ for query $q _ { t }$ can be defined to select top- $k$ entries,

$$
\begin{array} { r l } & { { m } _ { t o p k } = \mathrm { T O P K } ( \pmb { u } _ { 1 : i } , k ) } \\ & { \Delta _ { t } ^ { h a r d } = \mathrm { M a s k S e l e c t } ( \mathrm { D i a g } ( { m } _ { t o p k } ) , { m } _ { t o p k } ) , } \end{array}
$$

where $m _ { t o p k }$ is an indicator vector, i.e., $m _ { t o p k } ( j ) = 1$ if $u _ { j }$ ranks within the top- $k$ of $\mathbf { \lambda } _ { \mathbf { u } _ { 1 : i } } =$ $\{ u _ { 1 } , u _ { 2 } , \ldots , u _ { i } \}$ and $m _ { t o p k } ( j ) = 0$ otherwise, and Diag $( m )$ is a matrix with $_ { \mathbf { \nabla } } \mathbf { m } _ { \mathbf { \nabla } }$ in the main diagonal. The function MaskSelect $( X , m )$ selects rows of $\boldsymbol { X }$ according to the mask $_ { m }$ .

TOPK is not differentiable with respect to its input, preventing us from updating ${ \pmb w } _ { s c o r e }$ with gradientbased methods. Therefore, we propose to use SPARSEK, a differentiable relaxation of TOPK, in Equation (7),

$$
\begin{array} { r l } & { m _ { s p a r s e k } = \mathrm { S P A R S E K } ( \pmb { u } _ { 1 : i } , k ) } \\ & { \Delta _ { t } ^ { s o f t } = \mathrm { M a s k S e l e c t } ( \mathrm { D i a g } ( m _ { s p a r s e k } ) , m _ { t o p k } ) , } \end{array}
$$

# 3.3 The Differentiable SparseK Operator

Definition We relax the constraint in Equation (4) from a probablistic simplex to a $\mathrm { k }$ -sum constraint $\mathbb { C } = \{ p | \mathbf { 0 } \leq p \leq \mathbf { 1 } , \mathbf { 1 } ^ { \top } p = k \}$ and define SPARSEK as follows,

$$
\begin{array} { r l } { \mathrm { S P A R S E K } ( z , k ) : = } & { \underset { \pmb { p } \in \mathbb { C } } { \arg \operatorname* { m i n } } \ : | | \pmb { p } - z | | ^ { 2 } } \\ & { = \underset { \pmb { p } \in \mathbb { C } } { \arg \operatorname* { m a x } } \pmb { p } ^ { \top } z + H ^ { G } ( \pmb { p } ) , } \end{array}
$$

where $\begin{array} { r } { H ^ { G } ( \pmb { p } ) = \frac { 1 } { 2 } \sum _ { j } p _ { j } ( 1 - p _ { j } ) } \end{array}$ is the generalized Gini entropy for $p \in \mathbb { C }$ instead of a distribution.

SPARSEK is related to SoftTopK as explained below. The generalized Gini entropy is a special case of the generalized $\alpha$ -Tsallis entropy $\dot { H } ^ { T } ( p )$ for $\alpha = 2$ [71], where $\begin{array} { r } { H ^ { T } ( \pmb { p } ) = \frac { \hat { \textbf { 1 } } } { \alpha ( \alpha - 1 ) } \sum _ { j } ^ { } ( p _ { j } - p _ { j } ^ { \alpha } ) } \end{array}$ for $\pmb { p } \in \mathbb { C }$ . Then we can define the generalized $\alpha$ -ENTTOPK operator as

$$
\alpha \mathrm { - } \mathrm { E N T T O P K } = \underset { \pmb { p } \in \mathbb { C } } { \arg \operatorname* { m a x } } \pmb { p } ^ { \top } \pmb { z } + H ^ { T } ( \pmb { p } ) .
$$

The entropic index $\alpha$ controls the smoothness of the solution. Taking the limit of $\alpha  \infty$ , we get the TOPK operator. Taking the limit of $\alpha  1$ , the generalized $\alpha \cdot$ -Tsallis entropy becomes the generalized Gibbs-Boltzmann-Shannon entropy and we obtain the SOFTTOPK operator [41].

Solution Similar to SPARSEMAX, SPARSEK is a soft-thresholding operation. Its solution is expressed as follows:

$$
\begin{array} { c } { p ^ { * } = \operatorname* { m a x } ( \operatorname* { m i n } ( z - \tau ( z ) , \mathbf { 1 } ) , \mathbf { 0 } ) , } \\ { \tau ( z ) = \frac { \sum _ { u ^ { * } < j \leq w ^ { * } } z _ { ( j ) } + u ^ { * } - k } { w ^ { * } - u ^ { * } } , } \end{array}
$$

where $\tau ( z ) : \mathbb { R } ^ { n }  \mathbb { R }$ is the threshold function that satisfies $\sum p ^ { * } = k$ $k , z _ { ( 1 ) } > z _ { ( 2 ) } > \cdot \cdot \cdot > z _ { ( m ) }$ is the sorted coordinates of $_ z$ , $u ^ { * }$ is the number of entries with value 1 in $p ^ { * }$ , and $\boldsymbol { w } ^ { * }$ is the number of entries with nonzero values. Algorithm 1 illustrates an $O ( m \log m )$ algorithm for evaluating SPARSEK. With the pre-computed cumulative sum of $_ z$ , line 5 can be evaluated in $O ( 1 )$ , and thus the overall complexity is primarily due to sorting. We give the proof in Appendix A.

# Algorithm 1 Evaluate SPARSEK(z, k)

1: Input: $_ z$   
2: Sort z as z(1) ≥ . . . ≥ z(m)   
3: Pre-compute the cumulative sum of $_ z$   
4: for $( u , w )$ in the descending order do   
5: $\tau \gets$ as in Equation (13)   
6: if $z _ { ( w ) } > \tau$ and $z _ { ( u ) } \ge \tau + 1$ then   
7: break   
8: end if   
9: end for   
10: $p \gets \operatorname* { m a x } ( \operatorname* { m i n } ( z - \tau , \mathbf { 1 } ) , \mathbf { 0 } )$   
11: Output: $\pmb { p }$

# Algorithm 2 Evaluate $\mathrm { S P A R S E K } ( z _ { 1 : t } , k )$ at step t from the result of step $t - 1$

# Algorithm 3 Train with chunk-wise recurrency

1: Input: $\pmb { X } = [ \pmb { X } _ { 1 } , \pmb { X } _ { 2 } , \dots , \pmb { X } _ { l } ]$   
2: Initialize a KV cache with scores   
3: for all $X _ { i }$ in $\boldsymbol { X }$ do   
4: $O _ { i } \gets$ attention with $X _ { i }$ and the KV cache   
5: Add new KV pairs and scores to the cache   
6: Prune the KV cache to size $k$ by scores   
7: Stop gradients of the KV cache   
8: end for   
9: Output: $[ O _ { 1 } , O _ { 2 } , \dots , O _ { l } ]$

1: Input: $z _ { t }$ , min-heaps $\begin{array} { r } {  { { \mathcal F } } ~ = ~  { \{ z _ { i } \vert z _ { i } } } ~ \geq  \end{array}$ $\tau ( z _ { 1 : t - 1 } ) + 1 \}$ and $S = \{ z _ { i } | z _ { i } > \tau ( z _ { 1 : t - 1 } ) \}$ , $\tau _ { t - 1 }$ from step t − 1 Sort z as z( z(m) Pre-compute the cumulative sum of z   
2: if $z _ { t } \ge \tau ( z _ { 1 : t - 1 } ) + 1$ then   
3: Insert $z _ { t }$ into $\mathcal { F }$   
4: end if   
5: if $z _ { t } > \tau \big ( z _ { 1 : t - 1 } \big )$ then   
6: Insert $z _ { t }$ into $s$   
7: end if   
8: for $( u , w )$ in the descending order from $( | S | , | F | )$ do   
9: $\tau  \mathrm { a s }$ in Equation (13)   
10: Prune $s$ and $\mathcal { F }$ with the new $\tau$   
11: if $z _ { ( w ) } > \tau$ and $z _ { ( u ) } \ge \tau + 1$ then   
12: break   
13: end if   
14: end for   
15: $p \gets \operatorname* { m a x } ( \operatorname* { m i n } ( z - \tau , \mathbf { 1 } ) , \mathbf { 0 } )$   
16: Output: $p , s , { \mathcal { F } } , \tau$

Algorithm 1 can be extended to evaluate SPARSEK on incremental sets. The idea is that we can compute step $t$ based on the results from step $t - 1$ instead of starting from scratch. Algorithm 2 illustrates the algorithm, where highlighted lines are the main difference from Algorithm 1. We introduce two min-heaps (and maintain the sum of elements for each heap) for tracking the search progress of $( u , w )$ and achieving $O ( 1 )$ evaluation of line 9 in Algorithm 2. Note that each insertion into a min-heap costs logarithmic time in the heap size and each $z _ { t }$ introduces at most two more possible $( u , w )$ pairs (lines 2-6 in Algorithm 2). Therefore, executing Algorithm 2 over $m$ incremental sets (i.e., $m$ steps) costs $O ( m \log m )$ in total.

As Peters et al. [54] have noted, the solution $p ^ { * }$ tends to contain only a few nonzeros, leading to small $u ^ { * }$ and $w ^ { * }$ . Therefore, in practice, we can use partial sort on the $k ^ { \prime } = O ( k )$ largest values instead of full sort in Algorithm 1, thereby achieving a complexity of $O ( m \log k )$ . With respect to Algorithm 2, this change is equivalent to restricting the size of the min-heap $s$ to an upper bound for achieving the same reduction in complexity.

# 3.4 Extensions

Training with fixed-size truncation-free cache Our selection method enables training on extremely long documents that need to be segmented into smaller chunks for recurrent processing. Algorithm 3 illustrates the process. Without introducing any additional truncation strategies or parameters, the algorithm maintains a fixed-size cache benefit by recurrent calculations and produces exactly the same results as calculating without chunking, which is guaranteed by the irreversibility of our selection method. To minimize the memory footprint, we stop the gradients of the cache, thereby pruning the computation graph, as in Transformer-XL [17]. With this algorithm, we can extend the training context length to hundreds of thousands of tokens.

Combine with other efficient attention mechanism Our SPARSEK attention can be combined with other sparse attention as long as they have irreversible selection patterns. In this work, we integrate SPARSEK attention with sliding window (SW) attention by default, motivated by the well-known experience that sliding windows are simple yet incredibly strong for language modeling [58, 37]. Specifically, given a sliding window size $w$ , we replace $\hat { K _ { i } } , \hat { V _ { i } }$ in (5) with

$$
\hat { K } _ { i } = \left[ \mathop { \Delta _ { i - w } K } _ { K _ { i - w + 1 : i } } \right] \quad \hat { V } _ { i } = \left[ \mathop { \Delta _ { i - w } V } _ { V _ { i - w + 1 : i } } \right] ,
$$

This combination does not introduce any overhead thanks to our fused Triton kernel. In this combination, SPARSEK attention attention aims at efficiently global (long-range) dependencies modeling, while SW attention is used for modeling local dependencies.

Besides, SPARSEK attention can also be combined with linear attention methods, which hypothesize the existence of low-rank structures in attention scores rather than sparsity. From a theoretical perspective, Chen et al. [10] reveal that linear attention and sparse attention capture different attention patterns, and their combination provides a closer approximation to full attention. In this work, we extend their results to SPARSEK attention and recent attention optimizations [19]. For technical details, please refer to Appendix B.1.

Straight-through estimator From TOPK to SPARSEK, we employ relaxation techniques to facilitate gradient-based training. Alternatively, the straight-through estimator (ST) [5] can be utilized, i.e., $\Delta ^ { s t } \breve { = } \Delta ^ { s o f t } - \mathrm { s t o p \_ g r a d } \breve { ( } \Delta ^ { s o f t } ) + \Delta ^ { h i r d }$ , allowing the model to perform true selection. By utilizing the ST method, the model achieves slightly improved efficiency since it bypasses the multiplication of selection scores during the forward pass. Our experimental results indicate that employing ST results in negligible performance degradation. Consequently, this technique shows promise in balancing performance and computational efficiency.

# 3.5 Techniques for Faster and More Stable Training

We introduce three beneficial modeling tricks discovered in our experiments. We also develop an optimized implementation based on FlashAttention- $2 ^ { 2 }$ for obtaining practically efficient sparse attention. Please refer to Appendix B.2 for details.

Score normalization We add timestep normalization [44] on the time dimension: $\begin{array} { r l } { \mathbf { u } ^ { \prime } } & { { } = } \end{array}$ TimestepNorm $( \mathbf { u } )$ , which computes cumulative mean and variance for each timestep. Note that the gradient of SPARSEK operation is sparse due to thresholding. We hypothesize that the additional normalization enables gradients in every position, thus resulting in better training. Moreover, timestep normalization is necessary to avoid numerical explosion when combining SPARSEK attention with linear attention. More discussion is in Appendix B.1.

Hard selection for keys and soft selection for values Recall that the SPARSEK attention is given by S $\operatorname { \mathrm { ) F T M A X } } ( \dot { \mathbf { Q } } ( \Delta ^ { s o f t } K ) ^ { \top } ) ( \Delta ^ { s o f t } V )$ . We have found it generally beneficial to use SO $\mathrm { \Delta ) F T M A X } ( Q ( \Delta ^ { h a r d } K ) ^ { \top } ) ( \Delta ^ { s o f t } V )$ instead, especially when fine-tuning pretrained models. This preference arises because the gradient of $\Delta ^ { s o f t }$ via $\Delta ^ { s o f t } K$ can be problematic given that, in pretrained models, some entries of $2 K ^ { \top }$ can be very close to the limitations of the bfloat16 data type, whereas $V$ and the output of SOFTMAX are generally stable.

Initialization to mimic attention score When integrating our SPARSEK attention into pre-trained models, we hypothesize that an effective initialization for ${ \pmb w } _ { s c o r e }$ should ensure that the importance scores u correspond with cumulative attention scores, preventing key-value pairs that receive heavy attention from being pruned. Inspired by the selection metric proposed in Yu et al. [78], whose ranking has been demonstrated to correlate strongly with the rankings of attention weights, we use $W _ { Q } , W _ { K }$ within the pretrained model for initialization: $\pmb { w } _ { s c o r e } = \pmb { w } ^ { \prime } / \| \pmb { w } ^ { \prime } \|$ where $\pmb { w } ^ { \prime } = \pmb { W } _ { Q } \pmb { W } _ { K } ^ { \top } \pmb { 1 }$ .

# 4 Experiments

To evaluate the efficiency, scalability, and compatibility of SPARSEK attention, we tested it across various model architectures and scales. Our experiments focus on language modeling tasks using the OpenWebText corpus [28] and the SlimPajama corpus [63], and downstream tasks in LongBench [3]. Empirically, the SPARSEK attention mechanism outperforms all previous efficient attention methods. More results, such as speed benchmark, ablation study and visualization, can be found in Appendix C.

# 4.1 Language Modeling from Scratch

We adopt the GPT-2 small architecture, comprising 124 million parameters [57]. Notably, we substitute the original absolute position embeddings with rotary position embeddings as proposed in Su et al. [64]. We alter the standard full attention in this architecture with our SPARSEK attention and several other efficient attention methods for comparison. For sparse attention, we include sliding window attention (SW) [56, 51], fixed sparse attention (Fixed) [13], randomized sparse attention (Random) [50] and hash attention (Hash) [39, 50]. We adjust sparsity configurations to restrict the context window size to about 256 when the context length is 1024

Table 1: Perplexity on the OpenWebText held-out set.   

<table><tr><td>Model</td><td>Training 1024 4096</td><td>Context Length 8192</td></tr><tr><td>Full attention</td><td>23.13 21.64</td><td>21.86</td></tr><tr><td>SW</td><td>23.90 23.99</td><td>23.10</td></tr><tr><td>Linear + SW</td><td>23.27 22.97</td><td>23.23</td></tr><tr><td>Fixed</td><td>23.26 22.47</td><td>22.86</td></tr><tr><td>Random</td><td>30.77 34.49</td><td>49.76</td></tr><tr><td>Hash</td><td>26.53 27.42</td><td>27.58</td></tr><tr><td>GLA</td><td>23.29</td><td>22.36 24.15</td></tr><tr><td>RetNet</td><td>24.55 24.50</td><td>26.75</td></tr><tr><td>SPARSEK + SW</td><td>22.85</td><td>21.98 21.84</td></tr><tr><td>SPARsEK + Linear + SW</td><td>22.46 21.55</td><td>21.32</td></tr></table>

or 4096 and about 512 when 8192. For linear attention, we utilize the kernelization proposed by Katharopoulos et al. [38]. Additionally, we compare our methods to recent linear attention works that employ their own architectures rather than the GPT-2 architecture: GLA [77] and RetNet [67]. As the smallest GLA and RetNet is 340M, We modify their hyperparameters to align with our setting. Detailed hyperparameters and results of other configurations can be found in Appendix C.1.

We trained all models on the OpenWebText3 corpus for 10,000 steps, varying the context length. The results are presented in Table 1. Our SPARSEK $\mathrm { . + S W }$ method consistently outperforms all previously established efficient attention methods. Particularly, SPARSEK $+ \mathbf { S } \mathbf { W }$ offers superior performance and has lower time complexity compared to previous learnable sparse attention methods, such as hash attention. Furthermore, linear attention methods, such as Linear $+ \mathbf { S } \mathbf { W } _ { }$ , GLA, and RetNet, demonstrate limitations, particularly in modeling long contexts. However, when combining linear attention with SPARSEK attention, we observed additional performance gains over SPARSEK $+ \mathrm { S W }$ , even surpassing full attention. This suggests the potential of exploring a mixture of different attention methods for more efficient modeling.

# 4.2 Fine-tuning Existing Models

![](images/8cc085b5c11bd58b671b6793d677b70e4a019982fbe7bafe6c00b984488eb61e.jpg)  
Figure 2: Perplexity on the held-out set of fine-tuned models. L denotes the training context length.

We replace the standard full attention in Pythia 160M, Pythia 410M [6] and TinyLlama 1.1B [80] with our SPARSEK attention and sliding window attention. The models are then fine-tuned over a few steps to ensure compatibility with the modified attention modules. Here we only consider sliding window attention because other efficient attention methods often require additional changes of the model architecture and sliding window attention is reported to be efficient in Chen et al. [11]. In fine-tuning, the NTK-aware interpolation [7] is adopted to extend the limit of pretrained positional encodings. For the Pythia models, we utilize a $1 \%$ sampled subset of the SlimPajama dataset4 [63] to perform fine-tuning on moderate-length settings (i.e., $4 \mathrm { k }$ and 8k). In contrast, we use an upsampled dataset comprising long documents [25] to fine-tune the TinyLlama models on long-length settings (i.e., 8k and 16k). Training hyperparameters are listed in Appendix C.2.

In Figure 2, we report the perplexity on the held-out set across various levels of sparsity and training context lengths. Extending the training context length and increasing the context size generally benefit all types of attention mechanisms. When matching the KV size, our $\mathrm { S P A R S E K + S W }$ attention consistently outperforms sliding window attention. For the TinyLlama models, SPARSEK $. + \mathbf { S } \mathbf { W }$ attention achieves comparable perplexity using only half the KV size required by sliding window attention. These results underscore the advantages of a more adaptable context as implemented in SPARSE $\zeta + \mathrm { S W } .$ . We further evaluate TinyLlama 1.1B, fine-tuned with an $^ \mathrm { 8 k }$ context window, across additional tasks as presented in the following sections.

# 4.3 Retrieval-based Evaluation and Length Extrapolation

A common concern with sparse attention is its potential to neglect informative history. To investigate this, we evaluated our fine-tuned models on the passkey retrieval task [47], along with two baseline methods that require no training: dynamic NTK [7, 23] and LM-Infinite [33]. The results are presented in Figure 3a. It is evident that the sliding window approach fails even within the trained context length. Furthermore, among the training-free methods, NTK utilizes full attention and extends the context length by a factor of four, whereas the memory-efficient method LM-Infinite fails in extrapolation. In contrast, $\mathbf { S P A R S E K + S W }$ is memory-efficient while maintaining performance for context lengths well beyond four times longer.

We also analyze the perplexity of tokens in various positional buckets within a long context, as depicted in Figure 3b. In the language modeling task, SW demonstrates the ability to effectively manage contexts four times longer than standard models, although it is less competitive in relatively short contexts. While SPARSEK $+ \mathbf { S } \mathbf { W }$ fails at contexts extending to 26k tokens, it outperforms both NTK and fine-tuned full attention models.

![](images/743d9ea3b8e2879cd490b62905f2cba26526d2e427db080948b0982fcf171d29.jpg)  
Figure 3: Length extrapolation results. \* denotes that the method is training-free. 2,048 is the context length of the original model. 8,192 is the context length in fune-tuning.

# 4.4 Downstream Task

We evaluated our method on the English subsets of LongBench [3] using the OpenCompass package [16], which encompasses a wide range of long-context downstream tasks. The choice of language is based on the fact that the training corpus of TinyLlama is primarily in English. We test all models using greedy decoding, with the evaluation context size set to 8192.

All results are presented in Table 2. Full attention offers the best performance but incurs the highest memory cost. Sliding window attention is memory-efficient; however, it results in significant performance degradation. In contrast, our SPARSEK $\mathrm { _ { + S W } }$ attention not only demonstrates strong performance but also achieves high memory efficiency. Notably, $\mathbf { S P A R S E K + S W }$ outperforms the training-free method, NTK, and the inference-time KV cache compression method, H2O [81]. This suggests the benefits of maintaining consistency between training and inference. However, SPARSEK ${ \bf \nabla } _ { + \mathrm { { S W } } }$ underperforms fine-tuned full attention, representing a trade-off between efficiency and performance.

Table 2: Results on LongBench. ∗ denotes that the method is training-free. † We use 512 globel (heavy-hitter) KV cache and 512 local KV cache in H2O.   

<table><tr><td rowspan="2">Model</td><td colspan="2">Single-Doc QA</td><td colspan="2">Multi-Doc QA</td><td colspan="2">Summarziation</td><td colspan="2"></td><td colspan="2">Few-shot Learning</td><td colspan="2">Synthetic</td><td rowspan="2">Code Avg.</td></tr><tr><td></td><td>NQA Qspr MulFi 1</td><td></td><td> HQA WMQA Musq</td><td></td><td>GRpt QMSM MulN &#x27;</td><td></td><td></td><td>TREC TriQA SMSM</td><td> PsgC PsgR</td><td>LCC</td><td>Repo</td></tr><tr><td>NTK* w = 8192</td><td>4.3410.3014.54</td><td></td><td>6.49</td><td>9.19</td><td>3.49</td><td>11.77</td><td>7.84</td><td>3.62 49.5</td><td>55.17</td><td>22.66</td><td>1.21</td><td>3.3852.19 48.90 19.04</td><td></td></tr><tr><td>Full w = 8192</td><td>3.95 13.0713.16</td><td></td><td>6.81</td><td>10.77</td><td>3.51</td><td>15.17</td><td>6.12 8.30</td><td>61.00</td><td>65.15</td><td>26.02 0.39</td><td></td><td>2.3756.72 50.36 21.42</td><td></td></tr><tr><td>Full H2Ot</td><td>7.66</td><td>9.33 13.73</td><td>6.36</td><td>10.23</td><td>3.26</td><td>12.10</td><td>7.00</td><td>0.87 51.00</td><td>54.92</td><td>18.31</td><td>2.39</td><td>2.6241.66 43.24 17.79</td><td></td></tr><tr><td>SW w = 1024</td><td>1.34</td><td>8.69 5.41</td><td>2.76</td><td>4.46</td><td>0.48</td><td>11.78</td><td>4.25</td><td>2.39 25.50</td><td>13.43</td><td>5.33</td><td>2.3 0.50</td><td>52.22 27.50 10.52</td><td></td></tr><tr><td>SparseK+SW k = ν = 512|i2</td><td>5.19</td><td>14.29 13.24</td><td>6.85</td><td>9.21</td><td>3.83</td><td>14.11</td><td>5.97</td><td>5.85 55.00</td><td>52.06</td><td>24.79 0.61</td><td></td><td>2.6153.90 50.89 19.90</td><td></td></tr></table>

# 5 Conclusion

We propose SPARSEK attention, a new approach to sparse attention that achieves both computational and memory efficiency. Within self-attention, we use an additional scoring network evaluating the importance of each key-value pair and select the top- $k$ pairs. We propose the differentiable SPARSEK operator, a relaxation of TOPK, to enable gradient-based optimization. Experiments on language modeling and downstream tasks demonstrate consistent improvements compared to previous efficient attention methods.

References   
[1] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontan’on, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit K. Sanghai. Colt5: Faster long-range transformers with conditional computation. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:257622671.   
[2] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurélien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. ArXiv, abs/2305.15805, 2023. URL https://api.semanticscholar.org/CorpusID:258888224.   
[3] Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508, 2023. URL https://api.semanticscholar. org/CorpusID:261245264.   
[4] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. ArXiv, abs/2004.05150, 2020. URL https://api.semanticscholar.org/CorpusID:215737171.   
[5] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. ArXiv, abs/1308.3432, 2013. URL https://api. semanticscholar.org/CorpusID:18406556.   
[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397–2430. PMLR, 2023.   
[7] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended $( 8 \mathbf { k } + )$ context size without any fine-tuning and minimal perplexity degradation., 2023. URL https://www.reddit.com/r/ LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/.   
[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.   
[10] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Chris Ré. Scatterbrain: Unifying sparse and low-rank attention approximation. ArXiv, abs/2110.15343, 2021. URL https://api.semanticscholar. org/CorpusID:248498407.   
[11] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. ArXiv, abs/2309.12307, 2023. URL https: //api.semanticscholar.org/CorpusID:262084134.   
[12] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. ArXiv, abs/2305.14788, 2023. URL https://api.semanticscholar.org/CorpusID: 258865249.   
[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. ArXiv, abs/1904.10509, 2019. URL https://api.semanticscholar.org/CorpusID: 129945531.   
[14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. ArXiv, abs/2009.14794, 2020. URL https: //api.semanticscholar.org/CorpusID:222067132.   
[15] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does bert look at? an analysis of bert’s attention. In BlackboxNLP@ACL, 2019. URL https://api.semanticscholar.org/ CorpusID:184486746.   
[16] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023.   
[17] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ArXiv, abs/1901.02860, 2019. URL https://api.semanticscholar.org/CorpusID:57759363.   
[18] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. ArXiv, abs/2006.03236, 2020. URL https: //api.semanticscholar.org/CorpusID:219401850.   
[19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023. URL https://api.semanticscholar.org/CorpusID:259936734.   
[20] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R’e. Flashattention: Fast and memory-efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022. URL https://api. semanticscholar.org/CorpusID:249151871.   
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   
[22] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. ArXiv, abs/2307.02486, 2023. URL https: //api.semanticscholar.org/CorpusID:259341682.   
[23] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA with zero finetuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_ scaled_rope_further_increases/.   
[24] Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.   
[25] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. ArXiv, abs/2402.10171, 2024. URL https: //api.semanticscholar.org/CorpusID:267682361.   
[26] Leo Gao, Jonathan Tow, Baber Abbasi, et al. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.   
[27] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. ArXiv, abs/2310.01801, 2023. URL https: //api.semanticscholar.org/CorpusID:263609075.   
[28] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   
[29] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv, abs/2312.00752, 2023. URL https://api.semanticscholar.org/CorpusID:265551773.   
[30] Albert Gu, Karan Goel, and Christopher R’e. Efficiently modeling long sequences with structured state spaces. ArXiv, abs/2111.00396, 2021. URL https://api.semanticscholar.org/CorpusID: 240354066.   
[31] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range pre-trained language model for code completion. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:259262301.   
[32] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. In SUSTAINLP, 2021. URL https://api.semanticscholar.org/CorpusID: 235422257.   
[33] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.   
[34] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. ArXiv, abs/1912.12180, 2019. URL https://api.semanticscholar.org/CorpusID: 209323787.   
[35] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar.org/CorpusID: 247011581.

[36] DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. ArXiv, abs/2203.07852, 2022. URL https://api.semanticscholar.org/CorpusID: 247451135.

[37] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:263830494.

[38] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:220250819.

[39] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ArXiv, abs/2001.04451, 2020. URL https://api.semanticscholar.org/CorpusID:209315300.

[41] Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-Wei Chang. Conditional adapters: Parameter-efficient transfer learning with fast inference. ArXiv, abs/2304.04947, 2023. URL https://api.semanticscholar. org/CorpusID:258060039.

[42] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. URL https://api.semanticscholar. org/CorpusID:258947558.

[43] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017. URL https://api.semanticscholar.org/CorpusID:3312944.

[44] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length, 2024.

[45] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. ArXiv, abs/1709.04057, 2017. URL https://api.semanticscholar.org/CorpusID:3497822.

[46] André F. T. Martins and Ramón Fernández Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. ArXiv, abs/1602.02068, 2016. URL https://api.semanticscholar. org/CorpusID:16432551.

[47] Amirkeivan Mohtashami and Martin Jaggi. Landmark Attention: Random-Access Infinite Context Length for Transformers. ArXiv, abs/2305.16300, 2023. URL https://api.semanticscholar.org/ CorpusID:258887482.

[48] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. ArXiv, abs/2404.07143, 2024. URL https://api. semanticscholar.org/CorpusID:269033427.

[49] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. ArXiv, abs/2303.06349, 2023. URL https://api.semanticscholar.org/CorpusID:257496654.

[50] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Franccois Fleuret. Faster causal attention over large sequences through sparse flash attention. ArXiv, abs/2306.01160, 2023. URL https://api. semanticscholar.org/CorpusID:259063695.

[51] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, 2018. URL https: //api.semanticscholar.org/CorpusID:3353110.

[52] Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand ´ Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan Sokrates Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui Zhu. Rwkv: Reinventing rnns for the transformer era. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:258832459.   
[53] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. ArXiv, abs/2103.02143, 2021. URL https://api.semanticscholar.org/ CorpusID:232105052.   
[54] Ben Peters, Vlad Niculae, and André F. T. Martins. Sparse sequence-to-sequence models. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504–1519, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1146. URL https://aclanthology.org/P19-1146.   
[55] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. Transnormerllm: A faster and better large language model with improved transnormer, 2024.   
[56] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. ArXiv, abs/1911.02972, 2019. URL https://api.semanticscholar. org/CorpusID:207847640.   
[57] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   
[58] Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524–7529, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.672. URL https://aclanthology.org/2020.acl-main.672.   
[59] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2019. URL https://api. semanticscholar.org/CorpusID:207930593.   
[60] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. ArXiv, abs/2107.05768, 2021. URL https://api.semanticscholar.org/CorpusID:235829099.   
[61] Aurko Roy, Mohammad Taghi Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9: 53–68, 2020. URL https://api.semanticscholar.org/CorpusID:212718077.   
[62] Jimmy Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling. ArXiv, abs/2208.04933, 2022. URL https://api.semanticscholar.org/CorpusID: 251442769.   
[63] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 6 2023.   
[64] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. ArXiv, abs/2104.09864, 2021. URL https://api.semanticscholar.org/ CorpusID:233307138.   
[65] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. ArXiv, abs/1905.07799, 2019. URL https://api.semanticscholar.org/CorpusID: 159041867.   
[66] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? ArXiv, abs/2109.09115, 2021. URL https://api.semanticscholar. org/CorpusID:237572264.   
[67] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv, abs/2307.08621, 2023. URL https://api.semanticscholar.org/CorpusID:259937453.   
[68] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/ CorpusID:211505992.   
[69] OpenAI Team. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
[70] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023. URL https://api.semanticscholar.org/CorpusID:257219404.   
[71] Constantino Tsallis. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52(1-2):479–487, July 1988. ISSN 0022-4715, 1572-9613. doi: 10.1007/BF01016429. URL http: //link.springer.com/10.1007/BF01016429.   
[72] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489.   
[73] Apoorv Vyas, Angelos Katharopoulos, and Franccois Fleuret. Fast transformers with clustered attention. ArXiv, abs/2007.04825, 2020. URL https://api.semanticscholar.org/CorpusID:220424511.   
[74] Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. Cluster-former: Clustering-based sparse transformer for long-range dependency encoding. ArXiv, abs/2009.06097, 2020. URL https://api.semanticscholar.org/CorpusID:260424300.   
[75] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. ArXiv, abs/2006.04768, 2020. URL https://api.semanticscholar.org/CorpusID: 219530577.   
[76] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.   
[77] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. ArXiv, abs/2312.06635, 2023. URL https://api. semanticscholar.org/CorpusID:266162792.   
[78] Haofei Yu, Cunxiang Wang, Yue Zhang, and Wei Bi. Trams: Training-free memory selection for long-range language modeling. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:264439578.   
[79] Michael Zhang, Kush S. Bhatia, Hermann Kumbong, and Christopher R’e. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. ArXiv, abs/2402.04347, 2024. URL https://api. semanticscholar.org/CorpusID:267523164.   
[80] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024.   
[81] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. ArXiv, abs/2306.14048, 2023. URL https://api.semanticscholar.org/CorpusID:259263947.   
[82] Zexuan Zhong, Mengzhou Xia, Danqi Chen, and Mike Lewis. Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training. 2024. URL https://api.semanticscholar.org/ CorpusID:268891288.   
[83] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.

# A Derivation of SparseK

# A.1 The solution of SparseK

Proposition A.1. The solution of ${ \mathrm { 3 P A R S E K } } : = { \mathrm { \ a r g \operatorname* { m i n } } } _ { p \in \mathbb { C } } | | p - z | | ^ { 2 }$ is in the form of $\pmb { p } ^ { * } =$ $\operatorname* { m a x } ( \operatorname* { m i n } ( z - \tau ( z ) , 1 ) , \mathbf { 0 } )$ , where $\mathbb { C } = \{ p | \mathbf { 0 } \leq p \leq \mathbf { 1 } , \mathbf { 1 } ^ { \top } p = k \}$ . Define $F ^ { * } ( z ) = \{ j | p _ { j } ^ { * } = 1 \}$ and $S ^ { * } ( z ) = \{ j | 0 < p _ { j } ^ { * } < 1 \}$ . Then,

$$
\tau ( z ) = \frac { \sum _ { j \in S ^ { * } ( z ) } z _ { j } + | F ^ { * } ( z ) | - k } { | S ^ { * } ( z ) | }
$$

Proof. Consider the Lagrangian problem of SPARSEK:

$$
\mathcal { L } ( z , \mu , \nu , \tau ) = \frac { 1 } { 2 } | | p - z | | ^ { 2 } - \mu ^ { \top } p + \nu ^ { \top } ( p - \bf { 1 } ) + \tau ( \bf { 1 } ^ { \top } p - \bf { k } )
$$

The optimal $( p ^ { * } , \mu ^ { * } , \nu ^ { * } , \tau ^ { * } )$ must satisfy the following Karush-Kuhn-Tucker conditions:

$$
\begin{array} { r l r } & { } & { \pmb { p } ^ { * } - \pmb { z } - \pmb { \mu } ^ { * } + \pmb { \nu } ^ { * } + \tau ^ { * } \mathbf { 1 } = \mathbf { 0 } , } \\ & { } & { \mathbf { 0 } \le p ^ { * } \le \mathbf { 1 } , \quad \mathbf { 1 } ^ { \top } \pmb { p } ^ { * } = k , \quad \pmb { \mu } ^ { * } \geq \mathbf { 0 } , \quad \pmb { \nu } ^ { * } \geq \mathbf { 0 } , } \\ & { } & { \mu _ { i } ^ { * } p _ { i } ^ { * } = 0 , \quad v _ { i } ^ { * } ( p _ { i } ^ { * } - 1 ) = 0 , \quad \forall i \in [ K ] . } \end{array}
$$

Case I If for $i \in [ K ]$ we have $0 < p _ { i } ^ { * } < 1$ , then from (19) we must have $\mu _ { i } ^ { * } = 0$ and $\nu _ { i } ^ { * } = 0$ , which from (17) we must have $p _ { i } ^ { * } = z _ { i } - \tau ^ { * }$ , i.e., $z _ { i } - 1 < \tau ^ { * }$ .

Case II If for $i \in [ K ]$ we have $\mu _ { i } ^ { * } > 0$ , then from (19) we must have $p _ { i } ^ { * } = 0$ and $\nu _ { i } ^ { * } = 0$ , which from (17) we must have $\mu _ { i } ^ { * } = \tau ^ { * } - z _ { i } > 0$ , i.e., $z _ { i } \leq \tau ^ { * }$ .

Case III If for $i \in [ K ]$ we have $\nu _ { i } ^ { * } > 0$ , then from (19) we must have $p _ { i } ^ { * } = 1$ and $\mu _ { i } ^ { * } = 0$ , which from (17) we must have $\nu _ { i } ^ { * } = z _ { i } - 1 - \tau ^ { * } > 0$ , i.e., $z _ { i } - 1 \ge \tau ^ { * }$ .

From Case I, II, III and (18) we obtain

$$
\begin{array} { l } { { \displaystyle p ^ { * } = \operatorname* { m a x } ( \operatorname* { m i n } ( z - \tau ^ { * } , 1 ) , 0 ) , } } \\ { { \mathrm { a n d ~ } \displaystyle \sum _ { j \in S ( z ) } ( z _ { j } - \tau ^ { * } ) + | F ( z ) | = k . } } \end{array}
$$

Rearrange (21) then we will get (15).

Let $z _ { ( 1 ) } > z _ { ( 2 ) } > \cdot \cdot \cdot > z _ { ( m ) }$ be the sorted coordinates of $_ z$ . We can define $u ^ { * } = | F ^ { * } ( z ) |$ and $w ^ { \ast } = | S ^ { \ast } ( z ) | + | F ^ { \ast } ( z ) |$ , so we have

$$
z _ { ( u ^ { * } ) } \geq \tau ( z ) + 1 > z _ { ( u ^ { * } + 1 ) } z _ { ( w ^ { * } ) } > \tau ( z ) \geq z _ { ( w ^ { * } + 1 ) }
$$

Consequently, (15) can be rewritten as

$$
\tau ( z ) = \frac { \sum _ { u ^ { * } < j \leq w ^ { * } } z _ { ( j ) } + u ^ { * } - k } { w ^ { * } - u ^ { * } } ,
$$

# A.2 Get Algorithm 5

The exact solution of (23) can be evaluated by searching $( u , w )$ in the descending order that satisfies (22). The descending order is given by the step functions with respect to an incremental threshold $z _ { ( m ) } - 1 < \beta \leq z _ { ( 1 ) }$ ,

$$
u ( \beta ) = \operatorname* { m a x } \{ j | z _ { ( j ) } \geq \beta + 1 \} \qquad w ( \beta ) = \operatorname* { m a x } \{ j | z _ { ( j ) } > \beta \} .
$$

Note that there are at most $2 m$ distinct $( u , w )$ pairs, corresponding to the values of $\beta$ that may trigger change of either $u ^ { * }$ or $w ^ { * }$ , i.e., $\{ z _ { 1 } , \dotsc , z _ { m } \} \cup \{ z _ { 1 } - 1 , \dotsc , z _ { m } - 1 \}$ .

# A.3 Gradient

The SPARSEK operator is differentiable everywhere except at splitting points causing changes to set $S ^ { * } ( z )$ or $F ^ { * } ( z )$ . Note that we have $j \in S ^ { * } ( z ) \Leftrightarrow \tau ( z ) < z _ { j } < \tau ( z ) + 1$ . Then, from Equation (12) and Equation (15), we have

$$
\begin{array} { r } { \frac { \partial \mathrm { S P A R S E K } _ { i } ( z ) } { \partial z _ { j } } = \left\{ \begin{array} { l l } { \displaystyle \delta _ { i j } - \frac { 1 } { | S ^ { * } ( z ) | } } & { \mathrm { i f ~ } z _ { i } \in S ^ { * } ( z ) , } \\ { 0 } & { \mathrm { o t h e r w i s e , } } \end{array} \right. } \end{array}
$$

where $\delta _ { i j } = 1$ if $i = j$ and 0 otherwise. Then, the Jacobian matrix $\pmb { J } ( z )$ and Jacobian-vector product (JVP) for a given vector $\textbf {  { v } }$ are given by

$$
\begin{array} { r l } & { J ( z ) = \mathrm { D i a g } ( s ) - s s ^ { \top } / | S ^ { * } ( z ) | } \\ & { J ( z ) \cdot v = s \odot ( v - \hat { v } \mathbf { 1 } ) , \mathrm { w i t h } \ \hat { v } = \frac { \sum _ { j \in S ^ { * } ( z ) } v _ { j } } { | S ^ { * } ( z ) | } } \end{array}
$$

where $\pmb { s }$ is an indicator vector taking 1 if $i \in S ^ { * } ( z )$ and 0 otherwise, and $\odot$ is the Hadamard product. Note that SPARSEK has the same form of gradients, Jacobian and jacobian-vector product (JVP) as SPARSEMAX [46] but with a different definition of $S ^ { * } ( z )$ .

# B Other Technical Details

# B.1 Combine SparseK with Low-rank Linear Attention

For simplicity, we consider the attention of one query, $\pmb q _ { i }$ . In Chen et al. [10], each attendable position $j$ $( j \leq i )$ is attended to via either sparse attention or linear attention. This hard strategy resembles top- $k$ attention. In our SPARSEK attention, we employ soft gating, denoted as $m ^ { s p a r s e \tilde { k } }$ . Consequently, the combination of SPARSEK and linear attention forms an interpolation between them, modulated by msparsek:

$$
o _ { i } = \sum _ { j } \frac { \left( m _ { s p a r s e k , j } \exp { ( q _ { i } ^ { \top } k _ { j } ) } + ( 1 - m _ { s p a r s e k , j } ) \phi ( { q _ { i } } ) ^ { \top } \phi ( { k _ { j ^ { \prime } } } ) \right) v _ { j } } { \sum _ { j ^ { \prime } } m _ { s p a r s e k , j } \exp ( q _ { i } ^ { \top } k _ { j ^ { \prime } } ) + ( 1 - m _ { s p a r s e k , j } ) \phi ( { q _ { i } } ) ^ { \top } \phi ( { k _ { j ^ { \prime } } } ) }
$$

We can use this notation to express other kinds of combination: (i) Linea $+ \mathbf { S } \mathbf { W }$ means to use a local gating $_ { m _ { l o c a l } }$ where $\pmb { m } _ { l o c a l , j } = 1$ if $i - j \le w$ and 0 otherwise. (ii) SparseK+Linear $+ \mathbf { S } \mathbf { W }$ means using a concatenation of $_ { m _ { l o c a l } }$ and $m _ { s p a r s e k }$ as mentioned in Section 3.4.

In this work, we adopt the feature map $\phi ( \pmb { x } ) = \mathrm { e l u } ( \mathrm { h e a d - w i s e - l i n e a r } ( \pmb { x } ) ) + 1$ based on the methodology proposed by Katharopoulos et al. [38]. The additional head-wise linear transformation is introduced to mitigate the dilemma of $W _ { q }$ and $W _ { k }$ having to perform two types of attention. This adjustment introduces less than $1 \%$ additional parameters. However, using linear attention can introduce significant overhead during training because head-wise-linear $\mathbf { \rho } ( { \pmb x } )$ consumes substantial memory that needs to be stored in the computation graph. In contrast, the combination of SPARSEK and sliding window (SW) techniques introduces minimal overhead since the same $Q , K$ , and $V$ are employed in both kinds of attention.

In our early experiments, we also considered the more recent Hedgehog feature map [79]. We observed slightly better performance compared to the aforementioned feature map. However, the Hedgehog feature map requires significantly more computational resources due to doubling the hidden dimension. While it is possible to fuse the feature map and linear attention within a single kernel, we leave this for future work.

Recently, Infini-attention [48] was proposed to integrate local attention and linear attention through an input-independent gating mechanism. We contend that our methods exhibit a superior expressive capability because our gating mechanism is data-dependent.

The numerical explosion issue Without normalization, we have observed that the gate $( 1 \textrm { -- }$ $\mathbf { m } _ { s p a r s e k , j } )$ is trained to put all credits away from linear attention part $\phi ( \mathbf { q } _ { i } ) ^ { \top } \phi ( \mathbf { k } _ { j } )$ (i.e., set $( 1 - \mathbf { m } _ { s p a r s e k , j } )$ to zero) because standard dot-product attention is generally better than linear attention, and in contrast, the linear attention part is trained to increase $\phi ( \mathbf { q } _ { i } ) ^ { \top } \phi ( \mathbf { k } _ { j } )$ significantly to survive. The score normalization breaks the competition and makes the training feasible.

# B.2 Efficient GPU Kernel of SparseK Attention

In this section, we discuss optimizations for obtaining practically efficient sparse attention. We discuss a few challenges here and present our solution briefly. We measure time and memory cost on one NVIDIA A6000 GPU when $n = 8 1 9 2 , k = 1 0 2 4 , h = 4 , d = 6 4$ , and $G = 1 2 8$ to show the effectiveness of each optimization.

High memory cost A naive implementation is to select contexts before doing attention. However, each selection produces $\hat { K } _ { i } , \hat { V _ { i } } \in \mathbb { R } ^ { k \times d }$ , resulting in a memory cost of $2 n k d$ for all selections. The cost is significant considering a reasonable size of selected contexts, such as $k = 1 0 0$ .

Solution We implement a fused Triton kernel to load needed key-value pairs on demand within FlashAttention [20]. By doing so, the memory cost is reduced from 8192 MB to 4.125 MB.

Underuse of hardware Our method requires $n$ matrix-vector multiplications (MVM) of $q _ { i } \hat { K } _ { i }$ instead of one matrix-matrix multiplication (MMM) of $Q K ^ { \top }$ in pure self-attention. This fails to capitalize on the hardware-optimized matrix-matrix multiplication in modern GPUs.

Solution We group $G$ successive queries to do attention jointly. This is efficient because the majority of selections remain unchanged in successive steps. With grouping, $n$ MVM are transformed into $\textstyle { \frac { n } { G } }$ MMM and hence hardware can be fully utilized. By doing so, the running time is reduced from ${ \stackrel {  } { 3 } } . 0 9 ~ \mathrm { m s }$ to 195 us, where about $2 \mathrm { m s }$ is due to the faster matrix-matrix multiplications, and the reset is credited to less IO.

IO-inefficiency Storing and using selection scores $m _ { s p a r s e k }$ has an IO complexity of $O ( n k )$ Besides, As SPARSEMAX, jacobian-vector product (JVP) computation of SPARSEK requires multiple read/write operations.

Solution Our approach involves storing only the unnormalized scores $\mathbf { \boldsymbol { u } } \in \mathbb { R } ^ { n }$ and the threshold $\tau \in \mathbb { R } ^ { n }$ . Then, $m _ { s p a r s e k }$ is computed dynamically as needed, akin to the re-computation technique in FlashAttention, which trades slow IO with fast computation. Regarding gradients, we extend the backward kernel of FlashAttention to include IO-aware JVP computation. Furthermore, we introduce additional IO optimizations for grouped queries, achieved by reducing intermediate results in a group instead of using a computing-then-reducing pipeline. By doing so, the running time is reduced from 12.07 ms to $1 . 4 6 \mathrm { m s }$ and the memory cost of backward pass is reduced from $1 9 2 . 1 3 \mathrm { M B }$ to $1 3 . 5 3 \mathrm { M B }$ .

# B.3 Practical Implementation of Algorithm 2

Algorithm 2 scans the sequence of scores $\textbf { \em u }$ and incorporates heaps, making it unsuitable for GPUs. Therefore, we implement the algorithm on the CPU, where it performs efficiently, even with millions of input elements. However, this raises the concern that frequent data transfers between the CPU and GPU might degrade efficiency. To address this, we leverage the asynchronous execution property of GPU kernels to conceal the data transfer overhead behind other large GPU kernels.

# C Additional Experimental Results

# C.1 Language Modeling from Scratch

For all methods, we use the AdamW optimizer [43] with a learning rate of $6 \times 1 0 ^ { - 4 }$ and a weight decay of 0.1. We apply gradient clipping with a threshold of 1.0. Our learning rate follows a cosine schedule, incorporating 100 warm-up steps from an initial learning rate of $1 \times 1 0 ^ { - 6 }$ . The batch size is set to 512K tokens.

We utilize only those documents whose lengths are equal to or exceed the training context length, ensuring that no data packing is applied. This approach simplifies data preprocessing because the incremental selection of SPARSEK on two unrelated sentences is not meaningful. There are more sophisticated strategies available, such as the similarity-based batching [82], which we plan to explore in future work. For evaluation, we compute perplexity on a held-out set using a sliding window approach without overlap.

For GLA, we adjusted the settings of its 340M architecture as follows: the hidden dimension was decreased from 1024 to 768, the number of layers was reduced from 24 to 12, and the number of heads was decreased from 4 to 2. For RetNet, the modifications were similar to those of the GLA, except that the number of heads was reduced to 3. For GLA and RetNet, we use the same training hyperparameters (such as learning rate scheduler and weight decay) as in our experiments of Transformers rather than the official configuration [77]. We found that our configuration leads to slightly better results. For Hash attention, we normalize $\kappa$ as suggested in Pagliardini et al. [50]. Table 3 lists all used sparsity configurations, and figure 4 plots perplexity achieved by each model relative to the average (or maximum, if applicable) number of attended KV pairs per query.

We use the flash-linear-attention package [76] for our linear attention related experiments, including GLA, RetNet, Linea $+ \mathrm { S W }$ and SparseK+Linea $+ \mathbf { S } \mathbf { W } .$ . We use the xformers [40] package for our Fixed and BigBird experiments. We use the open-sourced code of Pagliardini et al. [50] for our Hash and Random experiments.

Table 3: Sparsity configurations used to produce Table 1 and Figure 4. # denotes the number of.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Hyperparameter</td><td colspan="3">Training Context Length</td></tr><tr><td>1024</td><td>4096</td><td>8192</td></tr><tr><td>SW and Full</td><td>window size</td><td>256,512</td><td>256,512, 1024</td><td>256,512, 1024</td></tr><tr><td>SparseK+SW</td><td>window size / k |</td><td colspan="3">128/128, 256/256, 512/512</td></tr><tr><td>Fixed</td><td># local/global</td><td>12/4, 24/8</td><td>24/1,2 ,24/2, 2418</td><td>24/8</td></tr><tr><td>BigBird</td><td># global/random/local |</td><td>2/4/12, 4/4/24</td><td>2/4/12, 4/4/24, 8/8/24</td><td></td></tr><tr><td>Hash</td><td>num clusters</td><td>2,4</td><td>8,16</td><td>8,16</td></tr></table>

![](images/26329436eaf2a68d38d89853ac233eecaf5aaf413620d013b4499843443dc38f.jpg)  
Figure 4: Training from scratch.

# C.2 Fine-tuning Existing Models

For all methods, we use the AdamW optimizer with a weight decay of 0.1. We apply gradient clipping with a threshold of 1.0. Our learning rate follows a cosine schedule, incorporating 100 warm-up steps from an initial learning rate of $1 0 \%$ of the peak learning rate, which is $\bar { 3 } \times 1 0 ^ { - 4 }$ for Pythia 160M, $1 . 5 \times 1 0 ^ { - 4 }$ for Pythia 410M and $1 \times 1 0 ^ { - 4 }$ for TinyLlama 1.1B. The batch size is set to 1M tokens.

For Pythia models, we use packing to generate training samples of the target length from a uniformly sampled subset of SlimPajama. For evaluation, we compute the perplexity on a held-out set using a sliding window approach with a step size of 512. For TinyLlama models, we utilize only those documents whose lengths are equal to or exceed the training context length in the up-sampled SlimPajama dataset $\mathrm { F u }$ et al. [25]. For evaluation, we compute perplexity on a held-out set using a sliding window approach without overlap.

# C.3 More Downstream Task Evaluation on Fine-tuned Models

We present the results of tasks that do not require long-context modeling in Table 4. These results were generated using the lm-eval-harness package [26]. Although SPARSEK ${ . + } \mathrm { S W }$ still outperforms $S W$ , a comparison with the original model indicates that fine-tuning on long data somewhat diminishes performance on short data [25]. This trade-off has been observed in other studies, prompting further research to address this issue.

Table 4: Results evaluated using lm-eval-harness.   

<table><tr><td>Model</td><td>Wiki. pp1↓</td><td>LMB. ppl↓</td><td>LMB. acc↑</td><td>PIQA acc↑</td><td>Hella. acc_norm ↑</td><td>Wino. acc↑</td><td>ARC-e acc↑</td><td>ARC-c acc_norm ↑</td><td>CoPA acc↑</td><td>OBQA acc_norm ↑</td><td>SciQA acc↑</td><td>BoolQA acc↑</td></tr><tr><td>Original</td><td>14.70</td><td>7.11</td><td>57.58</td><td>73.12</td><td>58.99</td><td>58.88</td><td>61.36</td><td>31.83</td><td>74.00</td><td>34.40</td><td>88.70</td><td>62.97</td></tr><tr><td>SW</td><td>14.57</td><td>7.23</td><td>58.16</td><td>73.07</td><td>57.66</td><td>58.41</td><td>60.14</td><td>29.86</td><td>73.00</td><td>35.40</td><td>87.60</td><td>61.28</td></tr><tr><td>SparseK+SW</td><td>14.02</td><td>7.40</td><td>58.33</td><td>72.58</td><td>57.52</td><td>59.27</td><td>59.01</td><td>29.35</td><td>77.00</td><td>34.60</td><td>87.90</td><td>61.83</td></tr></table>

# C.4 Compare with Memory-efficient Attention Method

We compare our method with Anagnostidis et al. [2] in Table 5, which uses a pairwise criterion to prune the KV cache based on the current query. Following Anagnostidis et al. [2], we use the GPT2 small architecture and train models with Wikipedia 20220301.en [24] and BookCorpus [83] datasets. DCP achieves a slightly better perplexity than ours, matching the stronger expressiveness of their method than our query-independent selection. However, their method costs a much longer training time due to the heavy pairwise criterion, making it hard to adopt in training on long documents. Meanwhile, our method remains efficient in this short context length experiments and achieves similar performance gains.

Table 5: Perplexity evaluation on the Wikipedia 20220301.en and BookCorpus held-out set. DCP: [2]. SW: sliding window attention. The training context length is 1024. We set $\gamma = 0 . 0 3$ to DCP, $w = 2 5 6$ to SW, $k = w = 1 2 8$ to our $\mathrm { S P A R S E K + S W } .$ This results in a similar sparsity among these models. We also report the training time relative to full attention.   

<table><tr><td></td><td>DCP</td><td>sW</td><td>SPARSEK+SW</td></tr><tr><td>PPL</td><td>17.91</td><td>18.36</td><td>18.06</td></tr><tr><td>Relative Training Time</td><td>264%</td><td>97%</td><td>98%</td></tr></table>

# C.5 Ablation Study

Previously, we choose SPARSEK ${ . + } \mathrm { S W }$ as the default model. In this section, we conduct ablation study to investigate the performance of SPARSEK-only model and effectiveness of learnable selection. We use the Pythia-160M architecture. The context length is 8192. All models are set to have a sparsity ratio similar to SW with a window size of 1024. We randomly initialize all parameters. All models are trained with 5000 steps.

Table 6 shows the results. We first study attention types. SparseK-only outperforms sliding window attention. A combination of these two attention leads to better performance. Then, we study how to select. We consider two baselines: random selection and unlearnable selection which is parameterized as in Section 3.5 initialization to mimic attention score. The two baseline obviously underperforms SparseK-only, validating the effectiveness of our learnable selection.

Table 6: Ablation study   

<table><tr><td>Model</td><td>SPARSEK+SW</td><td>SPARsEK-only</td><td>SPARsEK-only random</td><td>SPARsEK-only unlearnable</td><td>SW</td></tr><tr><td>PPL w/ slope</td><td>27.38</td><td>27.91</td><td>33.72</td><td>36.20</td><td rowspan="2">28.12</td></tr><tr><td>PPL w/o slope</td><td>27.41</td><td>40.48</td><td>54.43</td><td>38.55</td></tr></table>

# C.6 Balance Between Selection Size and Sliding Window Size

In Table 7, we present a comparison of various ratios of $k$ and $w$ . Pythia 410M is utilized as the base model, with a training context length of 16,384. The results indicate an optimal performance at approximately $k = w = 1 0 2 4$ . Consequently, we set $k = w$ in all subsequent experiments for consistency and simplicity.

Table 7: Comparison on different ratios of $k$ and $w$ in SPARSEK $\mathbf { \partial } _ { + \mathrm { { S W } } }$   

<table><tr><td>k W</td><td>0 2048</td><td>128 1920</td><td>256 1792</td><td>512 1536</td><td>768 1280</td><td>1024 1024</td><td>1280 768</td><td>1536 512</td></tr><tr><td>PPL</td><td>13.73</td><td>13.65</td><td>13.64</td><td>13.62</td><td>13.59</td><td>13.59</td><td>13.61</td><td>13.61</td></tr></table>

# C.7 Multi-group Selection

It is straightforward to use head-specific selection or grouped selection; the only modification required is to employ separate scoring networks for each head or group. In Table 8, we compare the performance of multi-group selection against single-group selection. We use Pythia 410M as the base model. The training context length is 4096. $w$ is set to 256. When utilizing two groups with $k = 2 5 6$ , the worst-case memory budget amounts to $2 5 6 \times 2 = 5 1 2$ . Our findings indicate that even with a $50 \%$ increase in the budget, amounting to 768 (where $k = 3 8 4$ ), the multi-group selection fails to achieve the performance of single-group attention. Consequently, we have opted to use single-group selection in our subsequent experiments.

Table 8: Study multi-group selection.   

<table><tr><td>Num of groups k</td><td>1 512</td><td>2 256</td><td>2 384</td></tr><tr><td>PPL</td><td>13.63</td><td>13.74</td><td>13.69</td></tr></table>

# C.8 Speed Benchmark

Figure 5 presents an sole comparison of the wall-clock time between our developed Triton kernel (with $k = 5 1 2$ and $w = 5 1 2$ ) and the Triton implementation of FlashAttention-2 [19], across different input lengths. While our kernel initially shows slower performance than FlashAttention-2 on short inputs due to overhead associated with selecting contexts, it outpaces FlashAttention-2 as input lengths surpass 4096 and 8192 for forward only and forward+backward, respectively. This performance gain is attributed to our method’s linear complexity. Additionally, using a straight-through estimator can reduce the IO operations required for reading $m _ { s p a r s e k }$ , thereby providing further improvements.

![](images/443eca1a3d867c2c039d0bfa0f181b1f82fe88cda9b3d123c7861a0366cb1bad.jpg)  
Figure 5: Benchmark the speed against FlashAttention-2. ST indicates the straight-through estimator

Figure 6 presents the times and steps required to achieve a specific perplexity. We use a pretrained Pythia- $1 6 0 \mathrm { m }$ model with learning rate decay disabled. The results indicate that SPARSEK ${ . + } \mathrm { S W }$ typically learns more quickly than the sliding window attention, achieving a better pareto-optimum in performance vs. efficiency.

![](images/fd22162481a4bee5f50c7acaed368496503c1fa617eca28c6fac22bc1091d5ec.jpg)  
Figure 6: Time and steps used to reach ppls.

In Table 9, we compare the training time when using different sparse attention. We use the Pythia160M architecture. The context length is 8192. All models are set to have a sparsity ratio similar to SW with a window size of 1024. All models are trained with 5000 steps.

Table 9: Relative training time (RTT) to complete a fixed-step training.   

<table><tr><td>Method</td><td>Full</td><td>SW</td><td>SparseK+SW</td><td>Random</td><td>Hash</td></tr><tr><td>RTT</td><td>100%</td><td>68.9%</td><td>83.2%</td><td>92.8%</td><td>113.2%</td></tr></table>

# C.9 Visualizationout.extend(["</p>", "</hdisplay(HTML(''.join(out)

![](images/62ca2a491941bcbe369a15eb1df3bfc9c019d1efcaea19b32c5ca78cf1bb859c.jpg)

i = 511]:Figure 7: Visualization of the soft selection in an example sequence. Ligher background color means sns.set_theme(rc={'figure.figsize':(24,10)})higher selection scores. We choose the 21st layer in a finetuned TinyLlama 1.1B SPARSEK+SW sns.heatmap(normalized_routing_score[i].reshape(16, 32model. To produce this visualization, we set $k = 1 2 8$ n, $w = 0$ e(16,32), fmt = '', annot_kws={'rotation': 40}), which is different from the fine-tuning --------setting $k = 5 1 2$ -, $w = 5 1 2$ ---------------------------------, for a smaller figure size.

# 3 annot=np.array([i----> 4 sns.heatmap(normaD Limitations

IndexError: index 511 is out of bounds for axis 0 with size 256This paper presents SPARSEK Attention, whose goal is to address both computational and memory ]:efficiency challenges in long-range Transformer computing. We list limitations of our work below:

• Limited model size and context length In this study, we validate the advantages of SPARSEK in settings involving models up to 1.1 billion parameters and context lengths up to 16,000 tokens. The primary reason for this limitation is our restricted computational resources. Despite these constraints, we consider a broad range of model sizes and context lengths to demonstrate the general applicability of our method. Nevertheless, with the advent of recent parameter-efficient fine-tuning techniques, it may be possible to experiment with our method on more limited devices. Decoder-only We restrict our discussion to applying SPARSEK within Transformer decoders. the incremental evaluation capability of the SPARSEK operation demonstrates superior complexity compared to previous approaches. Nonetheless, the SPARSEK operation is also applicable in various other contexts, such as Transformer encoders and routing in mixture-of-expert models, where a top- $\mathbf { \nabla } \cdot \mathbf { k }$ operation might be employed. The sparse output produced by the SPARSEK operation offers a notable advantage in these scenarios because it is close to the top- $k$ selection performed somewhere.   
Text-only We focus exclusively on text tasks. However, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method.

# E Impact Statement

This paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SPARSEK as follows:

• Efficient Long-Range Modeling. First and foremost, the SPARSEK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SPARSEK attention effectively reduces the memory footprint without sacrificing model performance. This is particularly advantageous for resource-constrained environments and edge computing devices. Moreover, this innovation enhances training speed and convergence, contributing to more efficient model development and experimentation.

• More powerful long-range pretrained models. The differentiable SPARSEK operator facilitates gradient-based optimization, contributing to accelerated training speed. This is particularly advantageous for pretrained language models, where training on massive datasets can be time-consuming. Faster convergence and training speed enable researchers and practitioners to experiment with and refine models more efficiently. The efficiency gains provided by SPARSEK attention make it more scalable to work with massive long-sequence datasets, enabling researchers to harness the wealth of information available on the internet and other sources for training robust and contextually aware language models.

• General Application to downstream tasks. The proposed efficient self-attention mechanism can benefit a spectrum of NLP and machine learning downstream tasks, such as long-context document analysis and generation, Transformer-based long-form video analysis, etc. Moreover, the SPARSEK attention mechanism is not limited to specific domains or tasks. Its adaptability makes it applicable across a wide range of natural language processing and machine learning applications, offering a versatile solution to improve efficiency and performance in diverse scenarios.