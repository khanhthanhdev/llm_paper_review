<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_V44P8Uq">LESS IS MORE: TRAINING-FREE SPARSE ATTENTION WITH GLOBAL LOCALITY FOR EFFICIENT REASONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-08-09">9 Aug 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lijie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<country>Carnegie</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Zhang</surname></persName>
							<email>zhihaoz3@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Arti</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Cao</surname></persName>
							<email>shijiecao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
								<orgName type="institution">Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baihong</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yiwei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
							<email>zhihao@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
							<email>rnetravali@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<country>Carnegie</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_WcYmdGN">LESS IS MORE: TRAINING-FREE SPARSE ATTENTION WITH GLOBAL LOCALITY FOR EFFICIENT REASONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-08-09">9 Aug 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">0327FD09C856816C91A175D55E54F9BF</idno>
					<idno type="arXiv">arXiv:2508.07101v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-01T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=true, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_vuJyYfr"><p xml:id="_ujRy4q3">Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves-and in some cases improves-accuracy while achieving a 1.1× average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2× fewer tokens without accuracy loss, achieving a 1.13× end-to-end speed-up compared to existing sparse attention methods. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_KgBeb3s">INTRODUCTION</head><p xml:id="_j3YadpK">Recent advancements in large reasoning models (LRMs) have significantly enhanced the capabilities of large language models (LLMs) for complex reasoning tasks. Models such as DeepSeek-R1 (DeepSeek-AI, 2025), <ref type="bibr">Gemini-2.5-pro (DeepMind, 2025)</ref>, <ref type="bibr">OpenAI-o3 (OpenAI, 2025)</ref>, Qwen3 <ref type="bibr" target="#b20">(Team, 2025)</ref>, and gpt-oss (OpenAI, 2025) leverage test-time scaling-generating large numbers of tokens-to enhance accuracy on challenging reasoning benchmarks <ref type="bibr" target="#b22">(Wei et al., 2023;</ref><ref type="bibr" target="#b2">AoPS, 2025;</ref><ref type="bibr" target="#b17">Rein et al., 2023)</ref>.</p><p xml:id="_FQf6VQf">Unlike traditional language processing tasks, which involve long inputs and short outputs, reasoning tasks exhibit a different computational profile: they generate extensive multi-step derivations-often spanning tens of thousands of output tokens (Research, 2024)-from relatively concise problem statements. This decode-heavy nature incurs substantial computational overhead <ref type="bibr" target="#b14">(Liu et al., 2025)</ref>. For example, using full attention in the HuggingFace framework, DeepSeek-R1-Distill-Llama-8B consumes more than 20 minutes on a single NVIDIA RTX A5000 GPU to generate 32,768 tokens for one AIME problem. This computational profile creates a unique optimization opportunity: while input processing benefits from full attention for accurate context understanding, the lengthy generation phase is well-suited for sparse attention mechanisms <ref type="bibr" target="#b3">(Cai et al., 2025;</ref><ref type="bibr" target="#b8">Gao et al., 2025)</ref>. Sparse attention selectively attends to a subset of critical tokens, offering a promising approach to reduce computational complexity and generation latency. Current techniques can be categorized into selection-based <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b19">Tang et al., 2024;</ref><ref type="bibr" target="#b9">Hao et al., 2025;</ref><ref type="bibr" target="#b13">Liu et al., 2024;</ref><ref type="bibr" target="#b8">Gao et al., 2025;</ref><ref type="bibr" target="#b27">Yuan et al., 2025)</ref> and These limitations motivated us to investigate the intrinsic attention distributions of reasoning models and tasks, with the goal of identifying patterns that enable more accurate and efficient token selection. Our token-level analysis across the reasoning process reveals two key observations on attention localities that fundamentally challenge the selection principles used in existing sparse attention methods.</p><p xml:id="_QtTMFsM">First, reasoning tasks exhibit prominent spatial locality across attention heads, particularly within the Grouped Query Attention (GQA) frameworks prevalent in open-source LLMs <ref type="bibr" target="#b21">(Touvron et al., 2023;</ref><ref type="bibr" target="#b1">AI, 2024;</ref><ref type="bibr" target="#b20">Team, 2025)</ref>. Contrary to conventional wisdom that different attention heads perform specialized roles that require distinct token subsets <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b24">Xiao et al., 2024;</ref><ref type="bibr" target="#b19">Tang et al., 2024)</ref>, we observe substantial overlap in token-importance rankings across heads within the same decoding layer. This overlap reveals that per-head top-k selection yields only a local optimum, overfitting to head-specific query patterns while potentially missing globally important tokens that could enhance performance in future decoding layers.</p><p xml:id="_nkG2BCd">Second, we observe a recency locality pattern across decoding steps: tokens that receive high attention in one decoding step tend to continue attracting substantial attention over multiple subsequent steps. Notably, the ratio between the size of this "recency window" and the total number of selected tokens remains relatively constant throughout decoding, reflecting the intuition that each logical step in reasoning builds directly on the conclusions of preceding steps <ref type="bibr" target="#b10">(Lee &amp; Hockenmaier, 2025)</ref>.</p><p xml:id="_XmSK5td">Building on these insights, this paper presents LessIsMore, a novel training-free sparse attention approach that achieves higher accuracy on reasoning tasks with lower latency by attending to fewer tokens. LessIsMore aggregates head-specific local information into a global attention pattern that is both more robust and more accurate. In each selection layer, LessIsMore exploits the identified locality patterns through a unified token selection process: each attention head first identifies its approximate top-k tokens using tailored selection schemes; these tokens are then aggregated across heads, globally ranked, and pruned to satisfy a predefined token budget. To capture recency locality, LessIsMore reserves a fixed proportion of this for a stable recency window, ensuring that recently generated tokens-critical for step-by-step reasoning-are consistently attended to.</p><p xml:id="_CaA8rKx">Evaluation on Qwen3-8B and Qwen3-4B across diverse reasoning tasks, including AIME-24/25, GPQA-Diamond, and MATH500, demonstrates that LessIsMore consistently and significantly outperforms existing sparse attention baselines, including reasoning-focused methods that require retraining. LessIsMore maintains full accuracy at substantially higher sparsity levels, achieving up to 87.5% sparsity and 1.1× end-to-end inference speed-up on AIME-24 with lossless accuracy, all without increasing reasoning length. These gains are further enabled by our customized kernel support for GQA models.</p><p xml:id="_QRAxEPC">In summary, our contributions are:</p><p xml:id="_d9W7DPG">• We present the first detailed, token-level analysis of attention distributions in the reasoning process, revealing fundamental spatial and recency locality patterns that challenge the conventional assumptions of highly specialized, independent attention heads.</p><p xml:id="_HqheQtd">• We propose LessIsMore, a training-free sparse attention mechanism featuring: (1) Unified Attention Head Selection globally aggregates head-level top-k selections, and (2) Stable Recency Window reserves recent contextual information for reasoning coherence. • We show that LessIsMore matches or improves accuracy on challenging reasoning benchmarks while achieving a 1.1× average decoding speed-up compared with the full attention baseline. Compared to state-of-the-art sparse attention methods, LessIsMore attends to at least 2× fewer tokens, achieves a 1.13× end-to-end speed-up, and shortens generation length by 7% without sacrificing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_P3jsdZh">OBSERVATION</head><p xml:id="_FAR4YBN">Attention mechanisms are central to the functionality of today's transformer-based LLMs. For each attention head i, attention scores and outputs are computed using the scaled-dot product of the query (Q i ), key (K i ), and value (V i ) tensors:</p><formula xml:id="formula_0">W i = Q i K T i √ d , O i = softmax(W i )V i (1)</formula><p xml:id="_6p6bZZ4">Here, W i represents the attention weights (scores) between tokens, and O i is the output from the i-th attention head.</p><p xml:id="_7dJNw7A">Sparse attention methods address the computational overhead associated with attending to all tokens by selectively attending to a limited subset. Existing approaches aim to retain tokens most likely to yield high attention scores, constrained by a fixed token budget k, through different approximation functions <ref type="bibr" target="#b19">(Tang et al., 2024;</ref><ref type="bibr" target="#b3">Cai et al., 2025;</ref><ref type="bibr" target="#b25">Yang et al., 2024)</ref>:</p><formula xml:id="formula_1">arg max ρ f (Q i , K i [ρ], V i [ρ], k), |ρ| = k (2)</formula><p xml:id="_mFT5PQd">where ρ denotes the selected subset of tokens in the KV cache, and the approximation function f is usually an efficient estimation of the underlying ground-truth attention score to obtain ρ. The primary goal of this approximation is to maximize attention recall, defined as the proportion of the ground-truth attention scores that the selected subset covers:</p><formula xml:id="formula_2">R i = (softmax(W i )[ρ]) (softmax(W i ))<label>(3)</label></formula><p xml:id="_GfEB7nP">A higher attention recall indicates a better coverage of the attention mass with the selected token, thereby improving overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_98ttA65">LIMITATIONS OF SPARSE ATTENTION IN REASONING</head><p xml:id="_AN32Sg8">Despite the critical role of attention recall in sparse attention mechanisms, existing methods demonstrate significant limitations when applied to reasoning tasks. Current approaches either overestimate token importance <ref type="bibr" target="#b19">(Tang et al., 2024;</ref><ref type="bibr" target="#b23">Xiao et al., 2023)</ref> or focus on locally optimal selections without adequately capturing global attention patterns across layers or decoding steps <ref type="bibr" target="#b25">(Yang et al., 2024)</ref>.</p><p xml:id="_G9JFjmF">As illustrated in Figure <ref type="figure">1a</ref>, both selection-and eviction-based sparse attention methods exhibit substantial attention recall degradation on the AIME-24 task, with degradation becoming more pronounced as decoding length increases. Specifically, selection-based approaches reach only 75% attention recall, while eviction-based approaches fall below 65%. Additionally, as shown in Figure <ref type="figure">1b</ref>, although TidalDecode achieves comparable attention recall on both AIME-24 and simpler retrieval tasks (e.g., needle-in-the-haystack), the reasoning tasks inherently involve much longer generation sequences. Consequently, inaccuracies from token selection accumulate over prolonged generation, significantly degrading the reasoning quality in sparse attention. These observations underscore the necessity for designing a selection approach capable of capturing critical tokens globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_4mqfsQ8">LOCALITIES IN REASONING</head><p xml:id="_nxUw6MW">Previous studies on traditional tasks have identified the locality property of attention patterns-different decoding layers can share a similar set of critical tokens <ref type="bibr" target="#b25">(Yang et al., 2024)</ref>. It 5000 10000 15000 20000 25000 30000 Generation Length 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Cumulative Average Attention Recall StreamingLLM vs. TidalDecode vs. LessIsMore on AIME-24 (4K Token Budget) StreamingLLM TidalDecode LessIsMore (Ours) (a) Attention recall of different approaches using 4K token budget on an AIME problem. 5000 10000 15000 20000 25000 30000 Sequence Length 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Running-Average Attention Recall Attention Recall Comparison (Qwen3-8B, 4K Token Budget) AIME-24 NiTH (b) Attention recall of retrieval (NiTH) and reasoning (AIME) tasks</p><p xml:id="_ndkkN2g">Figure <ref type="figure">1</ref>: Analysis of attention recall degradation for sparse attention methods on reasoning tasks. Figure <ref type="figure">1a</ref> compares cumulative average attention recall between eviction-based StreamingLLM <ref type="bibr" target="#b23">(Xiao et al., 2023)</ref> and selection-based TidalDecode <ref type="bibr" target="#b25">(Yang et al., 2024)</ref> on an AIME-24 reasoning task, using a token budget of 4K and generation length up to 32K tokens on Qwen3-8B. Figure <ref type="figure">1b</ref> contrasts running-average attention recall of TidalDecode between the reasoning-intensive AIME-24 task and the simpler needle-in-the-haystack retrieval task under the same token budget on Qwen3-8B.</p><p xml:id="_EQkETsx">has also been suggested that different attention heads have distinct functional roles, thereby benefiting from specialized token subsets <ref type="bibr" target="#b24">(Xiao et al., 2024)</ref>. In Figure <ref type="figure" target="#fig_0">2</ref> and Figure <ref type="figure" target="#fig_1">3</ref>, we analyze the distribution of the top-4K tokens across all 32 attention heads at different decoding steps using Qwen3-8B, a 36-layer model with Group Query Attention (GQA). Our analysis extends these findings by demonstrating that attention localities in reasoning tasks manifest both spatially (within the same key-value group and even across attention heads) and with recency (across decoding steps). Preprint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1" xml:id="_rCrqz98">SPATIAL LOCALITY ACROSS ATTENTION HEADS</head><p xml:id="_YajrzZY">The visualization of Figure <ref type="figure" target="#fig_0">2</ref> highlights high overlap among selected tokens across consecutive groups of four attention heads within the same key-value group (yellow regions). Additionally, broader overlaps spanning all attention heads (red regions) include frequently attended recent tokens. This observation contradicts the common belief that each attention head serves specialized functions with distinct attention score distributions, thereby requiring head-wise token selection with different token subsets for optimal performance. Instead, our findings suggest that reasoning tasks exhibit remarkable consistency in token importance across attention heads, indicating that a unified global selection strategy may be more effective than maintaining separate token subsets per head in the reasoning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2" xml:id="_gYxXnMH">RECENCY LOCALITY OF RECENT TOKENS</head><p xml:id="_AXQKrs2">Across-Attn-Head Recent Token Locality</p><p xml:id="_AV66eVd">Positional Token Selected Positional Token Not Selected Figure 3 illustrates the distribution of top-4K tokens across multiple decoding lengths. As shown, the most recently generated tokens consistently receive high attention scores in subsequent steps. Furthermore, the size of this "recency window" remains relatively constant throughout the decoding process, as shown in all subfigures of Figure 3.</p><p xml:id="_KHhQ59D">This recency locality directly reflects the nature of step-by-step reasoning, where each new logical step maintains coherence with immediately preceding conclusions <ref type="bibr" target="#b10">(Lee &amp; Hockenmaier, 2025)</ref>. Prior work like StreamingLLM <ref type="bibr" target="#b23">(Xiao et al., 2023)</ref> has recognized the importance of recent tokens by maintaining a fixed sliding window alongside attention sink tokens. Building on this, our analysis reveals that the ratio between critical recency window size and the number of selected tokens remains stable throughout the reasoning process. This observation supports the design of adaptive token Preprint selection mechanisms that allocate a fixed proportional budget to recent tokens to maintain reasoning accuracy efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_Jbb5NEj">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qnrZbeH">Full Attention</head><p xml:id="_vBK9ePJ">Layer 0</p><p xml:id="_tFY3FbW">Token Selection</p><p xml:id="_EAuh86p">Layer 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FRkxqZ2">Sparse Attention</head><p xml:id="_VGvjXwK">Layer 3</p><p xml:id="_5Ub7DNS">Sparse Attention</p><p xml:id="_Bh4gqpy">Layer (i-1)</p><p xml:id="_zew2SFq">Token Selection Layer i Sparse Attention Layer (i+1) Sparse Attention Layer 35 ⋯ Full Attention Layer 1 ⋯ Figure 4: Walkthrough of a single decoding step of the selection-based approach TidalDecode (Yang et al., 2024), which performs full attention for the first two layers, full attention with token selection for the third layer and a middle layer, and sparse attention for the other layers. This section introduces LessIsMore, an efficient and effective sparse attention system explicitly designed for reasoning tasks. LessIsMore exploits the unified global attention patterns identified through the analysis of spatial and recency localities presented in Section 2.2.1 and Section 2.2.2 by integrating two key techniques: Unified Attention Head Selection and Stable Recency Window. In this paper, LessIsMore adopts TidalDecode (Yang et al., 2024), one of the best-performing sparse attention methods, as a backbone; specifically, it starts with two full attention layers, includes two dedicated token selection layers, and employs sparse attention in the remaining layers. However, as discussed in Section 1 and shown in Section 5.2, LessIsMore's underlying techniques can be incorporated into any sparse attention using approximation algorithms. 3.1 LESSISMORE ( ) 0 1 2 3 Attn Head Index 1.1 2.2 1.2 1.9 2.1 3.2 3.6 0.9 3.2 4.5 6.7 2.1 1.3 2.6 1.3 2.4 0.3 2.8 4.2 2.8 1.4 1.2 3.6 4.2 1.5 0.8 5.5 3.6 1.7 1.9 3.8 1.7 Token Position P ← matmul⟨Q, K⟩ Index Rank 0 1 2 2 2 2 5 1 1 6 6 6 4 4 4</p><formula xml:id="formula_3">ρ head ← Top_k(P[: , : -1], k = K ⋅ r) range(n)[ -(K ⋅ r) :] 2 5 1 7 @ 0 1 2 3 4 5 6 7</formula><p xml:id="_JfT2Gua">Recent Window Token</p><p xml:id="_SuXZgmC">0 1 2 3 Attn Head Index ([: 3] ∘ ∪ ∘ flatten)ρ head Sparse Attention Layers Token Selection Layer 0 1 2 3 4 5 6 7 0 1 2 3 Attn Head Index KV Cache 1 2 5 7 Loaded KV Cache 1 2 5 7 Loaded KV Cache ⋯ (n, K, r) = (8, 4, 0.25) 7 (1) (2) (3) ρ := {1,2,5,7} Algorithm 1 LessIsMore Decoding Pipeline 1: Input: Current hidden state h, KV cache C, token budget K, static ratio r 2: Output: Logits 3: Initialize: ρ = [] ▷ Token buffer for selected indices 4: for each decoder layer i do 5:</p><formula xml:id="formula_4">q, k, v = f (W qkv , h) 6: C.append(k, v) 7: if i is Full Attention Layer then 8: o = FullAttention(q, C[:]) 9:</formula><p xml:id="_Tq5mKaB">else if i is Token Selection Layer then 10:</p><formula xml:id="formula_5">o = FullAttention(q, C[:]), P := q • C.K ⊤</formula><p xml:id="_adACdBW">▷ Full attention and Extract QK product 11:</p><formula xml:id="formula_6">ρ head = TopKIndices(P [:, : -(K • r)], k = K • (1 -r)) ▷ Top-k for each head 12:</formula><p xml:id="_VdQxwWD">ρ unified = UnionFlatten(ρ head )</p><p xml:id="_xQyAbG4">13:</p><formula xml:id="formula_7">ρ recent = Recent(K • r) 14: ρ = ρ unified [: K • (1 -r)] ∪ ρ recent 15:</formula><p xml:id="_rrcN5hC">else</p><formula xml:id="formula_8">16: o = SparseAttention(q, C[ρ]) ▷ Use selected</formula><p xml:id="_eEDR8PY">token indices 17: end if 18: h = FFN(o) 19: end for 20: return lm head(h)</p><p xml:id="_zTgs32F">processes each decoding step with three distinct layer types. For full attention layers (lines 7-8), standard attention computation is performed. For token selection layers (lines 9-15), the overall token budget K is partitioned into two subsets: top-k tokens and the most recent tokens. During token selection, full attention is first used to calculate the QK-product P = q • C.K T (line 10).</p><p xml:id="_EEu4C3P">LessIsMore then applies Unified Attention Head Selection through the unified token selection process (lines 11-14), excluding the most recent tokens to focus the selection process on historical context. Each attention head independently selects its top-k token indices based on attention scores (line 11), followed by global aggregation and sorting across all heads (line 12). The top-ranked indices are then combined with the recent token indices determined by Stable Recency Window (lines 13-14) to form the final selected token set ρ. LessIsMore then fetches the corresponding key-value tensors from the KV cache for the selected indices. The indices are subsequently shared by all sparse attention layers (lines 16) until the next selection layer or the end of the current decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_E6Gfk7g">UNIFIED ATTENTION HEAD SELECTION</head><p xml:id="_CY5v3y5">Unified Attention Head Selection aims to take advantage of the spatial locality observed with the token attention of Section 2.2.1 to improve efficiency and effectiveness. The core mechanism is implemented following lines 11-12 of Algorithm 1, where each attention head independently computes attention scores and identifies the top-k tokens most relevant to its query through TopKIndices(P [:, :</p><formula xml:id="formula_9">-(K•r)], k = K•(1-r))</formula><p xml:id="_nBN5PQd">. Instead of maintaining separate sets of tokens per head, which increases the selection overhead and complexity of KV cache access, Unified Attention Head Selection aggregates the independently selected tokens from all attention heads.</p><p xml:id="_Vy5nwhw">The aggregation process, formalized as UnionFlatten(ρ head ) in line 12, flattens the top-k token indices selected by each head into a single unified set. This combined set is then globally sorted according to the tokens' rank within its attention head. Subsequently, only the globally highest-ranked tokens, limited by the predefined token budget K •(1-r) (line 14), are retained. This unified selection strategy not only improves the attention recall shown in Section 4.4 by using the observed spatial location, where tokens frequently overlap in importance across heads, but also significantly simplifies token retrieval, enhancing computational efficiency during sparse attention computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_zYSsVD6">STABLE RECENCY WINDOW</head><p xml:id="_W88ePsw">Stable Recency Window addresses the consistent recency locality observed in reasoning tasks, where recently generated tokens are repeatedly and consistently attended by all attention heads in Figure <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_njksWS2">Preprint</head><p xml:id="_gReDP7B">To exploit this pattern, Stable Recency Window dedicates a fixed ratio of the total token budget K exclusively to the most recently generated tokens, forming a "stable recency window." This mechanism is implemented in lines 13-14 of Algorithm 1.</p><p xml:id="_D89bFNK">Prior sparse attention training approaches maintain a constant number of tokens as the sliding window size regardless of token budgets <ref type="bibr" target="#b27">(Yuan et al., 2025)</ref>. In contrast, the stable recency window in LessIsMore is determined by a predefined ratio r, typically a small fraction of K, through Recent(K • r) (line 13). The final set of tokens used for sparse attention computation is formed by the union operation ρ = ρ unif ied [: K • (1 -r)] ∪ ρ recent (line 14), consisting of the globally unified top-k tokens selected through Unified Attention Head Selection and the most recent tokens determined by Stable Recency Window. This design directly reflects the empirical observation that recently generated tokens inherently possess critical contextual information necessary for accurate and coherent step-by-step reasoning. By explicitly allocating resources to recent tokens through this algorithmic approach, Stable Recency Window effectively ensures high attention recall and improved reasoning quality while maintaining computational efficiency, as shown in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_PET3TdV">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_xyuRDPr">EXPERIMENT SETUP</head><p xml:id="_dSaq4Na">We conduct extensive experiments to evaluate the accuracy and efficiency of LessIsMore. Our experiments consider two widely-used reasoning models, Qwen3-8B and Qwen3-4B <ref type="bibr" target="#b20">(Team, 2025)</ref> backed up with GQA. Both models are specifically trained for reasoning tasks and perform the effective thinking process by generating extensive tokens. Further, we evaluate on multiple mainstream reasoning tasks, including AIME-24/25, GPQA-Diamond, and MATH500.</p><p xml:id="_dtZyPuE">In Section 4.2, we compare LessIsMore with both training-free (TidalDecode, Quest) and trainingrequired (SeerAttention-r) sparse attention baselines. For training-free approaches, we keep the first two full attention layers for TidalDecode. To ensure a fair comparison, we set the same selection layer for LessIsMore and TidalDecode -Layer 12 for Qwen3-8B and Layer 20 for Qwen3-4B. <ref type="foot" target="#foot_0">2</ref> The static ratio r of LessIsMore is set to 0.25 and 4 tokens are always reserved for attention sink in this section. We maintain the same experiment configuration of Quest and SeerAttention-r in <ref type="bibr" target="#b8">(Gao et al., 2025)</ref>, where the block size is set to 64 and all layers perform sparse attention. To guarantee the consistency of evaluation results, the maximum generation length of Qwen3-8B and 4B models are set to 32,768 across all reasoning tasks. For LessIsMore and TidalDecode, we calculate average pass@1 accuracy over 16 samples for AIME-24/25, 8 samples for MATH500 and GPQA-Diamond, with Full Attention, Quest, and SeerAttention-r results using the reported sampling configurations in <ref type="bibr" target="#b8">(Gao et al., 2025)</ref>.</p><p xml:id="_WankcVk">In Section 4.3, we compare the per-token decoding latency of our system implemented using customized kernels with the selection-based model (TidalDecode) and full attention using FlashInfer <ref type="bibr">(Ye et al., 2024)</ref>. Finally, in Section 4.4, we show that LessIsMore's global selection techniques generalize to other sparse attention approaches, analyze the impact of sparse attention on reasoning length, and examine the effect of the ratio of recent window size on LessIsMore's attention recall throughout the generation in reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_spjByTc">EVALUATION ON REASONING TASKS</head><p xml:id="_6SMKxuS">Figure <ref type="figure" target="#fig_4">6</ref> presents the accuracy comparison among Full Attention baseline, LessIsMore, and other sparse attention methods across mainstream reasoning benchmarks, including the complex mathematical contests AIME-24/25, and the simpler reasoning tasks MATH500 and GPQA-Diamond, evaluated on the reasoning-focused language models Qwen3-8B and Qwen3-4B. For the challenging AIME tasks, experiments span token budgets of 2K, 4K, and 6K tokens, corresponding to overall generation lengths averaging between 14K and 20K tokens. In contrast, the MATH500 and GPQA-Diamond tasks involve shorter reasoning sequences averaging between 4K and 9K tokens. LessIsMore consistently achieves the highest accuracy across all evaluated tasks and token budgets, closely matching the performance of full attention, even under stringent token constraints. Specifically, for Qwen3-8B on the AIME-24 task at the smallest token budget (2K tokens), LessIsMore attains a nearly lossless performance, markedly surpassing training-free methods such as Quest and TidalDecode, as well as the training-required SeerAttention-r, all of which suffer notable accuracy degradation at reduced token budgets. A similar trend is observed across all tasks, underscoring the capability of LessIsMore to effectively retain critical contextual information and facilitate accurate reasoning even with limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_ugGs84p">EFFICIENCY EVALUATION</head><p xml:id="_TEc2eqK">To evaluate the practical efficiency gains of LessIsMore, we implement customized kernels optimized for GQA-based models and conduct average decoding latency measurements on reasoning tasks. Our evaluation uses LLama-3.1-8B (AI, 2024) (as a reference to the evaluated Qwen3-8B with the same parameter size and attention mechanism GQA) deployed on a single NVIDIA RTX A5000 GPU, comparing the efficiency-accuracy trade-offs among LessIsMore, TidalDecode, and Full Attention within a unified system built on FlashInfer <ref type="bibr">(Ye et al., 2024)</ref>. The token index aggregation stage illustrated in Figure <ref type="figure" target="#fig_3">5</ref> is implemented using PyTorch primitives.</p><p xml:id="_btchbq7">As demonstrated in Figure <ref type="figure">7</ref>, even if TidalDecode is an efficient selection-based approach that only performs a two-time top-k selection for each decoding step, LessIsMore consistently outperforms TidalDecode in all token budget configurations, achieving substantially higher accuracy while maintaining faster average decoding speeds. Notably, LessIsMore delivers near-lossless performance (73.75 vs. 74.48 full attention baseline) using only a 2K token budget with a 1.10× speed-up over full attention. In contrast, TidalDecode achieves merely 53.33 accuracy under identical constraints.</p><p xml:id="_pSD3ZrM">In Table <ref type="table">1</ref>, even with a token budget of 6K, TidalDecode obtains a lower accuracy and generates 15.9K tokens. Meanwhile, LessIsMore achieves a 1.06× average decoding speedup and 7% shorter generation length, which contributes to a 1.13× end-to-end speedup compared to TidalDecode. This demonstrates LessIsMore's superior ability to maintain reasoning quality while delivering meaningful computational savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" xml:id="_99rHUzM">ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1" xml:id="_FxCgCGX">EFFECTIVENESS OF LESSISMORE'S AGGREGATION ON GQA</head><p xml:id="_pfUWSjF">Applying sparse attention to GQA-based models often requires aggregating tokens from the query heads to the KV group <ref type="bibr" target="#b27">(Yuan et al., 2025)</ref>. To demonstrate the generalizability of our unified selection in LessIsMore beyond the TidalDecode pipeline, we evaluate different token aggregation schemes on Qwen3-8B with GQA, where each KV head is shared across multiple query (or attention) Preprint better better Figure <ref type="figure">7</ref>: Efficiency-accuracy tradeoff comparison on AIME-24 using LLama-3.1-8B. Each point represents the average decoding latency across the corresponding average generation length in Table <ref type="table">1</ref>. LessIsMore (orange squares) consistently achieves higher accuracy than TidalDecode (blue circles) while maintaining lower latency across all token budgets (1K, 2K, 4K, 6K). The closer to the top-left corner, the better the method performs. Full Attention baseline (triangle) provides the accuracy upper bound but with higher computational cost.</p><p xml:id="_HwT2QWA">LessIsMore(L2) LessIsMore(All) Head-to-Head(L2) Full Attention Randomized Top-k(L2) Randomized Top-k(All) 5000 10000 15000 20000 25000 30000 Generation Length 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00</p><p xml:id="_rZxFfdr">Cumulative Average Attention Recall Generalization Recall on AIME-24 (4K Token Budget) Preprint LessIsMore</p><p xml:id="_Pvc4W3Y">Figure <ref type="figure">9</ref>: Ablation study on the impact of varying the recent window ratio in LessIsMore (⋆) on the AIME-24 reasoning task, using a token budget of 4K and generation length up to 32K tokens on Qwen3-8B. LessIsMore corresponds to the 25% recent setting combined with Unified Attention Head Selection, (labeled with (⋆)). We compare it against alternative recent window ratios, the 100% recent baseline (i.e., using only recent tokens), and TidalDecode. Curves are annotated with "(T)" or "(F)" to indicate whether the configuration yields the correct answer. Notably, only configurations that incorporate recent window with Unified Attention Head Selection (25%, 50%, 75%) succeed in solving the task.</p><p xml:id="_ZJhm6Da">different configurations. Using only recent tokens (100% recent) provides the lowest attention recall, as it discards distant but important contextual tokens. TidalDecode, which selectively retains tokens without explicitly accounting for reasoning-specific attention locality, significantly improves attention recall but still fails to produce the correct answer. Building upon TidalDecode, simply using Unified Attention Head Selection variant (0% recent) further improves attention recall by leveraging our selection scheme, yet it also fails to generate the correct answer. Incorporating a proportion of recent tokens consistently boosts attention recall. Specifically, configurations with 25%, 50%, and 75% recent windows all manage to generate the correct answer. Among them, 25% recent-which corresponds to the full design of LessIsMore-achieves the highest attention recall throughout the most generation process. This validates the design choice of allocating 25% of the token budget to the recent window in LessIsMore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_XprsEpR">FUTURE WORK AND LIMITATIONS</head><p xml:id="_UeRu6Mu">5.1 ADAPTIVE SHORT-CONTEXT RATIO While it is optimal for sparse attention mechanisms to dynamically determine the best token budget and parameter ratios at runtime, this remains challenging in practice <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b8">Gao et al., 2025;</ref><ref type="bibr" target="#b19">Tang et al., 2024;</ref><ref type="bibr" target="#b3">Cai et al., 2025)</ref>. LessIsMore is no exception to this limitation. Currently, we employ a pre-defined token budget and a fixed 25% short-context ratio based on our observation of the consistent size of important recent context across reasoning tasks.</p><p xml:id="_egZJjeb">However, it would be optimal to determine this ratio adaptively during generation. One effective solution that has been explored by prior works is through top-p sampling-inspired approaches, where the ratio could be dynamically adjusted based on the attention score distribution at each decoding step <ref type="bibr" target="#b4">(Chen et al., 2024;</ref><ref type="bibr" target="#b12">Lin et al., 2025)</ref>.</p><p xml:id="_eMSsvAm">Preprint 5.2 GENERALIZATION OF LESSISMORE Currently, LessIsMore is based upon the pipeline used by TidalDecode. Evaluations on GQA-based models Qwen3-4B and Qwen3-8B demonstrate superior reasoning performance compared to existing sparse attention approaches. More than just a model-specific optimization, LessIsMore offers a principled approach to designing more accurate and efficient sparse attention for reasoning tasks. Given the fundamental locality observations presented in Section 2.2, future work should prioritize extending LessIsMore's locality-based principles to models beyond GQA, such as MLA or MoE architectures that employ different attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_ZxtcHEn">FROM TOKEN SAVING TO MEMORY SAVING</head><p xml:id="_5cTdv8w">System-level optimizations in LessIsMore remain unexplored that could translate token savings into substantial memory savings. Currently, several implementation optimizations limit the full potential of LessIsMore: (1) The union operation in selection is implemented using PyTorch primitives rather than optimized CUDA kernels.</p><p xml:id="_z2XDRNx">(2) The token selection process still requires maintaining the full KV cache in memory, limiting memory efficiency compared to eviction-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_56jFxpb">BEYOND TRAINING-FREE: NATIVE TRAINING WITH HYBRID ATTENTION</head><p xml:id="_rR36PXG">The success of LessIsMore demonstrates that a training-free, hybrid attention approach which combines full attention and sparse attention can effectively address the efficiency challenges of reasoning models without sacrificing accuracy. This hybrid model architecture echoes the emerging trend in pretraining where the model architecture is moving towards combining different attention mechanisms <ref type="bibr" target="#b7">Dong et al.;</ref><ref type="bibr" target="#b16">OpenAI (2025)</ref>. Looking ahead, directly integrating the principles of LessIsMore into the pretraining process opens a compelling avenue for future research. In addition, LessIsMore employs an inter-layer hybrid approach, a more advanced step would be to explore intra-layer hybrid attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_PuUSzP2">RELATED WORK</head><p xml:id="_mQmA2Ew">Efficient sparse attention with KV cache compression. Sparse attention mechanisms reduce the computational overhead and memory requirements of attention computation by selectively attending to only a subset of tokens, significantly improving inference efficiency for long-sequence tasks <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b19">Tang et al., 2024)</ref>. Current approaches can be broadly categorized into two main paradigms: eviction-based and selection-based methods. Eviction-based approaches <ref type="bibr" target="#b23">(Xiao et al., 2023;</ref><ref type="bibr" target="#b28">Zhang et al., 2023;</ref><ref type="bibr" target="#b25">Li et al., 2024;</ref><ref type="bibr" target="#b0">Adnan et al., 2024)</ref> permanently discard tokens from the KV cache based on predefined criteria, achieving better memory savings by maintaining a smaller cache size throughout generation. In contrast, selection-based methods <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b19">Tang et al., 2024;</ref><ref type="bibr" target="#b9">Hao et al., 2025;</ref><ref type="bibr" target="#b13">Liu et al., 2024)</ref> retain the full KV cache but dynamically choose which tokens to attend to during computation, typically optimizing for locally maximal attention scores and choosing different tokens for each attention head. While both approaches demonstrate effectiveness on standard long-context tasks such as retrieval and summarization, they face significant challenges when applied to reasoning tasks due to the accumulation of selection errors over extended generation sequences.</p><p xml:id="_E6PgJKd">Sparse attention in reasoning. Recent reasoning models leverage the principle that scaling testtime compute can be more effective than scaling model parameters <ref type="bibr" target="#b22">(Wei et al., 2023;</ref><ref type="bibr" target="#b6">DeepSeek-AI, 2025)</ref>, generating extensive token sequences to enhance reasoning accuracy through deliberative processes. However, the lengthened generation nature of reasoning tasks poses unique challenges for applying existing sparse attention methods. When applied on reasoning tasks, prior works either suffer from significant accuracy degradation when using small token retention ratios <ref type="bibr" target="#b25">(Yang et al., 2024;</ref><ref type="bibr" target="#b19">Tang et al., 2024;</ref><ref type="bibr" target="#b3">Cai et al., 2025)</ref> or require computationally expensive post-training procedures to mitigate accuracy loss accumulated during generation <ref type="bibr" target="#b8">(Gao et al., 2025)</ref>, both of which also significantly increase the generation length of reasoning tasks. In contrast, LessIsMore is proposed as a selection-based, training-free approach that leverages intrinsic spatial and recency attention patterns within the reasoning process to achieve high accuracy with substantially reduced token utilization ratios but without extending the generation length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FyVc8YE">Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_VMwpuPA">CONCLUSION</head><p xml:id="_RqtQV4e">In this paper, we propose LessIsMore, a novel training-free sparse attention mechanism specifically designed to address the inaccurate selection limitation of prior approaches on reasoning tasks. Our approach fundamentally challenges the conventional paradigm that each attention head should independently select distinct subsets of tokens optimized locally. Instead, by leveraging observed spatial and recency locality patterns, LessIsMore globally aggregates token importance across all heads, significantly enhancing token selection accuracy. Notably, LessIsMore achieves nearlossless accuracy using extremely low token budgets across diverse reasoning benchmarks. For instance, it maintains full accuracy on the challenging AIME-24 task with merely a 2K token budget, outperforming existing methods that suffer significant accuracy drops under similar constraints. Furthermore, unlike prior sparse attention methods that inherently extend reasoning length due to accumulated selection errors, LessIsMore retains efficiency with generation lengths comparable to full attention. Consequently, comprehensive evaluations demonstrate that LessIsMore can preserve (and even improve) accuracy on challenging reasoning benchmarks while having 1.1× average decoding speed-ups compared with the full attention baseline; moreover, it preserves the accuracy while attending to at least 2× fewer tokens and achieves a 1.13× end-to-end speedup stemming from 7% shorter generation length and 1.06× average decoding speedup compared to other state-of-the-art sparse attention methods. Our results underscore the effectiveness and promise of exploiting global attention patterns in sparse attention mechanisms tailored for reasoning-intensive tasks.</p><p xml:id="_ZaaDYmb">Preprint </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc xml:id="_bU8bGq6">Figure2: The distribution of the ground-truth top-4K tokens across all attention heads at 20K-th decoding step at Layer 4 on AIME-24 with Qwen3-8B. The dark blue positions stand for tokens included in ground-truth top-4K budget. We enclose the highly overlapped area of attention heads within the same kv group and across all heads with different colors</figDesc><graphic coords="4,136.19,447.44,391.58,221.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc xml:id="_KU9qxD8">Figure 3: The distribution of the ground-truth top-4K tokens across all attention heads at 10K-, 15K-, 20K-, and 25K-th decoding step at Layer 4 on AIME-24 with Qwen3-8B. We enclose the highly overlapped area of attention heads within the same KV group with red, which forms a most recent window across all decoding steps</figDesc><graphic coords="5,317.53,405.38,212.12,120.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc xml:id="_AZeYcht">Figure 5: The selection process of LessIsMore is three-fold: (1) under token budget K = 4, r = 0.25, compute attention score matrix W and extract the top-k (k = K * (1 -r), r is the ratio of the most recent tokens that will be, by default, reserved as recency window) token indices for each attention head as ρ head ; (2) flatten and union the selected indices for all heads, keeping the first k indices; (3) concatenate them with the most recent tokens, resulting in final token set ρ. The sparse attention layers only load the tensors of tokens in ρ from KV cache for all attention heads until the next selection layer or the end of the decoding step. The detailed mechanism of token selection and sparse attention layers in LessIsMore is illustrated in Figure 5 and formalized in Algorithm 1. As shown in lines 4-19 of Algorithm 1, LessIsMore</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc xml:id="_JPwzmjh">Figure6: Accuracy results of LessIsMore (ours), Quest, TidalDecode, SeerAttention-r, and Full Attention across for multiple main-stream reasoning tasks. Across all evaluated tasks, LessIsMore consistently achievs the lossless accuracy with small token budgets (1K or 2K), always outperforming all other baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc xml:id="_xaTN72S">Figure 8: The Top-4K attention recall of different selection schemes applied only on Layer 2(L2) or all decoding layers(All). (1) LessIsMore: our unified top-k selection across all attention heads with 25% tokens for recency window, (2) Randomized Top-k: random application of one query head's top-k tokens to the entire KV group, and (3) Head-to-Head: direct utilization of top-k tokens for each individual attention head</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc xml:id="_duFNsv6">Results of 1K-, 2K-, and 4K-token-budget in Figure6evaluated on Qwen3-8B and Qwen3-4B for MATH500 and GPQA benchmarks.</figDesc><table><row><cell cols="2">Model (Task) Method / Budget</cell><cell cols="3">K=1000 K=2000 K=4000</cell></row><row><cell></cell><cell>Quest</cell><cell>36.95</cell><cell>66.98</cell><cell>87.80</cell></row><row><cell>Qwen-3-8B (MATH500)</cell><cell>TidalDecode SeerAttention-r LessIsMore (Ours)</cell><cell>73.85 83.57 93.35</cell><cell>86.00 91.67 94.55</cell><cell>89.95 94.00 94.45</cell></row><row><cell></cell><cell>Full Attention</cell><cell>94.43</cell><cell>94.43</cell><cell>94.43</cell></row><row><cell></cell><cell>Quest</cell><cell>11.80</cell><cell>25.06</cell><cell>47.22</cell></row><row><cell>Qwen-3-8B (GPQA)</cell><cell>TidalDecode SeerAttention-r LessIsMore (Ours)</cell><cell>34.84 39.43 58.84</cell><cell>52.39 54.41 60.86</cell><cell>57.57 60.48 61.36</cell></row><row><cell></cell><cell>Full Attention</cell><cell>60.54</cell><cell>60.54</cell><cell>60.54</cell></row><row><cell></cell><cell>Quest</cell><cell>11.62</cell><cell>44.45</cell><cell>73.40</cell></row><row><cell>Qwen-3-4B (MATH500)</cell><cell>TidalDecode SeerAttention-r LessIsMore (Ours)</cell><cell>74.85 84.67 92.50</cell><cell>85.80 91.85 94.12</cell><cell>90.15 94.10 94.16</cell></row><row><cell></cell><cell>Full Attention</cell><cell>93.93</cell><cell>93.93</cell><cell>93.93</cell></row><row><cell></cell><cell>Quest</cell><cell>4.29</cell><cell>14.64</cell><cell>24.74</cell></row><row><cell>Qwen-3-4B (GPQA)</cell><cell>TidalDecode SeerAttention-r LessIsMore (Ours)</cell><cell>40.97 39.84 56.31</cell><cell>47.28 49.94 56.56</cell><cell>53.41 55.40 56.56</cell></row><row><cell></cell><cell>Full Attention</cell><cell>56.19</cell><cell>56.19</cell><cell>56.19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p xml:id="_CqDdVGA">the choice is justified in Appendix A.1</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_cYXCT8C"><p xml:id="_wA9kKBU">Preprint heads. We compare three selection strategies LessIsMore, Randomized Top-k, and Head-to-Head in Figure <ref type="figure">8</ref>.</p><p xml:id="_WsWAJJ9">The results reveal a critical distinction between local and global optimization strategies. When selection is performed on all decoding layers (All), locally optimal schemes like Randomized Top-k can achieve competitive performance by overfitting to layer-specific patterns. However, when selection frequency is reduced to only Layer 2 (L2)-a more general scenario that reduces computational overhead-our unified approach significantly outperforms alternative aggregation schemes.</p><p xml:id="_dp3dC6E">This performance gap demonstrates that locally optimal selection methods, while effective for immediate layer optimization, fail to generalize robustly across future decoding layers. The superior attention recall of LessIsMore under sparse selection conditions indicates that our global aggregation strategy, combined with the stable recency window, provides more robust token importance estimation that generalizes effectively across the reasoning process. This finding validates the core principle that unified global selection, rather than head-specific local optimization, is essential for maintaining high attention recall in computation-constrained reasoning scenarios.  <ref type="table">1</ref> and corroborated by prior research <ref type="bibr" target="#b8">(Gao et al., 2025)</ref>. This phenomenon reflects the accumulation of selection errors discussed in Section 1, where imprecise token retention forces models into inefficient reasoning patterns that compromise both accuracy and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2" xml:id="_MSN8xun">GENERATION LENGTH ANALYSIS UNDER SPARSE ATTENTION</head><p xml:id="_cKRquu2">Table <ref type="table">1</ref> presents the average generation lengths of different approaches under various token budgets on AIME-24 using Qwen3-8B. Under restrictive token budgets (K=2000), existing methods generate substantially longer sequences compared to full attention: Quest, SeerAttention-r and TidalDecode each generate 30.0K, 19.8K, and 17.4K tokens, representing 103%, 34%, and 18% increases respectively over the full attention baseline of 14.8K tokens. These extended sequences indicate that sparse attention errors accumulate over time and may force models to engage in a redundant reasoning process. In contrast, LessIsMore maintains generation lengths closely aligned with full attention across all token budgets. At K=4000, LessIsMore generates the same number of tokens as full attention does while achieving better accuracy. Meanwhile, even with a token budget of 6K, TidalDecode obtains a significant lower accuracy and generates 15.9K tokens. Combining with the average decoding latency in Figure <ref type="figure">7</ref>, LessIsMore achieves a 1.13× end-to-end speedup compared to TidalDecode.</p><p xml:id="_5JQnnEg">Since inaccurate token selection leads to extended generation lengths, attention recall serves as an indicator of both selection accuracy and computational efficiency. Therefore, evaluating attention recall dynamics throughout generation becomes more crucial for assessing sparse attention methods on reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3" xml:id="_KdWdePh">EFFECT OF RECENT WINDOW RATIO</head><p xml:id="_k2Q4GNm">To better understand the design choices in LessIsMore, we evaluate how varying the ratio of recent window size impacts attention recall and answer correctness in Figure <ref type="figure">9</ref>. We run the AIME-24 reasoning task under a fixed 4K token budget and record the cumulative attention recall across Following the procedure of choosing optimal re-selection layer of TidalDecode <ref type="bibr" target="#b25">(Yang et al., 2024)</ref>, we conduct a simple 5K-context-length needle-in-the-haystack test on TidalDecode with Qwen3-8B and Qwen3-4B. With a token budget of 256, Layer 12 achieves the best accuracy of 86% on Qwen3-8B; Layer 12 and 20 achieve the best accuracies of 86% and 84% on Qwen3-4B, respectively. Moreover, prior work has found taht in the same model family, the optimal re-selection layer is similar. For Qwen3, we validate that Layer 12 is an important layer. To demonstrate the generalization of our approach on different models, we choose different re-selection layers for different Qwen3 models.</p><p xml:id="_PNXQPY9">In this paper's experiments Section 4, we apply the same re-selection layer index on TidalDecode and LessIsMore for a fair comparison -Layer 12 and Layer 20 for Qwen3-8B and Qwen3-4B, respectively.</p><p xml:id="_gcrE8ej">A.2 REASONING EVALUATION RESULTS </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_tg9Nkzn">Keyformer: Kv cache reduction through key tokens selection for efficient generative inference</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">J</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Soloveychik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purushotham</forename><surname>Kamath</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.09054" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_4r9BSZS">Llama 3.1: Advanced open-source language model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Meta</surname></persName>
		</author>
		<ptr target="https://ai.meta.com/blog/meta-llama-3-1/" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_n2sP4mb">Aime problems and solutions</title>
		<author>
			<persName><surname>Aops</surname></persName>
		</author>
		<ptr target="https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions" />
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="2025" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_TrcbECs">R-kv: Redundancy-aware kv cache compression for reasoning models</title>
		<author>
			<persName><forename type="first">Zefan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abedelkadir</forename><surname>Asi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2505.24133" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_rvZBtuV">Magicpig: Lsh sampling for efficient llm generation</title>
		<author>
			<persName><forename type="first">Zhuoming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranajoy</forename><surname>Sadhukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.16179" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
		</author>
		<ptr target="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/" />
		<title level="m" xml:id="_e63sKFW">Gemini 2.5: Our most intelligent ai model</title>
		<imprint>
			<date type="published" when="2025-03">March 2025</date>
			<biblScope unit="page" from="2025" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main" xml:id="_h58zEVx">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12948" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_WDMVWW2">Hymba: A hybridhead architecture for small language models</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Sunil Mahabaleshwarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Van Keirsbilck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshi</forename><surname>Suhara</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.13676" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_vAKjmGB">Seerattention-r: Sparse attention adaptation for long reasoning</title>
		<author>
			<persName><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayden</forename><surname>Kwok-Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2506.08889" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_ezbxbq3">Om-niKV: Dynamic context selection for efficient long-context LLMs</title>
		<author>
			<persName><forename type="first">Jitai</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ulCAPXYXfa" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_mbThX99">The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_fFg7reP">Evaluating step-by-step reasoning traces: A survey</title>
		<author>
			<persName><forename type="first">Jinu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2502.12289" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_TqVPXky">Snapkv: Llm knows what you are looking for before generation</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Venkitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acyr</forename><surname>Locatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.14469" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_4hupsUx">Twilight: Adaptive attention sparsity with hierarchical top-p pruning</title>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2502.02770" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_9Vm5WSB">Retrievalattention: Accelerating long-context llm inference via vector retrieval</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengruidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.10516" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongcheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolong</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2503.23077" />
		<title level="m" xml:id="_Km9DHQu">Efficient inference for large reasoning models: A survey</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main" xml:id="_YGpu8Db">Introducing gpt-oss</title>
		<ptr target="https://openai.com/index/introducing-gpt-oss/" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/introducing-o3-and-o4-mini/" />
		<imprint>
			<date type="published" when="2025-04">April 2025</date>
			<biblScope unit="page" from="2025" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_NnTR54Y">Gpqa: A graduate-level google-proof q&amp;a benchmark</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Betty</forename><forename type="middle">Li</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2311.12022" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_r7wWZKp">Open reasoning tasks: Llm reasoning tasks collection</title>
		<ptr target="https://github.com/NousResearch/Open-Reasoning-Tasks" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_kJMwqBT">Nous Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_eC9XdpM">Quest: Query-aware sparsity for efficient long-context llm inference</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.10774" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Qwen</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2505.09388" />
		<title level="m" xml:id="_yeguAM2">Qwen3 technical report</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main" xml:id="_JamQjys">Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_5xZYSnS">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_GP6FU6u">Efficient streaming language models with attention sinks</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6Bu4Rkb">arXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main" xml:id="_45q5RJZ">Duoattention: Efficient long-context llm inference with retrieval and streaming heads</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.10819" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_F9ZjgW5">Tidaldecode: Fast and accurate llm decoding with position persistent sparse attention</title>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuofu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.05076" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_5swxEPP">Cascade inference: Memory bandwidth efficient shared prefix batch decoding</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Zihao Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lequn</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<ptr target="https://flashinfer.ai/2024/01/08/cascade-inference.html" />
		<imprint>
			<date type="published" when="2024-01">Jan 2024</date>
			<biblScope unit="page" from="2024" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_Kgbd7bW">Native sparse attention: Hardware-aligned and natively trainable sparse attention</title>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazuo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangding</forename><surname>Zeng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2502.11089" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main" xml:id="_5wCxBJP">H 2 o: Heavyhitter oracle for efficient generative inference of large language models</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
