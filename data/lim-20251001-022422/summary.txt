The submission "LESS IS MORE: TRAINING-FREE SPARSE ATTENTION WITH GLOBAL LOCALITY FOR EFFICIENT REASONING" introduces a novel training-free sparse attention mechanism, "LessIsMore," which aggregates local information into a global pattern to enhance efficiency in large language models. This approach is distinct from existing methods due to its training-free nature, although claims of improved accuracy and latency require further evidence. The paper's contributions include a unified attention head selection and a stable recency window, both of which are novel but need clearer performance validation against similar methods. The submission aligns with current trends in sparse attention optimization, focusing on efficiency without retraining, but could benefit from citing additional relevant works to strengthen its context. Overall, while the training-free aspect is a significant differentiation, some claims of empirical improvements may be overstated without direct comparisons.