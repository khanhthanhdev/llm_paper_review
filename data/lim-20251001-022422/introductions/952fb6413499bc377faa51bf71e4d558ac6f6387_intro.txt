Large Language Models (LLMs) exhibit remarkable versatility across numerous applications (Brown et al., 2020; Tay et al., 2022; Wan et al., 2023). Central to LLM is the attention mechanism (Vaswani et al., 2017), which computes interactions among tokens within a certain span, thereby enabling context understanding. Scaling input length is crucial for enhancing LLM capabilities (Chen et al., 2023; Tworkowski et al., 2023), including fact retrieval, summarization, few-shot learning, question answering and so on (Bai et al., 2023; Yuan et al., 2024). However, the ever-growing attention computation and Key-Value Cache (KV-Cache) pose significant efficiency challenges (Sheng et al., 2023; Xiao et al., 2024c; Han et al., 2023; Kwon et al., 2023).

Previous work proposes sparse attention methods to address the efficiency challenges of long contexts in generative LLMs. These methods typically employ a uniform, fixed-span sliding window mask across all heads and input lengths, limiting attention to local contexts only (Xiao et al., $2 0 2 4 \mathrm { c }$ Han et al., 2023). This approach allows the LLM to take long inputs with a fixed attention span, keeping bounded attention computation and KV caching overhead. Following previous works (Chen et al., 2023; Tworkowski et al., 2023), we quantify the effective context length as the maximum input length where content retrieval accuracy exceeds a $90 \%$ threshold. In principle, fixed-span local attention can gradually aggregate global information through multiple model layers, yielding a longer effective context length than each attention span (Feng et al., 2022; Zaheer et al., 2020). Nonetheless, we reveal that uniform masks, like StreamingLLM (Xiao et al., 2024c), hardly extend effective context length beyond the span, as shown in Figure 6. Figure 1(b) further illustrates such limitation: with a $50 \%$ attention span mask, StreamingLLM fails to accurately retrieve content from the earlier half of the input and performs even worse at longer input lengths. Figure 2 reveals one possible explanation for the problem: while some attention heads focus on local contexts, others encompass the broad span of the entire input sequence. Consequently, the uniform approach fails to achieve a long effective context length as it limits the attention span of the global-context heads, while excessively allocates compute and memory budget for local-context heads. Additionally, as the input length increases, some attention heads need a faster increase in attention span than others to avoid serious performance degradation, as shown in Table 1. Unfortunately, the uniform approaches do not include heterogeneous rules to scale the attention spans differently for various heads. Besides, existing model compression methods (Men et al., 2024; Lin et al., 2023; Xiao et al., 2024b; Li et al., 2024a; Kim et al., 2023; Li et al., 2024b) use general language modeling corpora to decide the compression plan, which cannot accurately profile the influence of compression on long-context tasks.

![](images/f809cdd2d4b898d979e121f6aab4bd9d5e0cc9a48ff6b1ef739af0f681ee8aad.jpg)  
Figure 1: Retrieval accuracy of the Vicuna-7B model using different attention methods across varying input lengths and retrieval positions on the LongEval benchmark (Li et al., 2023). This retrieval benchmark takes massive key-value pairs as inputs and tests the accuracy to retrieve values based on given keys from diverse positions. (a) Original model with a full attention span; (b) StreamingLLM with half the attention span, showing reduced effectiveness beyond the span; (c) MoA with half the attention span, maintaining effectiveness beyond the span.

In this work, we propose Mixture of Attention (MoA), a training-free sparse attention method. As illustrated in Figure 3, MoA constructs the search space of heterogeneous elastic rules of attention spans. For automatic LLM compression, MoA first utilizes gradient-based profiling to inspect the influences of each attention position on the prediction loss. Based on the profiling results, MoA tailors heterogeneous sparse attention configurations for each model layer and attention head. During profiling, MoA employs a calibration dataset with long-range dependencies and uses the original dense model’s response instead of the human-written response as the reference to calculate the loss. This ensures an accurate profiling of the attention influences to facilitate better compression results. Our contributions are summarized as follows.

• Heterogeneous Elastic Rules. We propose heterogeneous elastic rules for masks of each attention head. We formulate MoA compression search space to include a diverse range of elastic rules that tailor the local attention span relative to the input length for each attention head. The heterogeneous elastic rules improve the fact retrieval accuracy of MoA from $2 5 \%$ to $98 \%$ compared with masks with uniform span and scaling function for each head.

• Calibration Dataset Construction We emphasize the importance of data engineering in LLM compression. Our findings demonstrate that, instead of relying on general language modeling datasets and human responses, using datasets with long-range dependencies and referencing the original LLM’s responses is essential for accurately profiling the effects of compression.

• Automatic Optimization. We propose an automatic pipeline to find the optimal compression plan encompassing heterogeneous elastic rules for various attention heads. This pipeline can efficiently find the optimal plan within several hours, for example, two hours for compressing Vicuna-13B.

Experiments show that MoA achieves $6 . 6 - 8 . 2 \times$ throughput improvements over FlashAttention2, $1 . 7 - 1 . 9 \times$ over vLLM framework on 7B and 13B dense LLMs at a $50 \%$ density (the average of KV-Cache length / input length), with only a $1 \%$ average relative degradation in retrieval accuracy. In Section 6.4, we show that this significant throughput improvements of MoA over FlashAttention2 can be attributed to four factors: (1) the static size of the KV-cache, (2) reduced attention computations, (3) increased batch size enabled by reduced memory usage, and (4) a specialized kernel implementation. Additionally, MoA achieves over $90 \%$ retrieval accuracy with just $2 5 \%$ average density, far surpassing sparse attention baselines that need a density of $7 5 \%$ to $100 \%$ for similar performance. On longcontext understanding benchmarks, MoA performs comparably to dense models, with a maximum relative performance drop of less than $5 \%$ , which is about one-sixth of that observed with the uniform sparse attention baseline. Our code is available at https://github.com/thu-nics/MoA.