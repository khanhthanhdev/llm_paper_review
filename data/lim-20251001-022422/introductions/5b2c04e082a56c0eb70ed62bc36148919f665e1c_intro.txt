Recent advances (Xiong et al., 2023; Liu et al., 2023a; Chen et al., 2023b; Li et al., 2023a; Chen et al., 2023a) race to scale the context window of large language models (LLMs) (Brown et al., 2020; Vaswani et al., 2017; Touvron et al., 2023) for more complex applications, including document analysis (Zhang et al., 2024a), code copilot (Chen et al., 2021c; Roziere et al., 2023), and prolonged conversations (Chiang et al., 2023; Taori et al., 2023). Popular LLMs like Gemini (Team et al., 2023), Claude (Anthropic, 2023) and Kimi (Moonshot, 2023) now support context lengths exceeding 1 million tokens. However, the increase in context length makes it challenging to support live interactions due to the quadratic complexity of attention mechanism. As illustrated in Figure 1, the attention computation time increases quadratically with sequence length, quickly dominating the Time to First Token (TTFT) latency (i.e. prefill latency). For example, in a 1 million token context, the attention of ChatGLM3-6B (Du et al., 2021) takes 1555 seconds, constituting over $90 \%$ of the TTFT when evaluated on an A100 GPU.

Prior work has consistently demonstrated that attention scores exhibit high sparsity (Zaheer et al., 2020; Kitaev et al., 2020; Jiang et al., 2024; Li et al., 2024). This characteristic makes sparse attention a promising approach for reducing prefill latency, as it allows the model to compute attention selectively for only the most important query and key-value tokens rather than the entire sequence. Based on these observations, plenty of approaches propose to approximate the dense attention with static sparse pattern, like LongFormer (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), LongNet (Ding et al., 2023) and StreamingLLM (Xiao et al., 2023b). However, these approaches fail to capture the dynamic sparse pattern across heads and inputs (Jiang et al., 2024; Likhosherstov et al., 2021) and cannot achieve the same accuracy of full attention.

![](images/69707a09a5a50e985d477ad8c1184db89191aa97bff8da8a08da614120245327.jpg)  
Figure 1. Compared to previous static and dynamic sparse attention methods, SampleAttention captures adaptive structured sparse patterns for each head. It achieves a significant reduction in TTFT compared to FlashAttention2.

Recent work proposes to address this dynamic sparse pattern through runtime attention index selection (Liu et al.,

2022; Han et al., 2023; Jiang et al., 2024). DSA (Liu et al., 2022) approximates attention patterns using low-rank hidden dimensions, but incurs significant computational overhead with long contexts. While HyperAttention (Han et al., 2023) employs Locality Sensitive Hashing (LSH) to identify important attention scores, its coarse-grained selection approach struggles to maintain model accuracy. MInference (Jiang et al., 2024) takes a hybrid approach: it predefines several sparse patterns and matches each attention head to its optimal pattern offline to achieve a target sparsification budget, then dynamically searches for sparse indices during the prefill phase. However, this approachâ€™s reliance on predefined patterns and fixed budgets fails to capture both the varying sparsity ratios across attention heads and the dynamic sparse patterns that adapt to different input contents.

In this paper, we find effectively exploiting the inherentlyhigh attention sparsity to accelerate prefill computation remains challenging due to two key factors. First, the optimal sparsity ratio varies adaptively across attention heads, input contents, and model architectures, making it necessary to be determined at runtime. Second, attention patterns also varies across heads and contents, often combining typical column and slash patterns. Some attention heads even display intricate combinations of these patterns, further complicating sparse pattern selection. These dynamic characteristics create significant challenges for existing methods to achieve an optimal trade-off due to their lack of flexibility. This highlights the need for a more adaptable, runtime-efficient approach to determine both the sparsity ratio and the pattern.

To address these challenges, we propose a novel approach, SampleAttention, which can dynamically determine sparse ratios and patterns at runtime. To select a minimal set of significant column and slash patterns while maintaining accuracy, we introduce a robust metric for evaluating model accuracy called Cumulative Residual Attention (CRA), which measures the capability of attention recall. The details of this key insight will be presented in Section 3. Based on CRA, we elaborate on a two-stage query-guided key-value filtering method proposed by SampleAttention in Section 4. This implementation is designed to efficiently identify important columns and slashes at runtime. SampleAttention also develops an automated tuning method that uses a small profiling dataset to determine the optimal hyperparameter setting for each model across different length ranges. SampleAttention significantly accelerates vanilla attention by reducing both I/O and computation requirements. We also implement hardware-efficient kernels. Notably, SampleAttention aims to reduce the computation overhead of attention, and is orthogonal and can be combined with existing KV cache eviction approaches (Zhang et al., $2 0 2 4 \mathrm { c }$ ; Ribar et al., 2023; Mu et al., 2024) to further reduce memory consumption.

We evaluate SampleAttention on ChatGLM (GLM et al., 2024), YI (Young et al., 2024) and InternLM (Cai et al., 2024) with a suite of popular benchmarks covering various generative tasks across different sequence lengths. Experimental results show that SampleAttention achieves nearly no accuracy loss1 for different LLMs, significantly outperforming prior works, and reduces the TTFT by up to $5 . 2 9 \times$ compared with FlashAttention2.