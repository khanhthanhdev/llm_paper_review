Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance on various applications. As LLMs evolve, they are increasingly being adapted to manage tasks with long contexts, such as Chain-of-Thought reasoning (Wei et al., 2023), document summarization (Huang et al., 2021), and retrieval-augmented generation (Ram et al., 2023; Zhang et al., 2024b). However, quickly and efficiently serving long-context LLMs is challenging due to the inherent memory and compute bottlenecks in the Transformer architectures (Vaswani et al., 2023).

LLM inference involves two separate stages: prefilling and decoding. The prefilling stage computes the activations for all input tokens and stores the keys and values for all tokens in the key-value (KV) cache, allowing the LLM to reuse these keys and values to compute attention for future tokens. In each decoding stage, the LLM decodes one new token using all input tokens and previously generated tokens. The KV cache size grows linearly in the sequence length (Kwon et al., 2023). For instance, with a context length of 128K tokens, the KV cache of LLama2-7B with half-precision can easily reach $6 4 \mathrm { G B }$ , creating substantial memory pressure for LLM serving. In addition, the LLM decoding stage is memory-bounded since decoding one new token requires accessing all previous tokens in the KV cache, making KV cache access the primary bottleneck for long-context LLM decoding. This memory-bound nature severely limits the scalability and efficiency of LLM serving.

![](images/32a84a1aa8b8fa1b0bb95b349b97ca26715b3f96d146c64c99544033842f3bfd.jpg)  
Figure 1: The heatmap for one decoding step of Llama3-8B-Instruct (AI, 2024a), where columns and rows indicate different Transformer layers and tokens in the KV cache, respectively. For each layer, the 5 tokens $10 \%$ sparsity) with the highest attention scores of the first attention head are highlighted in yellow, which are the tokens used for sparse attention. We feed an input prompt “Use only the provided search results to write a high-quality, concise answer to the question. $\backslash \mathrm { n } <$ —begin of text— $\cdot > \backslash { \mathtt { n } }$ The magic number is: 15213. $\backslash \boldsymbol { \mathrm { n } } \backslash \boldsymbol { \mathrm { n } } \backslash \boldsymbol { \mathrm { n } }$ Question: What is the magic number? Keep the response short and direct. Answer: ”, and the LLM outputs “15213”. The results show strong spatial coherence of tokens chosen for sparse attention in the decoding step.

To address this problem, recent work has introduced sparse attention, which approximates full attention using a small portion of tokens with the highest attention scores. Compared to full attention, sparse attention reduces computation cost and memory access while preserving the LLM’s generative performance (Ge et al., 2024; Zhang et al., 2023). Existing sparse attention techniques can be classified into two categories: eviction- and selection-based methods.

First, eviction-based sparse attention reduces memory usage for the KV cache by selectively discarding less relevant tokens from the KV cache, therefore reducing the number of tokens computed in attention mechanisms (Xiao et al., 2023; Zhang et al., 2023). While these methods decrease the size of the KV cache, they can be inadequate for tasks where critical information is carried by tokens that are prematurely evicted, such as the needle-in-the-haystack tasks (Peng et al., 2023). On the other hand, selection-based sparse attention maintains all tokens in the KV cache, estimates their attention scores, and selects a small subset of tokens to participate in each LLM decoding step. This approach is prone to issues related to distribution shifts caused by appending sparsely attended, biased KV representations back into the cache.

This paper presents TidalDecode, an algorithm and system for fast and precise LLM decoding, utilizing position persistent sparse attention (PPSA). A key insight behind TidalDecode is the observation that tokens chosen for sparse attention — based on their highest attention scores — exhibit significant overlap across consecutive Transformer layers within each decoding phase. Figure 1 illustrates this overlap in a single decoding step of LLaMA-3-8B instruct AI (2024a) with an input of 51 tokens. Each column in the figure corresponds to a Transformer layer, and each row indicates one token in the KV cache. Selection-based sparse attention methods select the 5 tokens with the highest attention scores (highlighted in yellow) for attention computation in each head. As the figure depicts, there is a recurring pattern where consecutive layers consistently focus on the same set of tokens, indicating a spatial coherence in the selection of tokens for sparse attention.

Instead of independently selecting tokens for sparse attention at each layer, TidalDecode introduces a few token selection layers, which perform full attention to identify the tokens with the highest attention scores. All remaining layers implement position persistent sparse attention, where only the tokens selected by the token selection layers are retrieved from the KV cache for attention. Consequently, all other layers between two token selection layers operate on the same set of tokens, reducing the overhead of token selection. Experiments across a diverse set of LLMs and datasets demonstrate that using just two token selection layers — one at the beginning and one in the middle — is sufficient to achieve high generative performance while minimizing computation and memory overheads.

This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Additionally, to address the KV cache distribution shift, TidalDecode introduces a cache-correction mechanism that periodically refills the KV cache using full attention for all sparsely decoded tokens to mitigate bias in the KV representations.

Comprehensive evaluation with the LongChat-7b-v1.5-32k, Llama-3-8B, Llama-3-70B, and Llama3.1-8B models on the Needle-in-the-Haystack, PG-19, and LongBench tasks demonstrates that TidalDecode can consistently achieve the best performance efficiency trade-off compared with the best existing sparse attention methods. We have implemented custom GPU kernels for PPSA and an end-to-end system for TidalDecode. Compared with existing full and sparse attention implementations, our system reduced the end-to-end inference latency by up to $2 . 1 \times$ and $1 . 2 \times$ , respectively. In conclusion, our contributions are:

• We propose TidalDecode, a streamlined and efficient algorithm and system for fast and high-quality LLM decoding, utilizing position persistent sparse attention. • To address KV cache distribution shifts, we introduce a cache-correction mechanism that periodically refills the KV cache with using full attention for sparsely decoded tokens. • Empirically, we demonstrate the effectiveness and efficiency of TidalDecode through comprehensive evaluation, showing that TidalDecode significantly outperforms existing sparse attention methods.