Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1, 20, 19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.

Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods [39, 61] allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually cost $O ( n \log n )$ to maintain clusters. Ainslie et al. [1] incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.

![](images/f7cedf213663e0f72b017d84873fae8e23ec692cf87d241e87ec16274d4e043d.jpg)  
Figure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query $( \tau ( \mathbf { u } ) )$ such that the sum of normalized scores is $k$ , which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency.

To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows.

# Incremental KV Selection.

The SPARSEK operator (ยง 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results.

Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65, 32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (ยง 3.2).

Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency.

We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments on language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention.