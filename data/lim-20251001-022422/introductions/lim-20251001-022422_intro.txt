Recent advancements in large reasoning models (LRMs) have significantly enhanced the capabilities of large language models (LLMs) for complex reasoning tasks. Models such as DeepSeek-R1 (DeepSeek-AI, 2025), Gemini-2.5-pro (DeepMind, 2025), OpenAI-o3 (OpenAI, 2025), Qwen3 (Team, 2025), and gpt-oss (OpenAI, 2025) leverage test-time scaling—generating large numbers of tokens—to enhance accuracy on challenging reasoning benchmarks (Wei et al., 2023; AoPS, 2025; Rein et al., 2023).

Unlike traditional language processing tasks, which involve long inputs and short outputs, reasoning tasks exhibit a different computational profile: they generate extensive multi-step derivations—often spanning tens of thousands of output tokens (Research, 2024)—from relatively concise problem statements. This decode-heavy nature incurs substantial computational overhead (Liu et al., 2025). For example, using full attention in the HuggingFace framework, DeepSeek-R1-Distill-Llama-8B consumes more than 20 minutes on a single NVIDIA RTX A5000 GPU to generate 32,768 tokens for one AIME problem.

This computational profile creates a unique optimization opportunity: while input processing benefits from full attention for accurate context understanding, the lengthy generation phase is well-suited for sparse attention mechanisms (Cai et al., 2025; Gao et al., 2025). Sparse attention selectively attends to a subset of critical tokens, offering a promising approach to reduce computational complexity and generation latency. Current techniques can be categorized into selection-based (Yang et al., 2024; Tang et al., 2024; Hao et al., 2025; Liu et al., 2024; Gao et al., 2025; Yuan et al., 2025) and eviction-based methods (Li et al., 2024; Xiao et al., 2023; Zhang et al., 2023; Adnan et al., 2024; Cai et al., 2025). Both identify important tokens using predefined criteria; eviction-based approaches permanently discard unselected tokens, while selection-based approaches maintain the full key-value (KV) cache.

However, existing sparse attention approaches suffer from significant accuracy degradation on reasoning tasks due to the accumulation of selection errors over long generation sequences (Gao et al., 2025). While standard generation tasks tolerate moderate information loss, step-by-step reasoning requires that crucial contextual information to be preserved throughout the entire process to maintain logical consistency (Lee & Hockenmaier, 2025). For instance, TidalDecode (Yang et al., 2024) achieves over $9 9 . 9 \%$ sparsity with no accuracy loss on retrieval task, but must reduce sparsity below $50 \%$ to preserve accuracy on AIME-24 reasoning tasks. In these settings, even small selection inaccuracies compound over thousands of generated tokens, leading to attention recall degradation and cascading accuracy drops. Moreover, prior work has shown that inaccurate sparse attention in reasoning models can also lengthen the generation process, which further harms the model’s inference efficiency (Gao et al., 2025).

These limitations motivated us to investigate the intrinsic attention distributions of reasoning models and tasks, with the goal of identifying patterns that enable more accurate and efficient token selection. Our token-level analysis across the reasoning process reveals two key observations on attention localities that fundamentally challenge the selection principles used in existing sparse attention methods.

First, reasoning tasks exhibit prominent spatial locality across attention heads, particularly within the Grouped Query Attention (GQA) frameworks prevalent in open-source LLMs (Touvron et al., 2023; AI, 2024; Team, 2025). Contrary to conventional wisdom that different attention heads perform specialized roles that require distinct token subsets (Yang et al., 2024; Xiao et al., 2024; Tang et al., 2024), we observe substantial overlap in token-importance rankings across heads within the same decoding layer. This overlap reveals that per-head top- $k$ selection yields only a local optimum, overfitting to head-specific query patterns while potentially missing globally important tokens that could enhance performance in future decoding layers.

Second, we observe a recency locality pattern across decoding steps: tokens that receive high attention in one decoding step tend to continue attracting substantial attention over multiple subsequent steps. Notably, the ratio between the size of this “recency window” and the total number of selected tokens remains relatively constant throughout decoding, reflecting the intuition that each logical step in reasoning builds directly on the conclusions of preceding steps (Lee & Hockenmaier, 2025).

Building on these insights, this paper presents LessIsMore, a novel training-free sparse attention approach that achieves higher accuracy on reasoning tasks with lower latency by attending to fewer tokens. LessIsMore aggregates head-specific local information into a global attention pattern that is both more robust and more accurate. In each selection layer, LessIsMore exploits the identified locality patterns through a unified token selection process: each attention head first identifies its approximate top- $k$ tokens using tailored selection schemes; these tokens are then aggregated across heads, globally ranked, and pruned to satisfy a predefined token budget. To capture recency locality, LessIsMore reserves a fixed proportion of this for a stable recency window, ensuring that recently generated tokens— critical for step-by-step reasoning—are consistently attended to.

Evaluation on Qwen3-8B and Qwen3-4B across diverse reasoning tasks, including AIME-24/25, GPQA-Diamond, and MATH500, demonstrates that LessIsMore consistently and significantly outperforms existing sparse attention baselines, including reasoning-focused methods that require retraining. LessIsMore maintains full accuracy at substantially higher sparsity levels, achieving up to $8 7 . 5 \%$ sparsity and $1 . 1 \times$ end-to-end inference speed-up on AIME-24 with lossless accuracy, all without increasing reasoning length. These gains are further enabled by our customized kernel support for GQA models.

In summary, our contributions are:

• We present the first detailed, token-level analysis of attention distributions in the reasoning process, revealing fundamental spatial and recency locality patterns that challenge the conventional assumptions of highly specialized, independent attention heads.

• We propose LessIsMore, a training-free sparse attention mechanism featuring: (1) Unified Attention Head Selection globally aggregates head-level top- $k$ selections, and (2) Stable Recency Window reserves recent contextual information for reasoning coherence.

• We show that LessIsMore matches or improves accuracy on challenging reasoning benchmarks while achieving a $1 . 1 \times$ average decoding speed-up compared with the full attention baseline. Compared to state-of-the-art sparse attention methods, LessIsMore attends to at least $2 \times$ fewer tokens, achieves a $1 . 1 3 \times$ end-to-end speed-up, and shortens generation length by $7 \%$ without sacrificing accuracy.