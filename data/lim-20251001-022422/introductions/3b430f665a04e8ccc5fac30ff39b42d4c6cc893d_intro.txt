Previous research has extensively investigated the use of attention sparsity (KV cache sparsity) to accelerate longcontext inference, both during the prefilling and decoding stages. The core idea is to compute approximate attention on a subset of tokens, often referred to as “critical tokens" or “heavy hitters" (Zhang et al., 2023). In practical deployments of these algorithms, it is necessary to specify the number of selected tokens, denoted as $B$ , commonly referred to as the KV cache budget. A top- $k$ operation is required to identify the indices of the critical tokens that correspond to the top$B$ highest estimated scores. As previously mentioned, a smaller $B$ significantly reduces the I/O operations, while a larger $B$ retains more contextual information, thereby minimizing accuracy loss.

However, identifying the optimal values for $B$ where both accuracy and efficiency are optimized is inherently challenging due to two major reasons: (a) Saturation points are runtime dynamic. Previous works (Wu et al., 2024; Xiao et al., 2024) have demonstrated that some heads, referred to as retrieval heads, are trained to extract important information from long contexts, while others focus only on local information. As shown in Figure 1, the distribution of attention weights may vary across different attention heads. Some attention distributions concentrate on a small subset of tokens, which we refer to as focused attention. In contrast, some attention distributions are flatter, where many tokens have similar attention weights; we define this as diffuse attention. For focused attention, using a fixed token budget for Top- $k$ attention often leads to over-selection, as only a few tokens are sufficient to accumulate sufficient attention weights. Similarly, for diffuse attention, a fixed token budget can result in under-selection, as a larger number of tokens is necessary to ensure accurate attention modeling. (b) Current algorithms suffer from varying degrees of inefficiency. As shown in Figure 2, we observe that the saturation point is highly dependent on the specific algorithm, necessitating offline calibration to determine the appropriate budget for each algorithm individually. The actual algorithms, like Quest (Tang et al., 2024b) or DS (Yang et al., 2024c), have to over-select some tokens as the inevitable inaccuracy in estimating the importance of tokens comparing with oracle.

![](images/8460d17909731b2a2dbc89c065d003b79c092e8feda23ca6637ba66d0be4ab91.jpg)  
(a) Top- $k$ Sparsity (H2O, Quest, …) fixed $\bar { k }$ -token budget results in either over-selection or under-selection.   
(b) Top- $p$ Sparsity (Ours) dynamically adjusts the budget to accumulate sufficient attention weights.   
Figure 1. Comparison of Top- $k$ and Top- $p$ Sparsity in Approximate Attention. Approximate attention typically employs techniques such as pooling, channel pruning, and quantization to approximate the query $( \tilde { Q } )$ and $\ker ( \tilde { K } )$ , enabling the estimation of attention weights. These weights are then used to select important tokens for sparse attention. (a) Top- $k$ Sparsity, utilized by methods like H2O and Quest, relies on a fixed $k$ -token budget, which often results in over-selection $( \sum \tilde { p _ { i } } > 0 . 9 )$ or under-selection $( \sum \tilde { p _ { i } } < 0 . 9 )$ . (b) Our proposed Top- $p$ Sparsity dynamically adjusts the budget to accumulate sufficient attention weights $( \sum \tilde { p _ { i } } = 0 . 9 )$ ), enabling more efficient and adaptive sparse attention.

In this work, we reveal that top- $k$ methods exhibit issues similar to those previously encountered in LLM sampling. Drawing on this analogy, we introduce top- $p$ sampling into sparse attention to address the budget estimation problem. Our study demonstrates that top- $p$ can determine the KV cache budget in a more intrinsic and dynamic way compared to top- $k$ . Based on these observations, we built Twilight, a hierarchical KV cache pruning framework that enhances existing sparse attention algorithms with adaptive budgeting capabilities. Specifically, Twilight first lets the base algorithm to select a large subset of tokens using a conservative budget, and then further refines this subset by retaining only the top- $p$ tokens.

Evaluations are conducted in two aspects: accuracy and efficiency. First, we demonstrate that Twilight optimizes the base algorithms with nearly no accuracy loss on both midcontext benchmarks (GSM8K (Cobbe et al., 2021), COQA (Reddy et al., 2019), PG19 dataset (Rae et al., 2019)) and a comprehensive long-context benchmark, Longbench (Bai et al., 2024). Next, we show that Twilight accelerates full attention by up to $1 5 . 4 \times$ times and existing sparse attention methods, by up to $2 . 2 \times$ times, leading to a $3 . 9 \times$ end-to-end speedup. Our contributions are summarized as follows:

• We conduct an in-depth investigation into a significant issue in top- $k$ sparse attention: the difficulty in identifying the optimal budget (the saturation point). We propose using top- $p$ sampling to dynamically determine this point at runtime.   
• We introduce Twilight, a framework that can endow any existing sparse attention method with adaptive budgeting capabilities, thereby improving their efficiency and facilitating their deployment.   
• We evaluate Twilight in terms of both accuracy and efficiency, demonstrating an $1 5 . 4 \times$ speedup on selfattention.