Up to now, most research efforts in statistical machine translation (SMT) research have relied on the use of a phrase-based system as suggested in (Koehn et al., 2003). Recently, however, an entirely new, neural network based approach has been proposed by several research groups (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), showing promising results, both as a standalone system or as an additional component in the existing phrase-based system. In this neural network based approach, an encoder 'encodes' a variable-length input sentence into a fixed-length vector and a decoder 'decodes' a variable-length target sentence from the fixedlength encoded vector.

works well with short sentences (e.g. $\lessapprox 2 0$ words), but has difficulty with long sentences (e.g. $\gtrapprox 2 0$ words), and particularly with sentences that are longer than those used for training. Training on long sentences is diffi cult because few available training corpora include sufficiently many long sentences, and because the computational overhead of each update iteration in training is linearly correlated with the length of training sentences. Additionally, by the nature of encoding a variablelength sentence into a fi xed-size vector representation, the neural network may fail to encode all the important details.

It has been observed in (Sutskever et al., 2014), (Kalchbrenner and Blunsom, 2013) and (Cho et al., 2014a) that this neural network approach

In this paper, hence, we propose to translate sentences piece-wise. We segment an input sentence into a number of short clauses that can be confi - dently translated by the model. We show empirically that this approach improves translation quality of long sentences, compared to using a neural network to translate a whole sentence without segmentation.