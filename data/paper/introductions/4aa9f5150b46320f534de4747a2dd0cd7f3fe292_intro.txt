Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8]. For that reason, RNNs have rarely been used for natural language processing tasks such as text classification despite their powers in representing sequential structures.

On a number of document classification tasks, we find that it is possible to train Long Short-Term Memory recurrent networks (LSTM RNNs) [9] to achieve good performance with careful tuning of hyperparameters. Furthermore, a simple pretraining step can significantly stabilize the training of LSTMs. For example, we can use a next step prediction model, i.e., a recurrent language model in NLP, as an unsupervised method. Another method is to use a sequence autoencoder, which uses a RNN to read a long input sequence into a single vector. This vector will then be used to reconstruct the original sequence. The weights obtained from these two pretraining methods can then be used as an initialization for standard LSTM RNNs to improve training and generalization.

In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly. This pretraining helps LSTMs reach or surpass previous baselines on these datasets without additional data.

Another important result from our experiments is that using more unlabeled data from related tasks in the pretraining can improve the generalization of a subsequent supervised model. For example, using unlabeled data from Amazon reviews to pretrain the sequence autoencoders can improve classification accuracy on Rotten Tomatoes from $7 9 . 7 \%$ to $8 3 . 3 \%$ , an equivalence of adding substantially more labeled data. This evidence supports the thesis that it is possible to use unsupervised learning with more unlabeled data to improve supervised learning. With sequence autoencoders, and outside unlabeled data, LSTMs are able to match or surpass previously reported results.

We believe our semi-supervised approach (as also argued by [1]) has some advantages over other unsupervised sequence learning methods, e.g., Paragraph Vectors [18], because it can allow for easy fine-tuning. Our semi-supervised learning approach is related to Skip-Thought vectors [13], with two differences. The first difference is that Skip-Thought is a harder objective, because it predicts adjacent sentences. The second is that Skip-Thought is a pure unsupervised learning algorithm, without fine-tuning.