[
  {
    "paper_id": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
    "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
    "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
    "publication_date": "2016-09-26",
    "venue": "",
    "year": 2016,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "publication_date": "2014-09-01",
    "venue": "International Conference on Learning Representations",
    "year": "2014",
    "citation_count": 27707,
    "authors": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "f0f970c6c30c4ea0c3c4847e29ae9cdc3839c3ef",
    "title": "Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation",
    "abstract": "Natural Language Processing (NLP) encompasses a broad range of techniques and methodologies for processing and understanding human language. One of the most important NLP applications that has experienced significant advancements and has gained immense importance over the years is Neural Machine Translation. Research on German-to-English language machine translation has remained a prominent area of research within the field of Natural Language Processing and Deep. This paper presents an in-depth analysis of three significant models that are used for Neural Machine Translation namely Recurrent Neural Network with Long Short-Term Memory, Recurrent Neural Network with Gated Recurrent Unit, and the Transformer. For the implementation of each model, a large data corpus of 221,534 sentence pairs is used. Two evaluation metrics are employed to assess the performance of models i.e., the BLEU Score and the ROUGE Score. BLEU-4 Score of 0.386, 0.402, and 0.482 is obtained for RNN+LSTM, RNN+GRU, and Transformer model respectively. Precision, Recall, and F1 Score of ROUGE Score are studied which points to similar results as that Learning of the BLEU Score. Both the evaluation metrics suggest that the transformer model outperforms both variants of RNN. The study also paves the way for further investigation in this area by offering important information about how each model is implemented and the outcomes it produces.",
    "publication_date": "2023-08-25",
    "venue": "",
    "year": 2023,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "0b544dfe355a5070b60986319a3f51fb45d1348e",
    "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "publication_date": "2014-06-03",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": "2014",
    "citation_count": 23920,
    "authors": "Kyunghyun Cho, B. V. Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
    "title": "Semi-supervised Sequence Learning",
    "abstract": "We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.",
    "publication_date": "2015-11-04",
    "venue": "",
    "year": 2015,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "publication_date": "2013-10-01",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": "2013",
    "citation_count": 1444,
    "authors": "Nal Kalchbrenner, Phil Blunsom",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "167ad306d84cca2455bc50eb833454de9f2dcd02",
    "title": "Joint Language and Translation Modeling with Recurrent Neural Networks",
    "abstract": "We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.",
    "publication_date": "2013-10-01",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": "2013",
    "citation_count": 290,
    "authors": "Michael Auli, Michel Galley, Chris Quirk, G. Zweig",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "43428880d75b3a14257c3ee9bda054e61eb869c0",
    "title": "Convolutional Sequence to Sequence Learning",
    "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
    "publication_date": "2017-05-08",
    "venue": "",
    "year": 2017,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "6658bbf68995731b2083195054ff45b4eca38b3a",
    "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition",
    "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.",
    "publication_date": "2012-01-01",
    "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
    "year": "2012",
    "citation_count": 3050,
    "authors": "George E. Dahl, Dong Yu, L. Deng, A. Acero",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "4b2937658f8d7f21b1c521e2224373976cf4ac15",
    "title": "A STUDY OF DATA AUGMENTATION AND ACCURACY IMPROVEMENT IN MACHINE TRANSLATION FOR VIETNAMESE SIGN LANGUAGE",
    "abstract": "Sign languages are independent languages of deaf communities. The translation from normal languages (i.e., Vietnamese Language - VL) as long as other sign languages to Vietnamese sign language (VSL) is a meaningful task that breaks down communication barriers and improves the quality of life for the deaf community. In this paper, we experimented with and proposed several methods for building and improving models for the VL to VSL translation task. We presented a data augmentation method to improve the performance of our neural machine translation models. Using an initial dataset of 10k bilingual sentence pairs, we were able to obtain a new dataset of 60k sentence pairs with a perplexity score no more than 1.5 times that of the original dataset. Experiments on the original dataset showed that rule-based models achieved the highest BLEU score of 68.02 among the translation models. However, with the augmented dataset, the Transformer model achieved the best performance with a BLEU score of 89.23, which is significantly better than that of other conventional approach methods.",
    "publication_date": "2023-06-12",
    "venue": "",
    "year": 2023,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "d12a4ae8f89570d6c5b5d30921c5a7eab1473120",
    "title": "Subset Retrieval Nearest Neighbor Machine Translation",
    "abstract": "k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose \u201cSubset kNN-MT\u201d, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efficient distance computation technique that is suitable for subset neighbor search using a look-up table. Our proposed method achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT\u201919 De-En translation task and the domain adaptation tasks in De-En and En-Ja.",
    "publication_date": null,
    "venue": "",
    "year": 2023,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "14f593d7e866d68f78fd84a283e2c751bfa2f667",
    "title": "A Case Study on Filtering for End-to-End Speech Translation",
    "abstract": "It is relatively easy to mine a large parallel corpus for any machine learning task, such as speech-to-text or speech-to-speech translation. Although these mined corpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model's performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.",
    "publication_date": "2024-02-02",
    "venue": "",
    "year": 2024,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "6122c95ac6475e965bf4e120f7a588d29bb00ecc",
    "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation",
    "abstract": "The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.",
    "publication_date": "2014-09-03",
    "venue": "SSST@EMNLP",
    "year": "2014",
    "citation_count": 83,
    "authors": "Jean Pouget-Abadie, Dzmitry Bahdanau, B. V. Merrienboer, Kyunghyun Cho, Yoshua Bengio",
    "novel": null,
    "cited_paper": true
  },
  {
    "paper_id": "b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347",
    "title": "Sequence-to-Sequence Learning with Latent Neural Grammars",
    "abstract": "Sequence-to-sequence learning with neural networks has become the de facto standard for sequence prediction tasks. This approach typically models the local distribution over the next word with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. We develop a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. We apply this latent neural grammar to various domains -- a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation -- and find that it performs respectably compared to standard baselines.",
    "publication_date": "2021-09-02",
    "venue": "",
    "year": 2021,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  {
    "paper_id": "3ead0b8a8984dfe327547652074e80421204a2e7",
    "title": "Application of LSTM Neural Network Technology Embedded in English Intelligent Translation",
    "abstract": "With the rapid development of computer technology, the loss of long-distance information in the transmission process is a prominent problem faced by English machine translation. The self-attention mechanism is combined with convolutional neural network (CNN) and long-term and short-term memory network (LSTM). An English intelligent translation model based on LSTM-SA is proposed, and the performance of this model is compared with other deep neural network models. The study adds SA to the LSTM neural network model and constructs the English translation model of LSTM-SA attention embedding. Compared with other deep learning algorithms such as 3RNN and GRU, the LSTM-SA neural network algorithm has faster convergence speed and lower loss value, and the loss value is finally stable at about 8.6. Under the three values of adaptability, the accuracy of LSTM-SA neural network structure is higher than that of LSTM, and when the adaptability is 1, the accuracy of LSTM-SA neural network improved the fastest, with an accuracy of nearly 20%. Compared with other deep learning algorithms, the LSTM-SA neural network algorithm has a better translation level map under the three hidden layers. The proposed LSTM-SA model can better carry out English intelligent translation, enhance the representation of source language context information, and improve the performance and quality of English machine translation model.",
    "publication_date": "2022-09-27",
    "venue": "",
    "year": 2022,
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  }
]